{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65629145-cb8e-40ae-8306-e04b7414869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from time import time\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c468349-f543-41ae-8d7d-56e7c34782c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "global logs, best_acc\n",
    "logs = []\n",
    "best_acc = 0 \n",
    "modellr = 1e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "number_class = 5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07437da4-eaf7-4323-aae2-0944cec0b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide training, validation, and testing sets by 70%, 15%, and 15%\n",
    "random.seed(66)\n",
    "\n",
    "def moveFile(input_path, rate, output_path):\n",
    "    pathDir = os.listdir(input_path)  #Retrieve the original path of the image\n",
    "    filenumber = len(pathDir)  #Number of original files\n",
    "    picknumber = int(filenumber * rate)  #Take a certain number of images from the folder according to the rate ratio\n",
    "    sample = random.sample(pathDir, picknumber)  #Randomly select sample images with a number of picknumbers\n",
    "    for file_name in sample:\n",
    "        shutil.move(input_path +'/'+ file_name, output_path + '/' + file_name)\n",
    "\n",
    "\n",
    "root = 'datasets'\n",
    "categories = os.listdir(root+'/train')\n",
    "np.savetxt(root+'/classes.txt', categories, fmt='%s')\n",
    "for m in categories:\n",
    "    #The input path is/train/0 in the root directory, and/train/1 in the root directory\n",
    "    input_path = root + '/train/' + str(m)\n",
    "    output_path1 = root + '/val/' + str(m)\n",
    "    output_path2 = root + '/test/' + str(m)\n",
    "   #Verify if the output path exists, if not, create it\n",
    "    isExists = os.path.exists(output_path1)\n",
    "    if not isExists:\n",
    "        os.makedirs(output_path1)\n",
    "        moveFile(input_path, 0.3, output_path1)\n",
    "    isExists = os.path.exists(output_path2)\n",
    "    if not isExists:\n",
    "        os.makedirs(output_path2)\n",
    "        moveFile(output_path1, 0.3, output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d377b007-b2b3-4f73-bd19-cc53f35b86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data augmentation\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.RandomHorizontalFlip(), #Random horizontal flip\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1/255, 1/255, 1/255])])\n",
    "transform_val = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1/255, 1/255, 1/255])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca2a1f6-9322-44f0-91b7-3efa16dfdf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training, validation, and test sets\n",
    "dataset_train = datasets.ImageFolder('datasets/train', transform)\n",
    "dataset_val = datasets.ImageFolder('datasets/val', transform_val)\n",
    "dataset_test = datasets.ImageFolder('datasets/test', transform_val)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5261401f-ec6a-4460-bad2-396f33a4e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        #This defines two consecutive convolutional layers within the residual block\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            #shortcutï¼Œin order to match the result structure of the two convolutional layers, it needs to be processed here\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        #Add the output of two convolutional layers to the processed x to implement the basic structure of ResNet\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x) \n",
    "    \n",
    "class SA_ResNet(nn.Module):\n",
    "    def __init__(self, ResBlock, num_classes=5):\n",
    "        super(SA_ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResBlock, 256, 2, stride=2)        \n",
    "        self.layer4 = self.make_layer(ResBlock, 512, 2, stride=2)   \n",
    "\n",
    "        self.sa1 = SpatialAttention()\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.sa1(out) * out\n",
    "        \n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class CA_ResNet(nn.Module):\n",
    "    def __init__(self, ResBlock, num_classes=5):\n",
    "        super(CA_ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResBlock, 256, 2, stride=2)        \n",
    "        self.layer4 = self.make_layer(ResBlock, 512, 2, stride=2)   \n",
    "\n",
    "        self.ca1 = ChannelAttention(512)\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.ca1(out) * out\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ensemble_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ensemble_model, self).__init__()\n",
    "        self.model1 = CA_ResNet(ResBlock)\n",
    "        self.model2 = SA_ResNet(ResBlock)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.model1(x)\n",
    "        out2 = self.model2(x)\n",
    "        out = out1+out2 \n",
    "        return out\n",
    "        \n",
    "model = ensemble_model()\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=modellr)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    modellrnew = modellr * (0.1 ** (epoch // 50))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = modellrnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1882c4-bb7d-46f0-86ff-85ffc14d36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    total_num = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print_loss = loss.data.item()\n",
    "        sum_loss += print_loss\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            log = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                (batch_idx + 1) * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item())\n",
    "            print(log)\n",
    "            logs.append(log)\n",
    "    ave_loss = sum_loss / len(train_loader)\n",
    "    log = 'epoch:{},loss:{}'.format(epoch, ave_loss)\n",
    "    print(log)\n",
    "    logs.append(log)\n",
    "\n",
    "def val(model, device, test_loader, phase):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total_num = len(test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            correct += torch.sum(pred == target)\n",
    "            print_loss = loss.data.item()\n",
    "            test_loss += print_loss\n",
    "        correct = correct.data.item()\n",
    "        acc = correct / total_num\n",
    "        avgloss = test_loss / len(test_loader)\n",
    "        if phase == 'train':\n",
    "            log = 'Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                avgloss,\n",
    "                correct,\n",
    "                len(test_loader.dataset),\n",
    "                100 * acc)\n",
    "        else:\n",
    "            log = 'Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                avgloss,\n",
    "                correct,\n",
    "                len(test_loader.dataset),\n",
    "                100 * acc)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(model, 'model_best.pth')\n",
    "        print(log)\n",
    "        logs.append(log)\n",
    "        np.savetxt('logs.txt', logs, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5425ae24-1b10-4000-8c2f-8861d2ae039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [40/2566 (2%)]\tLoss: 1.507423\n",
      "Train Epoch: 1 [80/2566 (3%)]\tLoss: 1.504002\n",
      "Train Epoch: 1 [120/2566 (5%)]\tLoss: 1.591035\n",
      "Train Epoch: 1 [160/2566 (6%)]\tLoss: 1.258470\n",
      "Train Epoch: 1 [200/2566 (8%)]\tLoss: 1.126681\n",
      "Train Epoch: 1 [240/2566 (9%)]\tLoss: 1.109025\n",
      "Train Epoch: 1 [280/2566 (11%)]\tLoss: 1.226363\n",
      "Train Epoch: 1 [320/2566 (12%)]\tLoss: 1.071216\n",
      "Train Epoch: 1 [360/2566 (14%)]\tLoss: 1.012170\n",
      "Train Epoch: 1 [400/2566 (16%)]\tLoss: 0.828345\n",
      "Train Epoch: 1 [440/2566 (17%)]\tLoss: 1.079406\n",
      "Train Epoch: 1 [480/2566 (19%)]\tLoss: 0.717533\n",
      "Train Epoch: 1 [520/2566 (20%)]\tLoss: 0.943379\n",
      "Train Epoch: 1 [560/2566 (22%)]\tLoss: 1.220411\n",
      "Train Epoch: 1 [600/2566 (23%)]\tLoss: 0.911196\n",
      "Train Epoch: 1 [640/2566 (25%)]\tLoss: 1.324490\n",
      "Train Epoch: 1 [680/2566 (26%)]\tLoss: 0.793185\n",
      "Train Epoch: 1 [720/2566 (28%)]\tLoss: 0.841843\n",
      "Train Epoch: 1 [760/2566 (30%)]\tLoss: 1.445873\n",
      "Train Epoch: 1 [800/2566 (31%)]\tLoss: 1.003184\n",
      "Train Epoch: 1 [840/2566 (33%)]\tLoss: 0.953198\n",
      "Train Epoch: 1 [880/2566 (34%)]\tLoss: 1.035206\n",
      "Train Epoch: 1 [920/2566 (36%)]\tLoss: 0.471785\n",
      "Train Epoch: 1 [960/2566 (37%)]\tLoss: 0.518724\n",
      "Train Epoch: 1 [1000/2566 (39%)]\tLoss: 0.708282\n",
      "Train Epoch: 1 [1040/2566 (40%)]\tLoss: 1.166154\n",
      "Train Epoch: 1 [1080/2566 (42%)]\tLoss: 0.452665\n",
      "Train Epoch: 1 [1120/2566 (44%)]\tLoss: 0.637672\n",
      "Train Epoch: 1 [1160/2566 (45%)]\tLoss: 0.887903\n",
      "Train Epoch: 1 [1200/2566 (47%)]\tLoss: 0.874995\n",
      "Train Epoch: 1 [1240/2566 (48%)]\tLoss: 0.872419\n",
      "Train Epoch: 1 [1280/2566 (50%)]\tLoss: 0.677111\n",
      "Train Epoch: 1 [1320/2566 (51%)]\tLoss: 0.660863\n",
      "Train Epoch: 1 [1360/2566 (53%)]\tLoss: 0.972829\n",
      "Train Epoch: 1 [1400/2566 (55%)]\tLoss: 2.197537\n",
      "Train Epoch: 1 [1440/2566 (56%)]\tLoss: 1.120546\n",
      "Train Epoch: 1 [1480/2566 (58%)]\tLoss: 1.145839\n",
      "Train Epoch: 1 [1520/2566 (59%)]\tLoss: 0.563791\n",
      "Train Epoch: 1 [1560/2566 (61%)]\tLoss: 0.524638\n",
      "Train Epoch: 1 [1600/2566 (62%)]\tLoss: 0.768651\n",
      "Train Epoch: 1 [1640/2566 (64%)]\tLoss: 1.601162\n",
      "Train Epoch: 1 [1680/2566 (65%)]\tLoss: 0.592369\n",
      "Train Epoch: 1 [1720/2566 (67%)]\tLoss: 1.126612\n",
      "Train Epoch: 1 [1760/2566 (69%)]\tLoss: 0.702084\n",
      "Train Epoch: 1 [1800/2566 (70%)]\tLoss: 0.966646\n",
      "Train Epoch: 1 [1840/2566 (72%)]\tLoss: 0.508442\n",
      "Train Epoch: 1 [1880/2566 (73%)]\tLoss: 0.674215\n",
      "Train Epoch: 1 [1920/2566 (75%)]\tLoss: 0.467174\n",
      "Train Epoch: 1 [1960/2566 (76%)]\tLoss: 1.044950\n",
      "Train Epoch: 1 [2000/2566 (78%)]\tLoss: 1.343654\n",
      "Train Epoch: 1 [2040/2566 (79%)]\tLoss: 0.741430\n",
      "Train Epoch: 1 [2080/2566 (81%)]\tLoss: 0.542277\n",
      "Train Epoch: 1 [2120/2566 (83%)]\tLoss: 0.694971\n",
      "Train Epoch: 1 [2160/2566 (84%)]\tLoss: 0.967176\n",
      "Train Epoch: 1 [2200/2566 (86%)]\tLoss: 0.977298\n",
      "Train Epoch: 1 [2240/2566 (87%)]\tLoss: 1.240101\n",
      "Train Epoch: 1 [2280/2566 (89%)]\tLoss: 0.717343\n",
      "Train Epoch: 1 [2320/2566 (90%)]\tLoss: 0.560680\n",
      "Train Epoch: 1 [2360/2566 (92%)]\tLoss: 0.322429\n",
      "Train Epoch: 1 [2400/2566 (93%)]\tLoss: 0.564619\n",
      "Train Epoch: 1 [2440/2566 (95%)]\tLoss: 0.362602\n",
      "Train Epoch: 1 [2480/2566 (97%)]\tLoss: 0.843348\n",
      "Train Epoch: 1 [2520/2566 (98%)]\tLoss: 0.813548\n",
      "Train Epoch: 1 [2560/2566 (100%)]\tLoss: 1.120763\n",
      "epoch:1,loss:0.9559458801493838\n",
      "Train set: Average loss: 0.8624, Accuracy: 1744/2566 (68%)\n",
      "Val set: Average loss: 0.8634, Accuracy: 231/327 (71%)\n",
      "Train Epoch: 2 [40/2566 (2%)]\tLoss: 0.565935\n",
      "Train Epoch: 2 [80/2566 (3%)]\tLoss: 0.590188\n",
      "Train Epoch: 2 [120/2566 (5%)]\tLoss: 0.729923\n",
      "Train Epoch: 2 [160/2566 (6%)]\tLoss: 0.411479\n",
      "Train Epoch: 2 [200/2566 (8%)]\tLoss: 0.646053\n",
      "Train Epoch: 2 [240/2566 (9%)]\tLoss: 0.807060\n",
      "Train Epoch: 2 [280/2566 (11%)]\tLoss: 1.653433\n",
      "Train Epoch: 2 [320/2566 (12%)]\tLoss: 1.013257\n",
      "Train Epoch: 2 [360/2566 (14%)]\tLoss: 1.070017\n",
      "Train Epoch: 2 [400/2566 (16%)]\tLoss: 0.577050\n",
      "Train Epoch: 2 [440/2566 (17%)]\tLoss: 0.956412\n",
      "Train Epoch: 2 [480/2566 (19%)]\tLoss: 0.996457\n",
      "Train Epoch: 2 [520/2566 (20%)]\tLoss: 0.880498\n",
      "Train Epoch: 2 [560/2566 (22%)]\tLoss: 0.600591\n",
      "Train Epoch: 2 [600/2566 (23%)]\tLoss: 0.767785\n",
      "Train Epoch: 2 [640/2566 (25%)]\tLoss: 0.407867\n",
      "Train Epoch: 2 [680/2566 (26%)]\tLoss: 1.721424\n",
      "Train Epoch: 2 [720/2566 (28%)]\tLoss: 0.621507\n",
      "Train Epoch: 2 [760/2566 (30%)]\tLoss: 0.462043\n",
      "Train Epoch: 2 [800/2566 (31%)]\tLoss: 0.374962\n",
      "Train Epoch: 2 [840/2566 (33%)]\tLoss: 1.105337\n",
      "Train Epoch: 2 [880/2566 (34%)]\tLoss: 0.535083\n",
      "Train Epoch: 2 [920/2566 (36%)]\tLoss: 0.840966\n",
      "Train Epoch: 2 [960/2566 (37%)]\tLoss: 0.793232\n",
      "Train Epoch: 2 [1000/2566 (39%)]\tLoss: 0.575275\n",
      "Train Epoch: 2 [1040/2566 (40%)]\tLoss: 0.675779\n",
      "Train Epoch: 2 [1080/2566 (42%)]\tLoss: 1.160355\n",
      "Train Epoch: 2 [1120/2566 (44%)]\tLoss: 0.931280\n",
      "Train Epoch: 2 [1160/2566 (45%)]\tLoss: 0.484779\n",
      "Train Epoch: 2 [1200/2566 (47%)]\tLoss: 0.836649\n",
      "Train Epoch: 2 [1240/2566 (48%)]\tLoss: 0.752244\n",
      "Train Epoch: 2 [1280/2566 (50%)]\tLoss: 1.711498\n",
      "Train Epoch: 2 [1320/2566 (51%)]\tLoss: 1.903973\n",
      "Train Epoch: 2 [1360/2566 (53%)]\tLoss: 0.443199\n",
      "Train Epoch: 2 [1400/2566 (55%)]\tLoss: 1.136414\n",
      "Train Epoch: 2 [1440/2566 (56%)]\tLoss: 0.904275\n",
      "Train Epoch: 2 [1480/2566 (58%)]\tLoss: 0.985014\n",
      "Train Epoch: 2 [1520/2566 (59%)]\tLoss: 1.097528\n",
      "Train Epoch: 2 [1560/2566 (61%)]\tLoss: 0.829654\n",
      "Train Epoch: 2 [1600/2566 (62%)]\tLoss: 0.764942\n",
      "Train Epoch: 2 [1640/2566 (64%)]\tLoss: 0.437708\n",
      "Train Epoch: 2 [1680/2566 (65%)]\tLoss: 1.059279\n",
      "Train Epoch: 2 [1720/2566 (67%)]\tLoss: 1.165516\n",
      "Train Epoch: 2 [1760/2566 (69%)]\tLoss: 0.667768\n",
      "Train Epoch: 2 [1800/2566 (70%)]\tLoss: 1.642692\n",
      "Train Epoch: 2 [1840/2566 (72%)]\tLoss: 0.710095\n",
      "Train Epoch: 2 [1880/2566 (73%)]\tLoss: 0.642284\n",
      "Train Epoch: 2 [1920/2566 (75%)]\tLoss: 0.871410\n",
      "Train Epoch: 2 [1960/2566 (76%)]\tLoss: 1.120742\n",
      "Train Epoch: 2 [2000/2566 (78%)]\tLoss: 0.402318\n",
      "Train Epoch: 2 [2040/2566 (79%)]\tLoss: 2.013246\n",
      "Train Epoch: 2 [2080/2566 (81%)]\tLoss: 1.399067\n",
      "Train Epoch: 2 [2120/2566 (83%)]\tLoss: 0.309611\n",
      "Train Epoch: 2 [2160/2566 (84%)]\tLoss: 0.904260\n",
      "Train Epoch: 2 [2200/2566 (86%)]\tLoss: 0.421301\n",
      "Train Epoch: 2 [2240/2566 (87%)]\tLoss: 0.464201\n",
      "Train Epoch: 2 [2280/2566 (89%)]\tLoss: 0.698455\n",
      "Train Epoch: 2 [2320/2566 (90%)]\tLoss: 1.683604\n",
      "Train Epoch: 2 [2360/2566 (92%)]\tLoss: 0.654907\n",
      "Train Epoch: 2 [2400/2566 (93%)]\tLoss: 0.373109\n",
      "Train Epoch: 2 [2440/2566 (95%)]\tLoss: 0.476656\n",
      "Train Epoch: 2 [2480/2566 (97%)]\tLoss: 0.995765\n",
      "Train Epoch: 2 [2520/2566 (98%)]\tLoss: 0.670122\n",
      "Train Epoch: 2 [2560/2566 (100%)]\tLoss: 0.757734\n",
      "epoch:2,loss:0.8133108920861627\n",
      "Train set: Average loss: 0.7730, Accuracy: 1870/2566 (73%)\n",
      "Val set: Average loss: 0.7945, Accuracy: 234/327 (72%)\n",
      "Train Epoch: 3 [40/2566 (2%)]\tLoss: 0.737229\n",
      "Train Epoch: 3 [80/2566 (3%)]\tLoss: 1.098890\n",
      "Train Epoch: 3 [120/2566 (5%)]\tLoss: 0.372183\n",
      "Train Epoch: 3 [160/2566 (6%)]\tLoss: 0.748312\n",
      "Train Epoch: 3 [200/2566 (8%)]\tLoss: 0.903954\n",
      "Train Epoch: 3 [240/2566 (9%)]\tLoss: 1.082154\n",
      "Train Epoch: 3 [280/2566 (11%)]\tLoss: 1.150477\n",
      "Train Epoch: 3 [320/2566 (12%)]\tLoss: 0.734751\n",
      "Train Epoch: 3 [360/2566 (14%)]\tLoss: 0.730714\n",
      "Train Epoch: 3 [400/2566 (16%)]\tLoss: 0.703903\n",
      "Train Epoch: 3 [440/2566 (17%)]\tLoss: 0.721114\n",
      "Train Epoch: 3 [480/2566 (19%)]\tLoss: 0.585396\n",
      "Train Epoch: 3 [520/2566 (20%)]\tLoss: 0.758937\n",
      "Train Epoch: 3 [560/2566 (22%)]\tLoss: 0.285224\n",
      "Train Epoch: 3 [600/2566 (23%)]\tLoss: 0.825979\n",
      "Train Epoch: 3 [640/2566 (25%)]\tLoss: 1.192548\n",
      "Train Epoch: 3 [680/2566 (26%)]\tLoss: 1.091403\n",
      "Train Epoch: 3 [720/2566 (28%)]\tLoss: 0.644308\n",
      "Train Epoch: 3 [760/2566 (30%)]\tLoss: 0.512720\n",
      "Train Epoch: 3 [800/2566 (31%)]\tLoss: 0.699490\n",
      "Train Epoch: 3 [840/2566 (33%)]\tLoss: 0.916657\n",
      "Train Epoch: 3 [880/2566 (34%)]\tLoss: 0.944963\n",
      "Train Epoch: 3 [920/2566 (36%)]\tLoss: 1.077985\n",
      "Train Epoch: 3 [960/2566 (37%)]\tLoss: 0.527668\n",
      "Train Epoch: 3 [1000/2566 (39%)]\tLoss: 0.737190\n",
      "Train Epoch: 3 [1040/2566 (40%)]\tLoss: 0.551734\n",
      "Train Epoch: 3 [1080/2566 (42%)]\tLoss: 1.557141\n",
      "Train Epoch: 3 [1120/2566 (44%)]\tLoss: 1.094947\n",
      "Train Epoch: 3 [1160/2566 (45%)]\tLoss: 0.806295\n",
      "Train Epoch: 3 [1200/2566 (47%)]\tLoss: 0.497587\n",
      "Train Epoch: 3 [1240/2566 (48%)]\tLoss: 0.556465\n",
      "Train Epoch: 3 [1280/2566 (50%)]\tLoss: 0.395560\n",
      "Train Epoch: 3 [1320/2566 (51%)]\tLoss: 0.451267\n",
      "Train Epoch: 3 [1360/2566 (53%)]\tLoss: 0.626606\n",
      "Train Epoch: 3 [1400/2566 (55%)]\tLoss: 0.549848\n",
      "Train Epoch: 3 [1440/2566 (56%)]\tLoss: 1.056255\n",
      "Train Epoch: 3 [1480/2566 (58%)]\tLoss: 0.592035\n",
      "Train Epoch: 3 [1520/2566 (59%)]\tLoss: 0.609492\n",
      "Train Epoch: 3 [1560/2566 (61%)]\tLoss: 0.645900\n",
      "Train Epoch: 3 [1600/2566 (62%)]\tLoss: 1.216186\n",
      "Train Epoch: 3 [1640/2566 (64%)]\tLoss: 0.730270\n",
      "Train Epoch: 3 [1680/2566 (65%)]\tLoss: 0.672182\n",
      "Train Epoch: 3 [1720/2566 (67%)]\tLoss: 0.957255\n",
      "Train Epoch: 3 [1760/2566 (69%)]\tLoss: 0.705142\n",
      "Train Epoch: 3 [1800/2566 (70%)]\tLoss: 0.369491\n",
      "Train Epoch: 3 [1840/2566 (72%)]\tLoss: 0.806093\n",
      "Train Epoch: 3 [1880/2566 (73%)]\tLoss: 1.337852\n",
      "Train Epoch: 3 [1920/2566 (75%)]\tLoss: 0.711694\n",
      "Train Epoch: 3 [1960/2566 (76%)]\tLoss: 0.833235\n",
      "Train Epoch: 3 [2000/2566 (78%)]\tLoss: 0.768921\n",
      "Train Epoch: 3 [2040/2566 (79%)]\tLoss: 0.845301\n",
      "Train Epoch: 3 [2080/2566 (81%)]\tLoss: 0.978528\n",
      "Train Epoch: 3 [2120/2566 (83%)]\tLoss: 0.357968\n",
      "Train Epoch: 3 [2160/2566 (84%)]\tLoss: 0.932504\n",
      "Train Epoch: 3 [2200/2566 (86%)]\tLoss: 1.131116\n",
      "Train Epoch: 3 [2240/2566 (87%)]\tLoss: 0.892825\n",
      "Train Epoch: 3 [2280/2566 (89%)]\tLoss: 0.755310\n",
      "Train Epoch: 3 [2320/2566 (90%)]\tLoss: 1.929886\n",
      "Train Epoch: 3 [2360/2566 (92%)]\tLoss: 0.662838\n",
      "Train Epoch: 3 [2400/2566 (93%)]\tLoss: 0.999455\n",
      "Train Epoch: 3 [2440/2566 (95%)]\tLoss: 0.719776\n",
      "Train Epoch: 3 [2480/2566 (97%)]\tLoss: 0.252975\n",
      "Train Epoch: 3 [2520/2566 (98%)]\tLoss: 0.967776\n",
      "Train Epoch: 3 [2560/2566 (100%)]\tLoss: 0.254495\n",
      "epoch:3,loss:0.7769930177017165\n",
      "Train set: Average loss: 0.7495, Accuracy: 1880/2566 (73%)\n",
      "Val set: Average loss: 0.7656, Accuracy: 244/327 (75%)\n",
      "Train Epoch: 4 [40/2566 (2%)]\tLoss: 0.453960\n",
      "Train Epoch: 4 [80/2566 (3%)]\tLoss: 0.232872\n",
      "Train Epoch: 4 [120/2566 (5%)]\tLoss: 0.345971\n",
      "Train Epoch: 4 [160/2566 (6%)]\tLoss: 0.460756\n",
      "Train Epoch: 4 [200/2566 (8%)]\tLoss: 0.708977\n",
      "Train Epoch: 4 [240/2566 (9%)]\tLoss: 0.959016\n",
      "Train Epoch: 4 [280/2566 (11%)]\tLoss: 0.612863\n",
      "Train Epoch: 4 [320/2566 (12%)]\tLoss: 0.632044\n",
      "Train Epoch: 4 [360/2566 (14%)]\tLoss: 0.337490\n",
      "Train Epoch: 4 [400/2566 (16%)]\tLoss: 0.924092\n",
      "Train Epoch: 4 [440/2566 (17%)]\tLoss: 0.502098\n",
      "Train Epoch: 4 [480/2566 (19%)]\tLoss: 0.497743\n",
      "Train Epoch: 4 [520/2566 (20%)]\tLoss: 0.693658\n",
      "Train Epoch: 4 [560/2566 (22%)]\tLoss: 0.823815\n",
      "Train Epoch: 4 [600/2566 (23%)]\tLoss: 0.490050\n",
      "Train Epoch: 4 [640/2566 (25%)]\tLoss: 0.785507\n",
      "Train Epoch: 4 [680/2566 (26%)]\tLoss: 0.312499\n",
      "Train Epoch: 4 [720/2566 (28%)]\tLoss: 1.494799\n",
      "Train Epoch: 4 [760/2566 (30%)]\tLoss: 1.083801\n",
      "Train Epoch: 4 [800/2566 (31%)]\tLoss: 0.623433\n",
      "Train Epoch: 4 [840/2566 (33%)]\tLoss: 0.756198\n",
      "Train Epoch: 4 [880/2566 (34%)]\tLoss: 1.536262\n",
      "Train Epoch: 4 [920/2566 (36%)]\tLoss: 1.416649\n",
      "Train Epoch: 4 [960/2566 (37%)]\tLoss: 0.984981\n",
      "Train Epoch: 4 [1000/2566 (39%)]\tLoss: 0.971167\n",
      "Train Epoch: 4 [1040/2566 (40%)]\tLoss: 0.888449\n",
      "Train Epoch: 4 [1080/2566 (42%)]\tLoss: 0.272333\n",
      "Train Epoch: 4 [1120/2566 (44%)]\tLoss: 0.698484\n",
      "Train Epoch: 4 [1160/2566 (45%)]\tLoss: 0.469126\n",
      "Train Epoch: 4 [1200/2566 (47%)]\tLoss: 0.451798\n",
      "Train Epoch: 4 [1240/2566 (48%)]\tLoss: 1.476640\n",
      "Train Epoch: 4 [1280/2566 (50%)]\tLoss: 0.798706\n",
      "Train Epoch: 4 [1320/2566 (51%)]\tLoss: 0.613531\n",
      "Train Epoch: 4 [1360/2566 (53%)]\tLoss: 0.351683\n",
      "Train Epoch: 4 [1400/2566 (55%)]\tLoss: 0.529273\n",
      "Train Epoch: 4 [1440/2566 (56%)]\tLoss: 0.535911\n",
      "Train Epoch: 4 [1480/2566 (58%)]\tLoss: 0.194568\n",
      "Train Epoch: 4 [1520/2566 (59%)]\tLoss: 1.020128\n",
      "Train Epoch: 4 [1560/2566 (61%)]\tLoss: 0.609933\n",
      "Train Epoch: 4 [1600/2566 (62%)]\tLoss: 0.937326\n",
      "Train Epoch: 4 [1640/2566 (64%)]\tLoss: 1.187510\n",
      "Train Epoch: 4 [1680/2566 (65%)]\tLoss: 0.465047\n",
      "Train Epoch: 4 [1720/2566 (67%)]\tLoss: 0.559898\n",
      "Train Epoch: 4 [1760/2566 (69%)]\tLoss: 0.379288\n",
      "Train Epoch: 4 [1800/2566 (70%)]\tLoss: 0.715690\n",
      "Train Epoch: 4 [1840/2566 (72%)]\tLoss: 0.991371\n",
      "Train Epoch: 4 [1880/2566 (73%)]\tLoss: 1.247726\n",
      "Train Epoch: 4 [1920/2566 (75%)]\tLoss: 1.344100\n",
      "Train Epoch: 4 [1960/2566 (76%)]\tLoss: 0.822505\n",
      "Train Epoch: 4 [2000/2566 (78%)]\tLoss: 0.400228\n",
      "Train Epoch: 4 [2040/2566 (79%)]\tLoss: 0.730466\n",
      "Train Epoch: 4 [2080/2566 (81%)]\tLoss: 1.196116\n",
      "Train Epoch: 4 [2120/2566 (83%)]\tLoss: 1.014454\n",
      "Train Epoch: 4 [2160/2566 (84%)]\tLoss: 0.495423\n",
      "Train Epoch: 4 [2200/2566 (86%)]\tLoss: 0.769846\n",
      "Train Epoch: 4 [2240/2566 (87%)]\tLoss: 1.302366\n",
      "Train Epoch: 4 [2280/2566 (89%)]\tLoss: 0.895546\n",
      "Train Epoch: 4 [2320/2566 (90%)]\tLoss: 0.813409\n",
      "Train Epoch: 4 [2360/2566 (92%)]\tLoss: 0.436792\n",
      "Train Epoch: 4 [2400/2566 (93%)]\tLoss: 0.937156\n",
      "Train Epoch: 4 [2440/2566 (95%)]\tLoss: 0.429288\n",
      "Train Epoch: 4 [2480/2566 (97%)]\tLoss: 0.290406\n",
      "Train Epoch: 4 [2520/2566 (98%)]\tLoss: 1.043655\n",
      "Train Epoch: 4 [2560/2566 (100%)]\tLoss: 0.619520\n",
      "epoch:4,loss:0.7516918193514102\n",
      "Train set: Average loss: 0.7077, Accuracy: 1894/2566 (74%)\n",
      "Val set: Average loss: 0.7265, Accuracy: 239/327 (73%)\n",
      "Train Epoch: 5 [40/2566 (2%)]\tLoss: 0.745936\n",
      "Train Epoch: 5 [80/2566 (3%)]\tLoss: 0.289158\n",
      "Train Epoch: 5 [120/2566 (5%)]\tLoss: 0.688602\n",
      "Train Epoch: 5 [160/2566 (6%)]\tLoss: 0.855290\n",
      "Train Epoch: 5 [200/2566 (8%)]\tLoss: 0.500968\n",
      "Train Epoch: 5 [240/2566 (9%)]\tLoss: 0.546296\n",
      "Train Epoch: 5 [280/2566 (11%)]\tLoss: 0.606237\n",
      "Train Epoch: 5 [320/2566 (12%)]\tLoss: 0.479790\n",
      "Train Epoch: 5 [360/2566 (14%)]\tLoss: 0.482218\n",
      "Train Epoch: 5 [400/2566 (16%)]\tLoss: 0.655685\n",
      "Train Epoch: 5 [440/2566 (17%)]\tLoss: 0.523582\n",
      "Train Epoch: 5 [480/2566 (19%)]\tLoss: 0.562149\n",
      "Train Epoch: 5 [520/2566 (20%)]\tLoss: 0.582179\n",
      "Train Epoch: 5 [560/2566 (22%)]\tLoss: 0.409016\n",
      "Train Epoch: 5 [600/2566 (23%)]\tLoss: 0.901207\n",
      "Train Epoch: 5 [640/2566 (25%)]\tLoss: 1.012879\n",
      "Train Epoch: 5 [680/2566 (26%)]\tLoss: 1.127539\n",
      "Train Epoch: 5 [720/2566 (28%)]\tLoss: 0.944969\n",
      "Train Epoch: 5 [760/2566 (30%)]\tLoss: 0.705978\n",
      "Train Epoch: 5 [800/2566 (31%)]\tLoss: 0.867625\n",
      "Train Epoch: 5 [840/2566 (33%)]\tLoss: 0.580511\n",
      "Train Epoch: 5 [880/2566 (34%)]\tLoss: 0.894641\n",
      "Train Epoch: 5 [920/2566 (36%)]\tLoss: 0.335230\n",
      "Train Epoch: 5 [960/2566 (37%)]\tLoss: 0.801917\n",
      "Train Epoch: 5 [1000/2566 (39%)]\tLoss: 0.656915\n",
      "Train Epoch: 5 [1040/2566 (40%)]\tLoss: 0.689931\n",
      "Train Epoch: 5 [1080/2566 (42%)]\tLoss: 0.841920\n",
      "Train Epoch: 5 [1120/2566 (44%)]\tLoss: 0.420701\n",
      "Train Epoch: 5 [1160/2566 (45%)]\tLoss: 0.197585\n",
      "Train Epoch: 5 [1200/2566 (47%)]\tLoss: 1.481708\n",
      "Train Epoch: 5 [1240/2566 (48%)]\tLoss: 0.263706\n",
      "Train Epoch: 5 [1280/2566 (50%)]\tLoss: 1.323105\n",
      "Train Epoch: 5 [1320/2566 (51%)]\tLoss: 0.280997\n",
      "Train Epoch: 5 [1360/2566 (53%)]\tLoss: 0.836821\n",
      "Train Epoch: 5 [1400/2566 (55%)]\tLoss: 0.951972\n",
      "Train Epoch: 5 [1440/2566 (56%)]\tLoss: 0.941320\n",
      "Train Epoch: 5 [1480/2566 (58%)]\tLoss: 1.391652\n",
      "Train Epoch: 5 [1520/2566 (59%)]\tLoss: 1.464946\n",
      "Train Epoch: 5 [1560/2566 (61%)]\tLoss: 1.252575\n",
      "Train Epoch: 5 [1600/2566 (62%)]\tLoss: 0.647542\n",
      "Train Epoch: 5 [1640/2566 (64%)]\tLoss: 1.035352\n",
      "Train Epoch: 5 [1680/2566 (65%)]\tLoss: 0.801194\n",
      "Train Epoch: 5 [1720/2566 (67%)]\tLoss: 1.045015\n",
      "Train Epoch: 5 [1760/2566 (69%)]\tLoss: 0.569092\n",
      "Train Epoch: 5 [1800/2566 (70%)]\tLoss: 0.184413\n",
      "Train Epoch: 5 [1840/2566 (72%)]\tLoss: 1.268256\n",
      "Train Epoch: 5 [1880/2566 (73%)]\tLoss: 0.868006\n",
      "Train Epoch: 5 [1920/2566 (75%)]\tLoss: 0.980072\n",
      "Train Epoch: 5 [1960/2566 (76%)]\tLoss: 0.675053\n",
      "Train Epoch: 5 [2000/2566 (78%)]\tLoss: 0.693922\n",
      "Train Epoch: 5 [2040/2566 (79%)]\tLoss: 0.224277\n",
      "Train Epoch: 5 [2080/2566 (81%)]\tLoss: 0.601678\n",
      "Train Epoch: 5 [2120/2566 (83%)]\tLoss: 0.950404\n",
      "Train Epoch: 5 [2160/2566 (84%)]\tLoss: 0.932797\n",
      "Train Epoch: 5 [2200/2566 (86%)]\tLoss: 0.422043\n",
      "Train Epoch: 5 [2240/2566 (87%)]\tLoss: 1.858348\n",
      "Train Epoch: 5 [2280/2566 (89%)]\tLoss: 0.909489\n",
      "Train Epoch: 5 [2320/2566 (90%)]\tLoss: 0.844854\n",
      "Train Epoch: 5 [2360/2566 (92%)]\tLoss: 0.672434\n",
      "Train Epoch: 5 [2400/2566 (93%)]\tLoss: 1.126170\n",
      "Train Epoch: 5 [2440/2566 (95%)]\tLoss: 0.942887\n",
      "Train Epoch: 5 [2480/2566 (97%)]\tLoss: 0.284137\n",
      "Train Epoch: 5 [2520/2566 (98%)]\tLoss: 0.971362\n",
      "Train Epoch: 5 [2560/2566 (100%)]\tLoss: 1.117082\n",
      "epoch:5,loss:0.7365712287641388\n",
      "Train set: Average loss: 0.7063, Accuracy: 1901/2566 (74%)\n",
      "Val set: Average loss: 0.7394, Accuracy: 236/327 (72%)\n",
      "Train Epoch: 6 [40/2566 (2%)]\tLoss: 0.760968\n",
      "Train Epoch: 6 [80/2566 (3%)]\tLoss: 0.532229\n",
      "Train Epoch: 6 [120/2566 (5%)]\tLoss: 0.871059\n",
      "Train Epoch: 6 [160/2566 (6%)]\tLoss: 0.466602\n",
      "Train Epoch: 6 [200/2566 (8%)]\tLoss: 0.866162\n",
      "Train Epoch: 6 [240/2566 (9%)]\tLoss: 0.736886\n",
      "Train Epoch: 6 [280/2566 (11%)]\tLoss: 0.323476\n",
      "Train Epoch: 6 [320/2566 (12%)]\tLoss: 0.679871\n",
      "Train Epoch: 6 [360/2566 (14%)]\tLoss: 0.996191\n",
      "Train Epoch: 6 [400/2566 (16%)]\tLoss: 0.349681\n",
      "Train Epoch: 6 [440/2566 (17%)]\tLoss: 0.744006\n",
      "Train Epoch: 6 [480/2566 (19%)]\tLoss: 0.643970\n",
      "Train Epoch: 6 [520/2566 (20%)]\tLoss: 0.801982\n",
      "Train Epoch: 6 [560/2566 (22%)]\tLoss: 0.505429\n",
      "Train Epoch: 6 [600/2566 (23%)]\tLoss: 0.977093\n",
      "Train Epoch: 6 [640/2566 (25%)]\tLoss: 0.386726\n",
      "Train Epoch: 6 [680/2566 (26%)]\tLoss: 0.662930\n",
      "Train Epoch: 6 [720/2566 (28%)]\tLoss: 0.286125\n",
      "Train Epoch: 6 [760/2566 (30%)]\tLoss: 0.446480\n",
      "Train Epoch: 6 [800/2566 (31%)]\tLoss: 1.190878\n",
      "Train Epoch: 6 [840/2566 (33%)]\tLoss: 0.630678\n",
      "Train Epoch: 6 [880/2566 (34%)]\tLoss: 0.403895\n",
      "Train Epoch: 6 [920/2566 (36%)]\tLoss: 0.362084\n",
      "Train Epoch: 6 [960/2566 (37%)]\tLoss: 0.424491\n",
      "Train Epoch: 6 [1000/2566 (39%)]\tLoss: 0.481774\n",
      "Train Epoch: 6 [1040/2566 (40%)]\tLoss: 0.679976\n",
      "Train Epoch: 6 [1080/2566 (42%)]\tLoss: 0.458839\n",
      "Train Epoch: 6 [1120/2566 (44%)]\tLoss: 0.534106\n",
      "Train Epoch: 6 [1160/2566 (45%)]\tLoss: 0.870887\n",
      "Train Epoch: 6 [1200/2566 (47%)]\tLoss: 0.967597\n",
      "Train Epoch: 6 [1240/2566 (48%)]\tLoss: 0.514401\n",
      "Train Epoch: 6 [1280/2566 (50%)]\tLoss: 0.479858\n",
      "Train Epoch: 6 [1320/2566 (51%)]\tLoss: 1.210592\n",
      "Train Epoch: 6 [1360/2566 (53%)]\tLoss: 0.927657\n",
      "Train Epoch: 6 [1400/2566 (55%)]\tLoss: 0.756593\n",
      "Train Epoch: 6 [1440/2566 (56%)]\tLoss: 0.394411\n",
      "Train Epoch: 6 [1480/2566 (58%)]\tLoss: 0.495756\n",
      "Train Epoch: 6 [1520/2566 (59%)]\tLoss: 0.680083\n",
      "Train Epoch: 6 [1560/2566 (61%)]\tLoss: 0.882089\n",
      "Train Epoch: 6 [1600/2566 (62%)]\tLoss: 0.636617\n",
      "Train Epoch: 6 [1640/2566 (64%)]\tLoss: 0.837945\n",
      "Train Epoch: 6 [1680/2566 (65%)]\tLoss: 0.729210\n",
      "Train Epoch: 6 [1720/2566 (67%)]\tLoss: 0.591933\n",
      "Train Epoch: 6 [1760/2566 (69%)]\tLoss: 0.222124\n",
      "Train Epoch: 6 [1800/2566 (70%)]\tLoss: 0.977292\n",
      "Train Epoch: 6 [1840/2566 (72%)]\tLoss: 0.774113\n",
      "Train Epoch: 6 [1880/2566 (73%)]\tLoss: 0.404205\n",
      "Train Epoch: 6 [1920/2566 (75%)]\tLoss: 0.186033\n",
      "Train Epoch: 6 [1960/2566 (76%)]\tLoss: 0.411814\n",
      "Train Epoch: 6 [2000/2566 (78%)]\tLoss: 0.920070\n",
      "Train Epoch: 6 [2040/2566 (79%)]\tLoss: 1.060977\n",
      "Train Epoch: 6 [2080/2566 (81%)]\tLoss: 1.041390\n",
      "Train Epoch: 6 [2120/2566 (83%)]\tLoss: 0.423746\n",
      "Train Epoch: 6 [2160/2566 (84%)]\tLoss: 0.559914\n",
      "Train Epoch: 6 [2200/2566 (86%)]\tLoss: 0.719210\n",
      "Train Epoch: 6 [2240/2566 (87%)]\tLoss: 0.454338\n",
      "Train Epoch: 6 [2280/2566 (89%)]\tLoss: 0.863121\n",
      "Train Epoch: 6 [2320/2566 (90%)]\tLoss: 0.711069\n",
      "Train Epoch: 6 [2360/2566 (92%)]\tLoss: 0.718615\n",
      "Train Epoch: 6 [2400/2566 (93%)]\tLoss: 0.457255\n",
      "Train Epoch: 6 [2440/2566 (95%)]\tLoss: 0.539480\n",
      "Train Epoch: 6 [2480/2566 (97%)]\tLoss: 0.432126\n",
      "Train Epoch: 6 [2520/2566 (98%)]\tLoss: 0.562174\n",
      "Train Epoch: 6 [2560/2566 (100%)]\tLoss: 1.592870\n",
      "epoch:6,loss:0.7281502902368518\n",
      "Train set: Average loss: 0.7276, Accuracy: 1913/2566 (75%)\n",
      "Val set: Average loss: 0.7645, Accuracy: 243/327 (74%)\n",
      "Train Epoch: 7 [40/2566 (2%)]\tLoss: 0.712107\n",
      "Train Epoch: 7 [80/2566 (3%)]\tLoss: 0.862407\n",
      "Train Epoch: 7 [120/2566 (5%)]\tLoss: 0.466282\n",
      "Train Epoch: 7 [160/2566 (6%)]\tLoss: 0.930976\n",
      "Train Epoch: 7 [200/2566 (8%)]\tLoss: 0.373969\n",
      "Train Epoch: 7 [240/2566 (9%)]\tLoss: 1.349831\n",
      "Train Epoch: 7 [280/2566 (11%)]\tLoss: 0.823204\n",
      "Train Epoch: 7 [320/2566 (12%)]\tLoss: 0.681766\n",
      "Train Epoch: 7 [360/2566 (14%)]\tLoss: 0.679585\n",
      "Train Epoch: 7 [400/2566 (16%)]\tLoss: 1.794572\n",
      "Train Epoch: 7 [440/2566 (17%)]\tLoss: 0.937791\n",
      "Train Epoch: 7 [480/2566 (19%)]\tLoss: 0.555809\n",
      "Train Epoch: 7 [520/2566 (20%)]\tLoss: 0.429791\n",
      "Train Epoch: 7 [560/2566 (22%)]\tLoss: 0.498714\n",
      "Train Epoch: 7 [600/2566 (23%)]\tLoss: 0.828154\n",
      "Train Epoch: 7 [640/2566 (25%)]\tLoss: 0.806966\n",
      "Train Epoch: 7 [680/2566 (26%)]\tLoss: 0.810939\n",
      "Train Epoch: 7 [720/2566 (28%)]\tLoss: 0.388946\n",
      "Train Epoch: 7 [760/2566 (30%)]\tLoss: 1.268126\n",
      "Train Epoch: 7 [800/2566 (31%)]\tLoss: 1.323479\n",
      "Train Epoch: 7 [840/2566 (33%)]\tLoss: 0.958140\n",
      "Train Epoch: 7 [880/2566 (34%)]\tLoss: 0.702194\n",
      "Train Epoch: 7 [920/2566 (36%)]\tLoss: 0.592174\n",
      "Train Epoch: 7 [960/2566 (37%)]\tLoss: 0.872178\n",
      "Train Epoch: 7 [1000/2566 (39%)]\tLoss: 0.718394\n",
      "Train Epoch: 7 [1040/2566 (40%)]\tLoss: 0.555161\n",
      "Train Epoch: 7 [1080/2566 (42%)]\tLoss: 0.279683\n",
      "Train Epoch: 7 [1120/2566 (44%)]\tLoss: 0.611170\n",
      "Train Epoch: 7 [1160/2566 (45%)]\tLoss: 1.591850\n",
      "Train Epoch: 7 [1200/2566 (47%)]\tLoss: 0.741047\n",
      "Train Epoch: 7 [1240/2566 (48%)]\tLoss: 0.944695\n",
      "Train Epoch: 7 [1280/2566 (50%)]\tLoss: 0.320810\n",
      "Train Epoch: 7 [1320/2566 (51%)]\tLoss: 0.783293\n",
      "Train Epoch: 7 [1360/2566 (53%)]\tLoss: 1.078541\n",
      "Train Epoch: 7 [1400/2566 (55%)]\tLoss: 0.777419\n",
      "Train Epoch: 7 [1440/2566 (56%)]\tLoss: 1.032150\n",
      "Train Epoch: 7 [1480/2566 (58%)]\tLoss: 0.528213\n",
      "Train Epoch: 7 [1520/2566 (59%)]\tLoss: 0.504324\n",
      "Train Epoch: 7 [1560/2566 (61%)]\tLoss: 0.762909\n",
      "Train Epoch: 7 [1600/2566 (62%)]\tLoss: 0.310539\n",
      "Train Epoch: 7 [1640/2566 (64%)]\tLoss: 0.650119\n",
      "Train Epoch: 7 [1680/2566 (65%)]\tLoss: 0.796086\n",
      "Train Epoch: 7 [1720/2566 (67%)]\tLoss: 1.042854\n",
      "Train Epoch: 7 [1760/2566 (69%)]\tLoss: 0.358494\n",
      "Train Epoch: 7 [1800/2566 (70%)]\tLoss: 0.948082\n",
      "Train Epoch: 7 [1840/2566 (72%)]\tLoss: 1.490166\n",
      "Train Epoch: 7 [1880/2566 (73%)]\tLoss: 0.603198\n",
      "Train Epoch: 7 [1920/2566 (75%)]\tLoss: 0.584228\n",
      "Train Epoch: 7 [1960/2566 (76%)]\tLoss: 0.561826\n",
      "Train Epoch: 7 [2000/2566 (78%)]\tLoss: 0.768578\n",
      "Train Epoch: 7 [2040/2566 (79%)]\tLoss: 0.705543\n",
      "Train Epoch: 7 [2080/2566 (81%)]\tLoss: 0.527903\n",
      "Train Epoch: 7 [2120/2566 (83%)]\tLoss: 0.907542\n",
      "Train Epoch: 7 [2160/2566 (84%)]\tLoss: 1.419805\n",
      "Train Epoch: 7 [2200/2566 (86%)]\tLoss: 0.940782\n",
      "Train Epoch: 7 [2240/2566 (87%)]\tLoss: 0.658864\n",
      "Train Epoch: 7 [2280/2566 (89%)]\tLoss: 0.837961\n",
      "Train Epoch: 7 [2320/2566 (90%)]\tLoss: 0.740175\n",
      "Train Epoch: 7 [2360/2566 (92%)]\tLoss: 1.094634\n",
      "Train Epoch: 7 [2400/2566 (93%)]\tLoss: 0.863162\n",
      "Train Epoch: 7 [2440/2566 (95%)]\tLoss: 0.953599\n",
      "Train Epoch: 7 [2480/2566 (97%)]\tLoss: 0.430959\n",
      "Train Epoch: 7 [2520/2566 (98%)]\tLoss: 0.820315\n",
      "Train Epoch: 7 [2560/2566 (100%)]\tLoss: 0.751868\n",
      "epoch:7,loss:0.7150925780846694\n",
      "Train set: Average loss: 0.6945, Accuracy: 1931/2566 (75%)\n",
      "Val set: Average loss: 0.7468, Accuracy: 238/327 (73%)\n",
      "Train Epoch: 8 [40/2566 (2%)]\tLoss: 0.273097\n",
      "Train Epoch: 8 [80/2566 (3%)]\tLoss: 0.896704\n",
      "Train Epoch: 8 [120/2566 (5%)]\tLoss: 0.326579\n",
      "Train Epoch: 8 [160/2566 (6%)]\tLoss: 0.506904\n",
      "Train Epoch: 8 [200/2566 (8%)]\tLoss: 1.068157\n",
      "Train Epoch: 8 [240/2566 (9%)]\tLoss: 0.301697\n",
      "Train Epoch: 8 [280/2566 (11%)]\tLoss: 0.563593\n",
      "Train Epoch: 8 [320/2566 (12%)]\tLoss: 0.285435\n",
      "Train Epoch: 8 [360/2566 (14%)]\tLoss: 0.278554\n",
      "Train Epoch: 8 [400/2566 (16%)]\tLoss: 0.915230\n",
      "Train Epoch: 8 [440/2566 (17%)]\tLoss: 0.863071\n",
      "Train Epoch: 8 [480/2566 (19%)]\tLoss: 1.088935\n",
      "Train Epoch: 8 [520/2566 (20%)]\tLoss: 1.030013\n",
      "Train Epoch: 8 [560/2566 (22%)]\tLoss: 0.432274\n",
      "Train Epoch: 8 [600/2566 (23%)]\tLoss: 0.675115\n",
      "Train Epoch: 8 [640/2566 (25%)]\tLoss: 0.147990\n",
      "Train Epoch: 8 [680/2566 (26%)]\tLoss: 1.261867\n",
      "Train Epoch: 8 [720/2566 (28%)]\tLoss: 0.315049\n",
      "Train Epoch: 8 [760/2566 (30%)]\tLoss: 0.598036\n",
      "Train Epoch: 8 [800/2566 (31%)]\tLoss: 0.853068\n",
      "Train Epoch: 8 [840/2566 (33%)]\tLoss: 0.814776\n",
      "Train Epoch: 8 [880/2566 (34%)]\tLoss: 0.650749\n",
      "Train Epoch: 8 [920/2566 (36%)]\tLoss: 0.406413\n",
      "Train Epoch: 8 [960/2566 (37%)]\tLoss: 0.486867\n",
      "Train Epoch: 8 [1000/2566 (39%)]\tLoss: 0.700393\n",
      "Train Epoch: 8 [1040/2566 (40%)]\tLoss: 0.158983\n",
      "Train Epoch: 8 [1080/2566 (42%)]\tLoss: 0.593655\n",
      "Train Epoch: 8 [1120/2566 (44%)]\tLoss: 0.710228\n",
      "Train Epoch: 8 [1160/2566 (45%)]\tLoss: 1.450953\n",
      "Train Epoch: 8 [1200/2566 (47%)]\tLoss: 0.279848\n",
      "Train Epoch: 8 [1240/2566 (48%)]\tLoss: 0.434295\n",
      "Train Epoch: 8 [1280/2566 (50%)]\tLoss: 0.629146\n",
      "Train Epoch: 8 [1320/2566 (51%)]\tLoss: 1.459609\n",
      "Train Epoch: 8 [1360/2566 (53%)]\tLoss: 0.381709\n",
      "Train Epoch: 8 [1400/2566 (55%)]\tLoss: 0.418624\n",
      "Train Epoch: 8 [1440/2566 (56%)]\tLoss: 0.718426\n",
      "Train Epoch: 8 [1480/2566 (58%)]\tLoss: 0.171597\n",
      "Train Epoch: 8 [1520/2566 (59%)]\tLoss: 0.680788\n",
      "Train Epoch: 8 [1560/2566 (61%)]\tLoss: 0.636062\n",
      "Train Epoch: 8 [1600/2566 (62%)]\tLoss: 0.328426\n",
      "Train Epoch: 8 [1640/2566 (64%)]\tLoss: 0.632693\n",
      "Train Epoch: 8 [1680/2566 (65%)]\tLoss: 1.811587\n",
      "Train Epoch: 8 [1720/2566 (67%)]\tLoss: 1.000794\n",
      "Train Epoch: 8 [1760/2566 (69%)]\tLoss: 0.986457\n",
      "Train Epoch: 8 [1800/2566 (70%)]\tLoss: 0.701545\n",
      "Train Epoch: 8 [1840/2566 (72%)]\tLoss: 0.956407\n",
      "Train Epoch: 8 [1880/2566 (73%)]\tLoss: 0.921386\n",
      "Train Epoch: 8 [1920/2566 (75%)]\tLoss: 0.700756\n",
      "Train Epoch: 8 [1960/2566 (76%)]\tLoss: 0.419160\n",
      "Train Epoch: 8 [2000/2566 (78%)]\tLoss: 0.871602\n",
      "Train Epoch: 8 [2040/2566 (79%)]\tLoss: 0.906961\n",
      "Train Epoch: 8 [2080/2566 (81%)]\tLoss: 0.624241\n",
      "Train Epoch: 8 [2120/2566 (83%)]\tLoss: 1.014050\n",
      "Train Epoch: 8 [2160/2566 (84%)]\tLoss: 0.928075\n",
      "Train Epoch: 8 [2200/2566 (86%)]\tLoss: 0.863843\n",
      "Train Epoch: 8 [2240/2566 (87%)]\tLoss: 0.422919\n",
      "Train Epoch: 8 [2280/2566 (89%)]\tLoss: 0.870253\n",
      "Train Epoch: 8 [2320/2566 (90%)]\tLoss: 1.139326\n",
      "Train Epoch: 8 [2360/2566 (92%)]\tLoss: 0.479280\n",
      "Train Epoch: 8 [2400/2566 (93%)]\tLoss: 0.530294\n",
      "Train Epoch: 8 [2440/2566 (95%)]\tLoss: 1.209950\n",
      "Train Epoch: 8 [2480/2566 (97%)]\tLoss: 0.594869\n",
      "Train Epoch: 8 [2520/2566 (98%)]\tLoss: 0.537002\n",
      "Train Epoch: 8 [2560/2566 (100%)]\tLoss: 0.569926\n",
      "epoch:8,loss:0.69830536703083\n",
      "Train set: Average loss: 0.6618, Accuracy: 1957/2566 (76%)\n",
      "Val set: Average loss: 0.6854, Accuracy: 242/327 (74%)\n",
      "Train Epoch: 9 [40/2566 (2%)]\tLoss: 0.755852\n",
      "Train Epoch: 9 [80/2566 (3%)]\tLoss: 1.307726\n",
      "Train Epoch: 9 [120/2566 (5%)]\tLoss: 0.696623\n",
      "Train Epoch: 9 [160/2566 (6%)]\tLoss: 0.608342\n",
      "Train Epoch: 9 [200/2566 (8%)]\tLoss: 0.208647\n",
      "Train Epoch: 9 [240/2566 (9%)]\tLoss: 0.877025\n",
      "Train Epoch: 9 [280/2566 (11%)]\tLoss: 1.236035\n",
      "Train Epoch: 9 [320/2566 (12%)]\tLoss: 0.508203\n",
      "Train Epoch: 9 [360/2566 (14%)]\tLoss: 0.701920\n",
      "Train Epoch: 9 [400/2566 (16%)]\tLoss: 0.698929\n",
      "Train Epoch: 9 [440/2566 (17%)]\tLoss: 0.767382\n",
      "Train Epoch: 9 [480/2566 (19%)]\tLoss: 0.527835\n",
      "Train Epoch: 9 [520/2566 (20%)]\tLoss: 0.308102\n",
      "Train Epoch: 9 [560/2566 (22%)]\tLoss: 1.461988\n",
      "Train Epoch: 9 [600/2566 (23%)]\tLoss: 0.223451\n",
      "Train Epoch: 9 [640/2566 (25%)]\tLoss: 0.365775\n",
      "Train Epoch: 9 [680/2566 (26%)]\tLoss: 1.791778\n",
      "Train Epoch: 9 [720/2566 (28%)]\tLoss: 0.562501\n",
      "Train Epoch: 9 [760/2566 (30%)]\tLoss: 0.527875\n",
      "Train Epoch: 9 [800/2566 (31%)]\tLoss: 1.046807\n",
      "Train Epoch: 9 [840/2566 (33%)]\tLoss: 0.490525\n",
      "Train Epoch: 9 [880/2566 (34%)]\tLoss: 0.580878\n",
      "Train Epoch: 9 [920/2566 (36%)]\tLoss: 0.366391\n",
      "Train Epoch: 9 [960/2566 (37%)]\tLoss: 0.646566\n",
      "Train Epoch: 9 [1000/2566 (39%)]\tLoss: 0.877327\n",
      "Train Epoch: 9 [1040/2566 (40%)]\tLoss: 1.369151\n",
      "Train Epoch: 9 [1080/2566 (42%)]\tLoss: 0.669045\n",
      "Train Epoch: 9 [1120/2566 (44%)]\tLoss: 0.545195\n",
      "Train Epoch: 9 [1160/2566 (45%)]\tLoss: 0.489145\n",
      "Train Epoch: 9 [1200/2566 (47%)]\tLoss: 0.737352\n",
      "Train Epoch: 9 [1240/2566 (48%)]\tLoss: 0.618954\n",
      "Train Epoch: 9 [1280/2566 (50%)]\tLoss: 0.668653\n",
      "Train Epoch: 9 [1320/2566 (51%)]\tLoss: 1.445020\n",
      "Train Epoch: 9 [1360/2566 (53%)]\tLoss: 0.688773\n",
      "Train Epoch: 9 [1400/2566 (55%)]\tLoss: 0.684625\n",
      "Train Epoch: 9 [1440/2566 (56%)]\tLoss: 0.662556\n",
      "Train Epoch: 9 [1480/2566 (58%)]\tLoss: 0.965439\n",
      "Train Epoch: 9 [1520/2566 (59%)]\tLoss: 1.040707\n",
      "Train Epoch: 9 [1560/2566 (61%)]\tLoss: 0.831646\n",
      "Train Epoch: 9 [1600/2566 (62%)]\tLoss: 0.514861\n",
      "Train Epoch: 9 [1640/2566 (64%)]\tLoss: 0.881386\n",
      "Train Epoch: 9 [1680/2566 (65%)]\tLoss: 0.530853\n",
      "Train Epoch: 9 [1720/2566 (67%)]\tLoss: 0.702255\n",
      "Train Epoch: 9 [1760/2566 (69%)]\tLoss: 0.660211\n",
      "Train Epoch: 9 [1800/2566 (70%)]\tLoss: 0.517100\n",
      "Train Epoch: 9 [1840/2566 (72%)]\tLoss: 0.925012\n",
      "Train Epoch: 9 [1880/2566 (73%)]\tLoss: 1.088696\n",
      "Train Epoch: 9 [1920/2566 (75%)]\tLoss: 1.539705\n",
      "Train Epoch: 9 [1960/2566 (76%)]\tLoss: 0.577498\n",
      "Train Epoch: 9 [2000/2566 (78%)]\tLoss: 0.697847\n",
      "Train Epoch: 9 [2040/2566 (79%)]\tLoss: 0.842523\n",
      "Train Epoch: 9 [2080/2566 (81%)]\tLoss: 0.670205\n",
      "Train Epoch: 9 [2120/2566 (83%)]\tLoss: 0.863343\n",
      "Train Epoch: 9 [2160/2566 (84%)]\tLoss: 0.530603\n",
      "Train Epoch: 9 [2200/2566 (86%)]\tLoss: 0.412742\n",
      "Train Epoch: 9 [2240/2566 (87%)]\tLoss: 0.855833\n",
      "Train Epoch: 9 [2280/2566 (89%)]\tLoss: 0.491898\n",
      "Train Epoch: 9 [2320/2566 (90%)]\tLoss: 1.270703\n",
      "Train Epoch: 9 [2360/2566 (92%)]\tLoss: 0.285705\n",
      "Train Epoch: 9 [2400/2566 (93%)]\tLoss: 0.924253\n",
      "Train Epoch: 9 [2440/2566 (95%)]\tLoss: 0.524682\n",
      "Train Epoch: 9 [2480/2566 (97%)]\tLoss: 0.822015\n",
      "Train Epoch: 9 [2520/2566 (98%)]\tLoss: 0.816058\n",
      "Train Epoch: 9 [2560/2566 (100%)]\tLoss: 0.638569\n",
      "epoch:9,loss:0.6924494071355861\n",
      "Train set: Average loss: 0.6598, Accuracy: 1946/2566 (76%)\n",
      "Val set: Average loss: 0.7021, Accuracy: 238/327 (73%)\n",
      "Train Epoch: 10 [40/2566 (2%)]\tLoss: 0.837630\n",
      "Train Epoch: 10 [80/2566 (3%)]\tLoss: 0.841856\n",
      "Train Epoch: 10 [120/2566 (5%)]\tLoss: 0.662910\n",
      "Train Epoch: 10 [160/2566 (6%)]\tLoss: 0.152154\n",
      "Train Epoch: 10 [200/2566 (8%)]\tLoss: 0.598443\n",
      "Train Epoch: 10 [240/2566 (9%)]\tLoss: 0.625405\n",
      "Train Epoch: 10 [280/2566 (11%)]\tLoss: 0.847947\n",
      "Train Epoch: 10 [320/2566 (12%)]\tLoss: 0.656628\n",
      "Train Epoch: 10 [360/2566 (14%)]\tLoss: 1.023371\n",
      "Train Epoch: 10 [400/2566 (16%)]\tLoss: 1.245381\n",
      "Train Epoch: 10 [440/2566 (17%)]\tLoss: 0.928708\n",
      "Train Epoch: 10 [480/2566 (19%)]\tLoss: 0.606152\n",
      "Train Epoch: 10 [520/2566 (20%)]\tLoss: 0.680260\n",
      "Train Epoch: 10 [560/2566 (22%)]\tLoss: 0.520155\n",
      "Train Epoch: 10 [600/2566 (23%)]\tLoss: 0.294005\n",
      "Train Epoch: 10 [640/2566 (25%)]\tLoss: 0.137964\n",
      "Train Epoch: 10 [680/2566 (26%)]\tLoss: 0.630153\n",
      "Train Epoch: 10 [720/2566 (28%)]\tLoss: 0.784446\n",
      "Train Epoch: 10 [760/2566 (30%)]\tLoss: 1.049761\n",
      "Train Epoch: 10 [800/2566 (31%)]\tLoss: 1.088419\n",
      "Train Epoch: 10 [840/2566 (33%)]\tLoss: 0.442595\n",
      "Train Epoch: 10 [880/2566 (34%)]\tLoss: 0.207953\n",
      "Train Epoch: 10 [920/2566 (36%)]\tLoss: 0.518131\n",
      "Train Epoch: 10 [960/2566 (37%)]\tLoss: 0.097783\n",
      "Train Epoch: 10 [1000/2566 (39%)]\tLoss: 0.215342\n",
      "Train Epoch: 10 [1040/2566 (40%)]\tLoss: 0.368604\n",
      "Train Epoch: 10 [1080/2566 (42%)]\tLoss: 0.748566\n",
      "Train Epoch: 10 [1120/2566 (44%)]\tLoss: 0.451365\n",
      "Train Epoch: 10 [1160/2566 (45%)]\tLoss: 0.777411\n",
      "Train Epoch: 10 [1200/2566 (47%)]\tLoss: 0.928177\n",
      "Train Epoch: 10 [1240/2566 (48%)]\tLoss: 1.017936\n",
      "Train Epoch: 10 [1280/2566 (50%)]\tLoss: 0.327154\n",
      "Train Epoch: 10 [1320/2566 (51%)]\tLoss: 1.590316\n",
      "Train Epoch: 10 [1360/2566 (53%)]\tLoss: 0.462564\n",
      "Train Epoch: 10 [1400/2566 (55%)]\tLoss: 0.433432\n",
      "Train Epoch: 10 [1440/2566 (56%)]\tLoss: 0.275026\n",
      "Train Epoch: 10 [1480/2566 (58%)]\tLoss: 1.467013\n",
      "Train Epoch: 10 [1520/2566 (59%)]\tLoss: 0.935338\n",
      "Train Epoch: 10 [1560/2566 (61%)]\tLoss: 0.417693\n",
      "Train Epoch: 10 [1600/2566 (62%)]\tLoss: 0.717661\n",
      "Train Epoch: 10 [1640/2566 (64%)]\tLoss: 0.985509\n",
      "Train Epoch: 10 [1680/2566 (65%)]\tLoss: 0.895830\n",
      "Train Epoch: 10 [1720/2566 (67%)]\tLoss: 0.640875\n",
      "Train Epoch: 10 [1760/2566 (69%)]\tLoss: 0.964809\n",
      "Train Epoch: 10 [1800/2566 (70%)]\tLoss: 0.148590\n",
      "Train Epoch: 10 [1840/2566 (72%)]\tLoss: 1.066777\n",
      "Train Epoch: 10 [1880/2566 (73%)]\tLoss: 0.591912\n",
      "Train Epoch: 10 [1920/2566 (75%)]\tLoss: 0.386394\n",
      "Train Epoch: 10 [1960/2566 (76%)]\tLoss: 0.570320\n",
      "Train Epoch: 10 [2000/2566 (78%)]\tLoss: 0.749360\n",
      "Train Epoch: 10 [2040/2566 (79%)]\tLoss: 0.422801\n",
      "Train Epoch: 10 [2080/2566 (81%)]\tLoss: 0.212907\n",
      "Train Epoch: 10 [2120/2566 (83%)]\tLoss: 0.836579\n",
      "Train Epoch: 10 [2160/2566 (84%)]\tLoss: 0.271783\n",
      "Train Epoch: 10 [2200/2566 (86%)]\tLoss: 0.465744\n",
      "Train Epoch: 10 [2240/2566 (87%)]\tLoss: 0.408923\n",
      "Train Epoch: 10 [2280/2566 (89%)]\tLoss: 1.091461\n",
      "Train Epoch: 10 [2320/2566 (90%)]\tLoss: 0.339240\n",
      "Train Epoch: 10 [2360/2566 (92%)]\tLoss: 0.562844\n",
      "Train Epoch: 10 [2400/2566 (93%)]\tLoss: 0.654109\n",
      "Train Epoch: 10 [2440/2566 (95%)]\tLoss: 0.535193\n",
      "Train Epoch: 10 [2480/2566 (97%)]\tLoss: 0.926668\n",
      "Train Epoch: 10 [2520/2566 (98%)]\tLoss: 0.579815\n",
      "Train Epoch: 10 [2560/2566 (100%)]\tLoss: 1.101415\n",
      "epoch:10,loss:0.6852959470121288\n",
      "Train set: Average loss: 0.6952, Accuracy: 1965/2566 (77%)\n",
      "Val set: Average loss: 0.7418, Accuracy: 237/327 (72%)\n",
      "Train Epoch: 11 [40/2566 (2%)]\tLoss: 1.002149\n",
      "Train Epoch: 11 [80/2566 (3%)]\tLoss: 0.580075\n",
      "Train Epoch: 11 [120/2566 (5%)]\tLoss: 0.229258\n",
      "Train Epoch: 11 [160/2566 (6%)]\tLoss: 0.192761\n",
      "Train Epoch: 11 [200/2566 (8%)]\tLoss: 0.523919\n",
      "Train Epoch: 11 [240/2566 (9%)]\tLoss: 0.659193\n",
      "Train Epoch: 11 [280/2566 (11%)]\tLoss: 0.849016\n",
      "Train Epoch: 11 [320/2566 (12%)]\tLoss: 0.966044\n",
      "Train Epoch: 11 [360/2566 (14%)]\tLoss: 1.186976\n",
      "Train Epoch: 11 [400/2566 (16%)]\tLoss: 0.731739\n",
      "Train Epoch: 11 [440/2566 (17%)]\tLoss: 0.457355\n",
      "Train Epoch: 11 [480/2566 (19%)]\tLoss: 0.480689\n",
      "Train Epoch: 11 [520/2566 (20%)]\tLoss: 0.385861\n",
      "Train Epoch: 11 [560/2566 (22%)]\tLoss: 1.090390\n",
      "Train Epoch: 11 [600/2566 (23%)]\tLoss: 0.791298\n",
      "Train Epoch: 11 [640/2566 (25%)]\tLoss: 1.041922\n",
      "Train Epoch: 11 [680/2566 (26%)]\tLoss: 0.971068\n",
      "Train Epoch: 11 [720/2566 (28%)]\tLoss: 1.005903\n",
      "Train Epoch: 11 [760/2566 (30%)]\tLoss: 0.454143\n",
      "Train Epoch: 11 [800/2566 (31%)]\tLoss: 0.879597\n",
      "Train Epoch: 11 [840/2566 (33%)]\tLoss: 0.927707\n",
      "Train Epoch: 11 [880/2566 (34%)]\tLoss: 0.667275\n",
      "Train Epoch: 11 [920/2566 (36%)]\tLoss: 0.308481\n",
      "Train Epoch: 11 [960/2566 (37%)]\tLoss: 1.084075\n",
      "Train Epoch: 11 [1000/2566 (39%)]\tLoss: 0.483137\n",
      "Train Epoch: 11 [1040/2566 (40%)]\tLoss: 0.750765\n",
      "Train Epoch: 11 [1080/2566 (42%)]\tLoss: 0.204705\n",
      "Train Epoch: 11 [1120/2566 (44%)]\tLoss: 1.322536\n",
      "Train Epoch: 11 [1160/2566 (45%)]\tLoss: 0.921664\n",
      "Train Epoch: 11 [1200/2566 (47%)]\tLoss: 0.670755\n",
      "Train Epoch: 11 [1240/2566 (48%)]\tLoss: 0.243814\n",
      "Train Epoch: 11 [1280/2566 (50%)]\tLoss: 0.686153\n",
      "Train Epoch: 11 [1320/2566 (51%)]\tLoss: 0.444625\n",
      "Train Epoch: 11 [1360/2566 (53%)]\tLoss: 0.232694\n",
      "Train Epoch: 11 [1400/2566 (55%)]\tLoss: 0.473745\n",
      "Train Epoch: 11 [1440/2566 (56%)]\tLoss: 1.157405\n",
      "Train Epoch: 11 [1480/2566 (58%)]\tLoss: 0.449170\n",
      "Train Epoch: 11 [1520/2566 (59%)]\tLoss: 1.076952\n",
      "Train Epoch: 11 [1560/2566 (61%)]\tLoss: 0.666463\n",
      "Train Epoch: 11 [1600/2566 (62%)]\tLoss: 0.897766\n",
      "Train Epoch: 11 [1640/2566 (64%)]\tLoss: 0.363638\n",
      "Train Epoch: 11 [1680/2566 (65%)]\tLoss: 1.268063\n",
      "Train Epoch: 11 [1720/2566 (67%)]\tLoss: 1.166053\n",
      "Train Epoch: 11 [1760/2566 (69%)]\tLoss: 0.552190\n",
      "Train Epoch: 11 [1800/2566 (70%)]\tLoss: 0.917738\n",
      "Train Epoch: 11 [1840/2566 (72%)]\tLoss: 0.240077\n",
      "Train Epoch: 11 [1880/2566 (73%)]\tLoss: 0.178563\n",
      "Train Epoch: 11 [1920/2566 (75%)]\tLoss: 0.505690\n",
      "Train Epoch: 11 [1960/2566 (76%)]\tLoss: 0.630492\n",
      "Train Epoch: 11 [2000/2566 (78%)]\tLoss: 0.480995\n",
      "Train Epoch: 11 [2040/2566 (79%)]\tLoss: 0.662387\n",
      "Train Epoch: 11 [2080/2566 (81%)]\tLoss: 0.844387\n",
      "Train Epoch: 11 [2120/2566 (83%)]\tLoss: 0.562825\n",
      "Train Epoch: 11 [2160/2566 (84%)]\tLoss: 0.677252\n",
      "Train Epoch: 11 [2200/2566 (86%)]\tLoss: 0.792597\n",
      "Train Epoch: 11 [2240/2566 (87%)]\tLoss: 0.546236\n",
      "Train Epoch: 11 [2280/2566 (89%)]\tLoss: 0.681466\n",
      "Train Epoch: 11 [2320/2566 (90%)]\tLoss: 0.856451\n",
      "Train Epoch: 11 [2360/2566 (92%)]\tLoss: 0.661556\n",
      "Train Epoch: 11 [2400/2566 (93%)]\tLoss: 0.688097\n",
      "Train Epoch: 11 [2440/2566 (95%)]\tLoss: 0.324406\n",
      "Train Epoch: 11 [2480/2566 (97%)]\tLoss: 0.386393\n",
      "Train Epoch: 11 [2520/2566 (98%)]\tLoss: 0.507344\n",
      "Train Epoch: 11 [2560/2566 (100%)]\tLoss: 0.406266\n",
      "epoch:11,loss:0.6726686157448641\n",
      "Train set: Average loss: 0.6520, Accuracy: 1965/2566 (77%)\n",
      "Val set: Average loss: 0.7069, Accuracy: 243/327 (74%)\n",
      "Train Epoch: 12 [40/2566 (2%)]\tLoss: 0.408368\n",
      "Train Epoch: 12 [80/2566 (3%)]\tLoss: 1.045859\n",
      "Train Epoch: 12 [120/2566 (5%)]\tLoss: 1.343766\n",
      "Train Epoch: 12 [160/2566 (6%)]\tLoss: 0.874201\n",
      "Train Epoch: 12 [200/2566 (8%)]\tLoss: 0.705068\n",
      "Train Epoch: 12 [240/2566 (9%)]\tLoss: 0.412198\n",
      "Train Epoch: 12 [280/2566 (11%)]\tLoss: 0.540189\n",
      "Train Epoch: 12 [320/2566 (12%)]\tLoss: 0.379009\n",
      "Train Epoch: 12 [360/2566 (14%)]\tLoss: 0.511305\n",
      "Train Epoch: 12 [400/2566 (16%)]\tLoss: 0.146406\n",
      "Train Epoch: 12 [440/2566 (17%)]\tLoss: 0.689505\n",
      "Train Epoch: 12 [480/2566 (19%)]\tLoss: 0.757015\n",
      "Train Epoch: 12 [520/2566 (20%)]\tLoss: 0.665979\n",
      "Train Epoch: 12 [560/2566 (22%)]\tLoss: 1.065477\n",
      "Train Epoch: 12 [600/2566 (23%)]\tLoss: 0.554013\n",
      "Train Epoch: 12 [640/2566 (25%)]\tLoss: 0.833984\n",
      "Train Epoch: 12 [680/2566 (26%)]\tLoss: 0.604486\n",
      "Train Epoch: 12 [720/2566 (28%)]\tLoss: 0.855605\n",
      "Train Epoch: 12 [760/2566 (30%)]\tLoss: 0.211276\n",
      "Train Epoch: 12 [800/2566 (31%)]\tLoss: 0.745021\n",
      "Train Epoch: 12 [840/2566 (33%)]\tLoss: 0.822151\n",
      "Train Epoch: 12 [880/2566 (34%)]\tLoss: 0.640093\n",
      "Train Epoch: 12 [920/2566 (36%)]\tLoss: 0.602798\n",
      "Train Epoch: 12 [960/2566 (37%)]\tLoss: 0.537983\n",
      "Train Epoch: 12 [1000/2566 (39%)]\tLoss: 0.658603\n",
      "Train Epoch: 12 [1040/2566 (40%)]\tLoss: 0.264488\n",
      "Train Epoch: 12 [1080/2566 (42%)]\tLoss: 0.420128\n",
      "Train Epoch: 12 [1120/2566 (44%)]\tLoss: 0.652187\n",
      "Train Epoch: 12 [1160/2566 (45%)]\tLoss: 0.485070\n",
      "Train Epoch: 12 [1200/2566 (47%)]\tLoss: 0.700323\n",
      "Train Epoch: 12 [1240/2566 (48%)]\tLoss: 0.484510\n",
      "Train Epoch: 12 [1280/2566 (50%)]\tLoss: 0.934421\n",
      "Train Epoch: 12 [1320/2566 (51%)]\tLoss: 1.014427\n",
      "Train Epoch: 12 [1360/2566 (53%)]\tLoss: 0.696305\n",
      "Train Epoch: 12 [1400/2566 (55%)]\tLoss: 0.294827\n",
      "Train Epoch: 12 [1440/2566 (56%)]\tLoss: 0.554245\n",
      "Train Epoch: 12 [1480/2566 (58%)]\tLoss: 0.491907\n",
      "Train Epoch: 12 [1520/2566 (59%)]\tLoss: 0.659537\n",
      "Train Epoch: 12 [1560/2566 (61%)]\tLoss: 0.263446\n",
      "Train Epoch: 12 [1600/2566 (62%)]\tLoss: 0.347716\n",
      "Train Epoch: 12 [1640/2566 (64%)]\tLoss: 0.548415\n",
      "Train Epoch: 12 [1680/2566 (65%)]\tLoss: 0.271743\n",
      "Train Epoch: 12 [1720/2566 (67%)]\tLoss: 0.196822\n",
      "Train Epoch: 12 [1760/2566 (69%)]\tLoss: 0.694025\n",
      "Train Epoch: 12 [1800/2566 (70%)]\tLoss: 1.146728\n",
      "Train Epoch: 12 [1840/2566 (72%)]\tLoss: 0.548437\n",
      "Train Epoch: 12 [1880/2566 (73%)]\tLoss: 1.120118\n",
      "Train Epoch: 12 [1920/2566 (75%)]\tLoss: 0.379714\n",
      "Train Epoch: 12 [1960/2566 (76%)]\tLoss: 0.304825\n",
      "Train Epoch: 12 [2000/2566 (78%)]\tLoss: 0.995806\n",
      "Train Epoch: 12 [2040/2566 (79%)]\tLoss: 0.482880\n",
      "Train Epoch: 12 [2080/2566 (81%)]\tLoss: 0.825571\n",
      "Train Epoch: 12 [2120/2566 (83%)]\tLoss: 0.512336\n",
      "Train Epoch: 12 [2160/2566 (84%)]\tLoss: 0.530507\n",
      "Train Epoch: 12 [2200/2566 (86%)]\tLoss: 0.443829\n",
      "Train Epoch: 12 [2240/2566 (87%)]\tLoss: 1.037763\n",
      "Train Epoch: 12 [2280/2566 (89%)]\tLoss: 0.903427\n",
      "Train Epoch: 12 [2320/2566 (90%)]\tLoss: 0.297538\n",
      "Train Epoch: 12 [2360/2566 (92%)]\tLoss: 0.953912\n",
      "Train Epoch: 12 [2400/2566 (93%)]\tLoss: 0.549941\n",
      "Train Epoch: 12 [2440/2566 (95%)]\tLoss: 0.332857\n",
      "Train Epoch: 12 [2480/2566 (97%)]\tLoss: 1.376135\n",
      "Train Epoch: 12 [2520/2566 (98%)]\tLoss: 0.234461\n",
      "Train Epoch: 12 [2560/2566 (100%)]\tLoss: 0.976100\n",
      "epoch:12,loss:0.6546316719222292\n",
      "Train set: Average loss: 0.6295, Accuracy: 1967/2566 (77%)\n",
      "Val set: Average loss: 0.7037, Accuracy: 238/327 (73%)\n",
      "Train Epoch: 13 [40/2566 (2%)]\tLoss: 0.266680\n",
      "Train Epoch: 13 [80/2566 (3%)]\tLoss: 0.520815\n",
      "Train Epoch: 13 [120/2566 (5%)]\tLoss: 0.567961\n",
      "Train Epoch: 13 [160/2566 (6%)]\tLoss: 0.289576\n",
      "Train Epoch: 13 [200/2566 (8%)]\tLoss: 1.070364\n",
      "Train Epoch: 13 [240/2566 (9%)]\tLoss: 0.670984\n",
      "Train Epoch: 13 [280/2566 (11%)]\tLoss: 0.713608\n",
      "Train Epoch: 13 [320/2566 (12%)]\tLoss: 0.520201\n",
      "Train Epoch: 13 [360/2566 (14%)]\tLoss: 0.783675\n",
      "Train Epoch: 13 [400/2566 (16%)]\tLoss: 0.243150\n",
      "Train Epoch: 13 [440/2566 (17%)]\tLoss: 0.333882\n",
      "Train Epoch: 13 [480/2566 (19%)]\tLoss: 0.669617\n",
      "Train Epoch: 13 [520/2566 (20%)]\tLoss: 0.513477\n",
      "Train Epoch: 13 [560/2566 (22%)]\tLoss: 0.343732\n",
      "Train Epoch: 13 [600/2566 (23%)]\tLoss: 0.728137\n",
      "Train Epoch: 13 [640/2566 (25%)]\tLoss: 0.710498\n",
      "Train Epoch: 13 [680/2566 (26%)]\tLoss: 0.706310\n",
      "Train Epoch: 13 [720/2566 (28%)]\tLoss: 1.405478\n",
      "Train Epoch: 13 [760/2566 (30%)]\tLoss: 0.740297\n",
      "Train Epoch: 13 [800/2566 (31%)]\tLoss: 0.559253\n",
      "Train Epoch: 13 [840/2566 (33%)]\tLoss: 1.648890\n",
      "Train Epoch: 13 [880/2566 (34%)]\tLoss: 0.776114\n",
      "Train Epoch: 13 [920/2566 (36%)]\tLoss: 0.528952\n",
      "Train Epoch: 13 [960/2566 (37%)]\tLoss: 0.453836\n",
      "Train Epoch: 13 [1000/2566 (39%)]\tLoss: 0.627584\n",
      "Train Epoch: 13 [1040/2566 (40%)]\tLoss: 0.510589\n",
      "Train Epoch: 13 [1080/2566 (42%)]\tLoss: 0.242345\n",
      "Train Epoch: 13 [1120/2566 (44%)]\tLoss: 1.291188\n",
      "Train Epoch: 13 [1160/2566 (45%)]\tLoss: 0.518882\n",
      "Train Epoch: 13 [1200/2566 (47%)]\tLoss: 0.855536\n",
      "Train Epoch: 13 [1240/2566 (48%)]\tLoss: 0.485215\n",
      "Train Epoch: 13 [1280/2566 (50%)]\tLoss: 1.311370\n",
      "Train Epoch: 13 [1320/2566 (51%)]\tLoss: 1.237134\n",
      "Train Epoch: 13 [1360/2566 (53%)]\tLoss: 0.573098\n",
      "Train Epoch: 13 [1400/2566 (55%)]\tLoss: 0.532971\n",
      "Train Epoch: 13 [1440/2566 (56%)]\tLoss: 1.644433\n",
      "Train Epoch: 13 [1480/2566 (58%)]\tLoss: 0.915301\n",
      "Train Epoch: 13 [1520/2566 (59%)]\tLoss: 0.615870\n",
      "Train Epoch: 13 [1560/2566 (61%)]\tLoss: 0.414271\n",
      "Train Epoch: 13 [1600/2566 (62%)]\tLoss: 0.768177\n",
      "Train Epoch: 13 [1640/2566 (64%)]\tLoss: 0.573316\n",
      "Train Epoch: 13 [1680/2566 (65%)]\tLoss: 0.613346\n",
      "Train Epoch: 13 [1720/2566 (67%)]\tLoss: 0.724705\n",
      "Train Epoch: 13 [1760/2566 (69%)]\tLoss: 0.531637\n",
      "Train Epoch: 13 [1800/2566 (70%)]\tLoss: 0.343974\n",
      "Train Epoch: 13 [1840/2566 (72%)]\tLoss: 0.688108\n",
      "Train Epoch: 13 [1880/2566 (73%)]\tLoss: 0.847042\n",
      "Train Epoch: 13 [1920/2566 (75%)]\tLoss: 0.362611\n",
      "Train Epoch: 13 [1960/2566 (76%)]\tLoss: 0.787560\n",
      "Train Epoch: 13 [2000/2566 (78%)]\tLoss: 0.614490\n",
      "Train Epoch: 13 [2040/2566 (79%)]\tLoss: 0.855882\n",
      "Train Epoch: 13 [2080/2566 (81%)]\tLoss: 0.605957\n",
      "Train Epoch: 13 [2120/2566 (83%)]\tLoss: 0.591962\n",
      "Train Epoch: 13 [2160/2566 (84%)]\tLoss: 0.504544\n",
      "Train Epoch: 13 [2200/2566 (86%)]\tLoss: 0.805716\n",
      "Train Epoch: 13 [2240/2566 (87%)]\tLoss: 0.129937\n",
      "Train Epoch: 13 [2280/2566 (89%)]\tLoss: 0.350381\n",
      "Train Epoch: 13 [2320/2566 (90%)]\tLoss: 0.469228\n",
      "Train Epoch: 13 [2360/2566 (92%)]\tLoss: 0.300059\n",
      "Train Epoch: 13 [2400/2566 (93%)]\tLoss: 0.813594\n",
      "Train Epoch: 13 [2440/2566 (95%)]\tLoss: 0.225342\n",
      "Train Epoch: 13 [2480/2566 (97%)]\tLoss: 0.550874\n",
      "Train Epoch: 13 [2520/2566 (98%)]\tLoss: 0.184536\n",
      "Train Epoch: 13 [2560/2566 (100%)]\tLoss: 1.127910\n",
      "epoch:13,loss:0.6520144666466757\n",
      "Train set: Average loss: 0.5943, Accuracy: 2017/2566 (79%)\n",
      "Val set: Average loss: 0.6707, Accuracy: 242/327 (74%)\n",
      "Train Epoch: 14 [40/2566 (2%)]\tLoss: 0.814039\n",
      "Train Epoch: 14 [80/2566 (3%)]\tLoss: 0.660452\n",
      "Train Epoch: 14 [120/2566 (5%)]\tLoss: 0.588966\n",
      "Train Epoch: 14 [160/2566 (6%)]\tLoss: 0.231466\n",
      "Train Epoch: 14 [200/2566 (8%)]\tLoss: 0.718044\n",
      "Train Epoch: 14 [240/2566 (9%)]\tLoss: 0.559849\n",
      "Train Epoch: 14 [280/2566 (11%)]\tLoss: 0.354888\n",
      "Train Epoch: 14 [320/2566 (12%)]\tLoss: 0.524727\n",
      "Train Epoch: 14 [360/2566 (14%)]\tLoss: 0.319809\n",
      "Train Epoch: 14 [400/2566 (16%)]\tLoss: 1.155796\n",
      "Train Epoch: 14 [440/2566 (17%)]\tLoss: 0.344507\n",
      "Train Epoch: 14 [480/2566 (19%)]\tLoss: 0.602812\n",
      "Train Epoch: 14 [520/2566 (20%)]\tLoss: 0.340414\n",
      "Train Epoch: 14 [560/2566 (22%)]\tLoss: 0.663052\n",
      "Train Epoch: 14 [600/2566 (23%)]\tLoss: 0.892023\n",
      "Train Epoch: 14 [640/2566 (25%)]\tLoss: 0.316003\n",
      "Train Epoch: 14 [680/2566 (26%)]\tLoss: 0.568010\n",
      "Train Epoch: 14 [720/2566 (28%)]\tLoss: 0.304355\n",
      "Train Epoch: 14 [760/2566 (30%)]\tLoss: 0.182200\n",
      "Train Epoch: 14 [800/2566 (31%)]\tLoss: 0.291423\n",
      "Train Epoch: 14 [840/2566 (33%)]\tLoss: 0.480867\n",
      "Train Epoch: 14 [880/2566 (34%)]\tLoss: 0.293020\n",
      "Train Epoch: 14 [920/2566 (36%)]\tLoss: 0.978661\n",
      "Train Epoch: 14 [960/2566 (37%)]\tLoss: 0.410243\n",
      "Train Epoch: 14 [1000/2566 (39%)]\tLoss: 0.421860\n",
      "Train Epoch: 14 [1040/2566 (40%)]\tLoss: 0.683467\n",
      "Train Epoch: 14 [1080/2566 (42%)]\tLoss: 1.146590\n",
      "Train Epoch: 14 [1120/2566 (44%)]\tLoss: 0.282254\n",
      "Train Epoch: 14 [1160/2566 (45%)]\tLoss: 0.665272\n",
      "Train Epoch: 14 [1200/2566 (47%)]\tLoss: 0.288767\n",
      "Train Epoch: 14 [1240/2566 (48%)]\tLoss: 0.607194\n",
      "Train Epoch: 14 [1280/2566 (50%)]\tLoss: 0.502013\n",
      "Train Epoch: 14 [1320/2566 (51%)]\tLoss: 0.634279\n",
      "Train Epoch: 14 [1360/2566 (53%)]\tLoss: 0.501410\n",
      "Train Epoch: 14 [1400/2566 (55%)]\tLoss: 0.163280\n",
      "Train Epoch: 14 [1440/2566 (56%)]\tLoss: 0.459525\n",
      "Train Epoch: 14 [1480/2566 (58%)]\tLoss: 0.859957\n",
      "Train Epoch: 14 [1520/2566 (59%)]\tLoss: 0.908658\n",
      "Train Epoch: 14 [1560/2566 (61%)]\tLoss: 0.916299\n",
      "Train Epoch: 14 [1600/2566 (62%)]\tLoss: 0.415688\n",
      "Train Epoch: 14 [1640/2566 (64%)]\tLoss: 1.381836\n",
      "Train Epoch: 14 [1680/2566 (65%)]\tLoss: 1.016274\n",
      "Train Epoch: 14 [1720/2566 (67%)]\tLoss: 0.752426\n",
      "Train Epoch: 14 [1760/2566 (69%)]\tLoss: 0.465436\n",
      "Train Epoch: 14 [1800/2566 (70%)]\tLoss: 0.565455\n",
      "Train Epoch: 14 [1840/2566 (72%)]\tLoss: 0.767297\n",
      "Train Epoch: 14 [1880/2566 (73%)]\tLoss: 0.581042\n",
      "Train Epoch: 14 [1920/2566 (75%)]\tLoss: 1.076630\n",
      "Train Epoch: 14 [1960/2566 (76%)]\tLoss: 0.289319\n",
      "Train Epoch: 14 [2000/2566 (78%)]\tLoss: 1.177903\n",
      "Train Epoch: 14 [2040/2566 (79%)]\tLoss: 0.932493\n",
      "Train Epoch: 14 [2080/2566 (81%)]\tLoss: 0.488900\n",
      "Train Epoch: 14 [2120/2566 (83%)]\tLoss: 0.466749\n",
      "Train Epoch: 14 [2160/2566 (84%)]\tLoss: 0.422976\n",
      "Train Epoch: 14 [2200/2566 (86%)]\tLoss: 0.348907\n",
      "Train Epoch: 14 [2240/2566 (87%)]\tLoss: 0.754952\n",
      "Train Epoch: 14 [2280/2566 (89%)]\tLoss: 0.561133\n",
      "Train Epoch: 14 [2320/2566 (90%)]\tLoss: 0.900165\n",
      "Train Epoch: 14 [2360/2566 (92%)]\tLoss: 0.297305\n",
      "Train Epoch: 14 [2400/2566 (93%)]\tLoss: 0.586331\n",
      "Train Epoch: 14 [2440/2566 (95%)]\tLoss: 0.785199\n",
      "Train Epoch: 14 [2480/2566 (97%)]\tLoss: 0.682590\n",
      "Train Epoch: 14 [2520/2566 (98%)]\tLoss: 0.951047\n",
      "Train Epoch: 14 [2560/2566 (100%)]\tLoss: 0.361178\n",
      "epoch:14,loss:0.6264411459134375\n",
      "Train set: Average loss: 0.6252, Accuracy: 1970/2566 (77%)\n",
      "Val set: Average loss: 0.6879, Accuracy: 238/327 (73%)\n",
      "Train Epoch: 15 [40/2566 (2%)]\tLoss: 0.256257\n",
      "Train Epoch: 15 [80/2566 (3%)]\tLoss: 0.668805\n",
      "Train Epoch: 15 [120/2566 (5%)]\tLoss: 0.607841\n",
      "Train Epoch: 15 [160/2566 (6%)]\tLoss: 0.611367\n",
      "Train Epoch: 15 [200/2566 (8%)]\tLoss: 0.162153\n",
      "Train Epoch: 15 [240/2566 (9%)]\tLoss: 0.512427\n",
      "Train Epoch: 15 [280/2566 (11%)]\tLoss: 0.605648\n",
      "Train Epoch: 15 [320/2566 (12%)]\tLoss: 0.340877\n",
      "Train Epoch: 15 [360/2566 (14%)]\tLoss: 0.240155\n",
      "Train Epoch: 15 [400/2566 (16%)]\tLoss: 0.144867\n",
      "Train Epoch: 15 [440/2566 (17%)]\tLoss: 1.134106\n",
      "Train Epoch: 15 [480/2566 (19%)]\tLoss: 0.531082\n",
      "Train Epoch: 15 [520/2566 (20%)]\tLoss: 0.458954\n",
      "Train Epoch: 15 [560/2566 (22%)]\tLoss: 0.716807\n",
      "Train Epoch: 15 [600/2566 (23%)]\tLoss: 0.497430\n",
      "Train Epoch: 15 [640/2566 (25%)]\tLoss: 0.690777\n",
      "Train Epoch: 15 [680/2566 (26%)]\tLoss: 1.208987\n",
      "Train Epoch: 15 [720/2566 (28%)]\tLoss: 0.331875\n",
      "Train Epoch: 15 [760/2566 (30%)]\tLoss: 0.986249\n",
      "Train Epoch: 15 [800/2566 (31%)]\tLoss: 0.436914\n",
      "Train Epoch: 15 [840/2566 (33%)]\tLoss: 0.387660\n",
      "Train Epoch: 15 [880/2566 (34%)]\tLoss: 0.490094\n",
      "Train Epoch: 15 [920/2566 (36%)]\tLoss: 1.475356\n",
      "Train Epoch: 15 [960/2566 (37%)]\tLoss: 0.899543\n",
      "Train Epoch: 15 [1000/2566 (39%)]\tLoss: 0.504524\n",
      "Train Epoch: 15 [1040/2566 (40%)]\tLoss: 0.502635\n",
      "Train Epoch: 15 [1080/2566 (42%)]\tLoss: 0.542260\n",
      "Train Epoch: 15 [1120/2566 (44%)]\tLoss: 0.428213\n",
      "Train Epoch: 15 [1160/2566 (45%)]\tLoss: 0.522985\n",
      "Train Epoch: 15 [1200/2566 (47%)]\tLoss: 0.382952\n",
      "Train Epoch: 15 [1240/2566 (48%)]\tLoss: 0.635810\n",
      "Train Epoch: 15 [1280/2566 (50%)]\tLoss: 0.934366\n",
      "Train Epoch: 15 [1320/2566 (51%)]\tLoss: 0.910006\n",
      "Train Epoch: 15 [1360/2566 (53%)]\tLoss: 0.626867\n",
      "Train Epoch: 15 [1400/2566 (55%)]\tLoss: 0.758096\n",
      "Train Epoch: 15 [1440/2566 (56%)]\tLoss: 0.435466\n",
      "Train Epoch: 15 [1480/2566 (58%)]\tLoss: 0.719750\n",
      "Train Epoch: 15 [1520/2566 (59%)]\tLoss: 0.426377\n",
      "Train Epoch: 15 [1560/2566 (61%)]\tLoss: 0.579193\n",
      "Train Epoch: 15 [1600/2566 (62%)]\tLoss: 0.634764\n",
      "Train Epoch: 15 [1640/2566 (64%)]\tLoss: 1.023806\n",
      "Train Epoch: 15 [1680/2566 (65%)]\tLoss: 0.713282\n",
      "Train Epoch: 15 [1720/2566 (67%)]\tLoss: 0.182846\n",
      "Train Epoch: 15 [1760/2566 (69%)]\tLoss: 0.991091\n",
      "Train Epoch: 15 [1800/2566 (70%)]\tLoss: 0.472854\n",
      "Train Epoch: 15 [1840/2566 (72%)]\tLoss: 0.769819\n",
      "Train Epoch: 15 [1880/2566 (73%)]\tLoss: 0.258040\n",
      "Train Epoch: 15 [1920/2566 (75%)]\tLoss: 1.303447\n",
      "Train Epoch: 15 [1960/2566 (76%)]\tLoss: 0.632013\n",
      "Train Epoch: 15 [2000/2566 (78%)]\tLoss: 0.461842\n",
      "Train Epoch: 15 [2040/2566 (79%)]\tLoss: 0.441025\n",
      "Train Epoch: 15 [2080/2566 (81%)]\tLoss: 0.864447\n",
      "Train Epoch: 15 [2120/2566 (83%)]\tLoss: 0.660857\n",
      "Train Epoch: 15 [2160/2566 (84%)]\tLoss: 1.328010\n",
      "Train Epoch: 15 [2200/2566 (86%)]\tLoss: 0.742298\n",
      "Train Epoch: 15 [2240/2566 (87%)]\tLoss: 0.932605\n",
      "Train Epoch: 15 [2280/2566 (89%)]\tLoss: 0.467473\n",
      "Train Epoch: 15 [2320/2566 (90%)]\tLoss: 0.429729\n",
      "Train Epoch: 15 [2360/2566 (92%)]\tLoss: 0.777249\n",
      "Train Epoch: 15 [2400/2566 (93%)]\tLoss: 1.058105\n",
      "Train Epoch: 15 [2440/2566 (95%)]\tLoss: 0.336550\n",
      "Train Epoch: 15 [2480/2566 (97%)]\tLoss: 0.819213\n",
      "Train Epoch: 15 [2520/2566 (98%)]\tLoss: 0.470772\n",
      "Train Epoch: 15 [2560/2566 (100%)]\tLoss: 0.912613\n",
      "epoch:15,loss:0.6125596569752396\n",
      "Train set: Average loss: 0.5546, Accuracy: 2044/2566 (80%)\n",
      "Val set: Average loss: 0.6533, Accuracy: 244/327 (75%)\n",
      "Train Epoch: 16 [40/2566 (2%)]\tLoss: 0.548203\n",
      "Train Epoch: 16 [80/2566 (3%)]\tLoss: 0.357788\n",
      "Train Epoch: 16 [120/2566 (5%)]\tLoss: 0.365900\n",
      "Train Epoch: 16 [160/2566 (6%)]\tLoss: 0.448860\n",
      "Train Epoch: 16 [200/2566 (8%)]\tLoss: 0.298312\n",
      "Train Epoch: 16 [240/2566 (9%)]\tLoss: 0.752490\n",
      "Train Epoch: 16 [280/2566 (11%)]\tLoss: 0.352271\n",
      "Train Epoch: 16 [320/2566 (12%)]\tLoss: 0.529321\n",
      "Train Epoch: 16 [360/2566 (14%)]\tLoss: 0.238227\n",
      "Train Epoch: 16 [400/2566 (16%)]\tLoss: 0.609438\n",
      "Train Epoch: 16 [440/2566 (17%)]\tLoss: 0.370991\n",
      "Train Epoch: 16 [480/2566 (19%)]\tLoss: 0.531059\n",
      "Train Epoch: 16 [520/2566 (20%)]\tLoss: 0.283473\n",
      "Train Epoch: 16 [560/2566 (22%)]\tLoss: 0.784789\n",
      "Train Epoch: 16 [600/2566 (23%)]\tLoss: 0.781904\n",
      "Train Epoch: 16 [640/2566 (25%)]\tLoss: 0.692770\n",
      "Train Epoch: 16 [680/2566 (26%)]\tLoss: 0.412507\n",
      "Train Epoch: 16 [720/2566 (28%)]\tLoss: 0.724190\n",
      "Train Epoch: 16 [760/2566 (30%)]\tLoss: 0.531861\n",
      "Train Epoch: 16 [800/2566 (31%)]\tLoss: 1.095524\n",
      "Train Epoch: 16 [840/2566 (33%)]\tLoss: 0.210450\n",
      "Train Epoch: 16 [880/2566 (34%)]\tLoss: 0.910493\n",
      "Train Epoch: 16 [920/2566 (36%)]\tLoss: 0.543481\n",
      "Train Epoch: 16 [960/2566 (37%)]\tLoss: 0.309313\n",
      "Train Epoch: 16 [1000/2566 (39%)]\tLoss: 0.279789\n",
      "Train Epoch: 16 [1040/2566 (40%)]\tLoss: 0.811994\n",
      "Train Epoch: 16 [1080/2566 (42%)]\tLoss: 0.203564\n",
      "Train Epoch: 16 [1120/2566 (44%)]\tLoss: 0.722842\n",
      "Train Epoch: 16 [1160/2566 (45%)]\tLoss: 0.304351\n",
      "Train Epoch: 16 [1200/2566 (47%)]\tLoss: 0.535198\n",
      "Train Epoch: 16 [1240/2566 (48%)]\tLoss: 0.291434\n",
      "Train Epoch: 16 [1280/2566 (50%)]\tLoss: 0.909936\n",
      "Train Epoch: 16 [1320/2566 (51%)]\tLoss: 0.838160\n",
      "Train Epoch: 16 [1360/2566 (53%)]\tLoss: 0.923861\n",
      "Train Epoch: 16 [1400/2566 (55%)]\tLoss: 0.449073\n",
      "Train Epoch: 16 [1440/2566 (56%)]\tLoss: 0.737319\n",
      "Train Epoch: 16 [1480/2566 (58%)]\tLoss: 0.521602\n",
      "Train Epoch: 16 [1520/2566 (59%)]\tLoss: 0.847525\n",
      "Train Epoch: 16 [1560/2566 (61%)]\tLoss: 0.436362\n",
      "Train Epoch: 16 [1600/2566 (62%)]\tLoss: 1.047467\n",
      "Train Epoch: 16 [1640/2566 (64%)]\tLoss: 0.579262\n",
      "Train Epoch: 16 [1680/2566 (65%)]\tLoss: 1.112298\n",
      "Train Epoch: 16 [1720/2566 (67%)]\tLoss: 0.585695\n",
      "Train Epoch: 16 [1760/2566 (69%)]\tLoss: 0.464151\n",
      "Train Epoch: 16 [1800/2566 (70%)]\tLoss: 0.981899\n",
      "Train Epoch: 16 [1840/2566 (72%)]\tLoss: 0.720822\n",
      "Train Epoch: 16 [1880/2566 (73%)]\tLoss: 0.688185\n",
      "Train Epoch: 16 [1920/2566 (75%)]\tLoss: 0.809212\n",
      "Train Epoch: 16 [1960/2566 (76%)]\tLoss: 0.271884\n",
      "Train Epoch: 16 [2000/2566 (78%)]\tLoss: 0.512873\n",
      "Train Epoch: 16 [2040/2566 (79%)]\tLoss: 0.308295\n",
      "Train Epoch: 16 [2080/2566 (81%)]\tLoss: 0.380309\n",
      "Train Epoch: 16 [2120/2566 (83%)]\tLoss: 0.237328\n",
      "Train Epoch: 16 [2160/2566 (84%)]\tLoss: 0.920158\n",
      "Train Epoch: 16 [2200/2566 (86%)]\tLoss: 0.628987\n",
      "Train Epoch: 16 [2240/2566 (87%)]\tLoss: 0.860765\n",
      "Train Epoch: 16 [2280/2566 (89%)]\tLoss: 0.330498\n",
      "Train Epoch: 16 [2320/2566 (90%)]\tLoss: 0.524529\n",
      "Train Epoch: 16 [2360/2566 (92%)]\tLoss: 0.445227\n",
      "Train Epoch: 16 [2400/2566 (93%)]\tLoss: 0.412693\n",
      "Train Epoch: 16 [2440/2566 (95%)]\tLoss: 0.844228\n",
      "Train Epoch: 16 [2480/2566 (97%)]\tLoss: 0.554812\n",
      "Train Epoch: 16 [2520/2566 (98%)]\tLoss: 0.329479\n",
      "Train Epoch: 16 [2560/2566 (100%)]\tLoss: 0.603090\n",
      "epoch:16,loss:0.5943147977843092\n",
      "Train set: Average loss: 0.5493, Accuracy: 2031/2566 (79%)\n",
      "Val set: Average loss: 0.6571, Accuracy: 246/327 (75%)\n",
      "Train Epoch: 17 [40/2566 (2%)]\tLoss: 0.380889\n",
      "Train Epoch: 17 [80/2566 (3%)]\tLoss: 0.308175\n",
      "Train Epoch: 17 [120/2566 (5%)]\tLoss: 0.531175\n",
      "Train Epoch: 17 [160/2566 (6%)]\tLoss: 0.734226\n",
      "Train Epoch: 17 [200/2566 (8%)]\tLoss: 0.437900\n",
      "Train Epoch: 17 [240/2566 (9%)]\tLoss: 0.330545\n",
      "Train Epoch: 17 [280/2566 (11%)]\tLoss: 0.752176\n",
      "Train Epoch: 17 [320/2566 (12%)]\tLoss: 0.350392\n",
      "Train Epoch: 17 [360/2566 (14%)]\tLoss: 0.463391\n",
      "Train Epoch: 17 [400/2566 (16%)]\tLoss: 0.855040\n",
      "Train Epoch: 17 [440/2566 (17%)]\tLoss: 0.627534\n",
      "Train Epoch: 17 [480/2566 (19%)]\tLoss: 0.390002\n",
      "Train Epoch: 17 [520/2566 (20%)]\tLoss: 0.288124\n",
      "Train Epoch: 17 [560/2566 (22%)]\tLoss: 0.658146\n",
      "Train Epoch: 17 [600/2566 (23%)]\tLoss: 0.844143\n",
      "Train Epoch: 17 [640/2566 (25%)]\tLoss: 0.091032\n",
      "Train Epoch: 17 [680/2566 (26%)]\tLoss: 0.788051\n",
      "Train Epoch: 17 [720/2566 (28%)]\tLoss: 0.450194\n",
      "Train Epoch: 17 [760/2566 (30%)]\tLoss: 0.437699\n",
      "Train Epoch: 17 [800/2566 (31%)]\tLoss: 0.560180\n",
      "Train Epoch: 17 [840/2566 (33%)]\tLoss: 0.695539\n",
      "Train Epoch: 17 [880/2566 (34%)]\tLoss: 0.479851\n",
      "Train Epoch: 17 [920/2566 (36%)]\tLoss: 0.675471\n",
      "Train Epoch: 17 [960/2566 (37%)]\tLoss: 0.368255\n",
      "Train Epoch: 17 [1000/2566 (39%)]\tLoss: 1.028898\n",
      "Train Epoch: 17 [1040/2566 (40%)]\tLoss: 0.427093\n",
      "Train Epoch: 17 [1080/2566 (42%)]\tLoss: 0.928537\n",
      "Train Epoch: 17 [1120/2566 (44%)]\tLoss: 0.763606\n",
      "Train Epoch: 17 [1160/2566 (45%)]\tLoss: 0.377496\n",
      "Train Epoch: 17 [1200/2566 (47%)]\tLoss: 0.924202\n",
      "Train Epoch: 17 [1240/2566 (48%)]\tLoss: 0.397313\n",
      "Train Epoch: 17 [1280/2566 (50%)]\tLoss: 0.514129\n",
      "Train Epoch: 17 [1320/2566 (51%)]\tLoss: 0.958612\n",
      "Train Epoch: 17 [1360/2566 (53%)]\tLoss: 0.722015\n",
      "Train Epoch: 17 [1400/2566 (55%)]\tLoss: 0.275336\n",
      "Train Epoch: 17 [1440/2566 (56%)]\tLoss: 0.995389\n",
      "Train Epoch: 17 [1480/2566 (58%)]\tLoss: 0.197204\n",
      "Train Epoch: 17 [1520/2566 (59%)]\tLoss: 0.632877\n",
      "Train Epoch: 17 [1560/2566 (61%)]\tLoss: 0.922028\n",
      "Train Epoch: 17 [1600/2566 (62%)]\tLoss: 0.524047\n",
      "Train Epoch: 17 [1640/2566 (64%)]\tLoss: 0.339197\n",
      "Train Epoch: 17 [1680/2566 (65%)]\tLoss: 0.354733\n",
      "Train Epoch: 17 [1720/2566 (67%)]\tLoss: 0.321858\n",
      "Train Epoch: 17 [1760/2566 (69%)]\tLoss: 0.741322\n",
      "Train Epoch: 17 [1800/2566 (70%)]\tLoss: 0.501214\n",
      "Train Epoch: 17 [1840/2566 (72%)]\tLoss: 0.778678\n",
      "Train Epoch: 17 [1880/2566 (73%)]\tLoss: 0.338794\n",
      "Train Epoch: 17 [1920/2566 (75%)]\tLoss: 0.545796\n",
      "Train Epoch: 17 [1960/2566 (76%)]\tLoss: 1.210865\n",
      "Train Epoch: 17 [2000/2566 (78%)]\tLoss: 0.371318\n",
      "Train Epoch: 17 [2040/2566 (79%)]\tLoss: 0.193885\n",
      "Train Epoch: 17 [2080/2566 (81%)]\tLoss: 0.482668\n",
      "Train Epoch: 17 [2120/2566 (83%)]\tLoss: 0.750566\n",
      "Train Epoch: 17 [2160/2566 (84%)]\tLoss: 0.184596\n",
      "Train Epoch: 17 [2200/2566 (86%)]\tLoss: 0.398061\n",
      "Train Epoch: 17 [2240/2566 (87%)]\tLoss: 0.786023\n",
      "Train Epoch: 17 [2280/2566 (89%)]\tLoss: 0.568203\n",
      "Train Epoch: 17 [2320/2566 (90%)]\tLoss: 0.532297\n",
      "Train Epoch: 17 [2360/2566 (92%)]\tLoss: 0.619211\n",
      "Train Epoch: 17 [2400/2566 (93%)]\tLoss: 0.147357\n",
      "Train Epoch: 17 [2440/2566 (95%)]\tLoss: 0.876280\n",
      "Train Epoch: 17 [2480/2566 (97%)]\tLoss: 0.816311\n",
      "Train Epoch: 17 [2520/2566 (98%)]\tLoss: 0.363082\n",
      "Train Epoch: 17 [2560/2566 (100%)]\tLoss: 0.780964\n",
      "epoch:17,loss:0.5813068704116753\n",
      "Train set: Average loss: 0.5307, Accuracy: 2061/2566 (80%)\n",
      "Val set: Average loss: 0.6376, Accuracy: 245/327 (75%)\n",
      "Train Epoch: 18 [40/2566 (2%)]\tLoss: 0.420714\n",
      "Train Epoch: 18 [80/2566 (3%)]\tLoss: 0.565574\n",
      "Train Epoch: 18 [120/2566 (5%)]\tLoss: 0.172541\n",
      "Train Epoch: 18 [160/2566 (6%)]\tLoss: 0.704661\n",
      "Train Epoch: 18 [200/2566 (8%)]\tLoss: 0.609034\n",
      "Train Epoch: 18 [240/2566 (9%)]\tLoss: 0.214374\n",
      "Train Epoch: 18 [280/2566 (11%)]\tLoss: 0.518662\n",
      "Train Epoch: 18 [320/2566 (12%)]\tLoss: 0.759803\n",
      "Train Epoch: 18 [360/2566 (14%)]\tLoss: 0.514399\n",
      "Train Epoch: 18 [400/2566 (16%)]\tLoss: 0.704836\n",
      "Train Epoch: 18 [440/2566 (17%)]\tLoss: 1.050371\n",
      "Train Epoch: 18 [480/2566 (19%)]\tLoss: 0.556223\n",
      "Train Epoch: 18 [520/2566 (20%)]\tLoss: 0.759734\n",
      "Train Epoch: 18 [560/2566 (22%)]\tLoss: 0.399025\n",
      "Train Epoch: 18 [600/2566 (23%)]\tLoss: 0.561014\n",
      "Train Epoch: 18 [640/2566 (25%)]\tLoss: 0.246626\n",
      "Train Epoch: 18 [680/2566 (26%)]\tLoss: 0.485431\n",
      "Train Epoch: 18 [720/2566 (28%)]\tLoss: 0.376466\n",
      "Train Epoch: 18 [760/2566 (30%)]\tLoss: 0.565035\n",
      "Train Epoch: 18 [800/2566 (31%)]\tLoss: 0.673598\n",
      "Train Epoch: 18 [840/2566 (33%)]\tLoss: 0.685564\n",
      "Train Epoch: 18 [880/2566 (34%)]\tLoss: 0.617592\n",
      "Train Epoch: 18 [920/2566 (36%)]\tLoss: 0.510617\n",
      "Train Epoch: 18 [960/2566 (37%)]\tLoss: 0.566608\n",
      "Train Epoch: 18 [1000/2566 (39%)]\tLoss: 1.109981\n",
      "Train Epoch: 18 [1040/2566 (40%)]\tLoss: 0.346176\n",
      "Train Epoch: 18 [1080/2566 (42%)]\tLoss: 0.983834\n",
      "Train Epoch: 18 [1120/2566 (44%)]\tLoss: 0.975907\n",
      "Train Epoch: 18 [1160/2566 (45%)]\tLoss: 0.501007\n",
      "Train Epoch: 18 [1200/2566 (47%)]\tLoss: 0.173856\n",
      "Train Epoch: 18 [1240/2566 (48%)]\tLoss: 0.629499\n",
      "Train Epoch: 18 [1280/2566 (50%)]\tLoss: 0.231192\n",
      "Train Epoch: 18 [1320/2566 (51%)]\tLoss: 0.606708\n",
      "Train Epoch: 18 [1360/2566 (53%)]\tLoss: 0.772855\n",
      "Train Epoch: 18 [1400/2566 (55%)]\tLoss: 0.674312\n",
      "Train Epoch: 18 [1440/2566 (56%)]\tLoss: 0.600554\n",
      "Train Epoch: 18 [1480/2566 (58%)]\tLoss: 0.673234\n",
      "Train Epoch: 18 [1520/2566 (59%)]\tLoss: 0.333859\n",
      "Train Epoch: 18 [1560/2566 (61%)]\tLoss: 0.516677\n",
      "Train Epoch: 18 [1600/2566 (62%)]\tLoss: 0.602650\n",
      "Train Epoch: 18 [1640/2566 (64%)]\tLoss: 0.773032\n",
      "Train Epoch: 18 [1680/2566 (65%)]\tLoss: 0.494011\n",
      "Train Epoch: 18 [1720/2566 (67%)]\tLoss: 0.782224\n",
      "Train Epoch: 18 [1760/2566 (69%)]\tLoss: 0.800819\n",
      "Train Epoch: 18 [1800/2566 (70%)]\tLoss: 0.333016\n",
      "Train Epoch: 18 [1840/2566 (72%)]\tLoss: 0.578329\n",
      "Train Epoch: 18 [1880/2566 (73%)]\tLoss: 0.519453\n",
      "Train Epoch: 18 [1920/2566 (75%)]\tLoss: 1.021588\n",
      "Train Epoch: 18 [1960/2566 (76%)]\tLoss: 0.592879\n",
      "Train Epoch: 18 [2000/2566 (78%)]\tLoss: 0.684161\n",
      "Train Epoch: 18 [2040/2566 (79%)]\tLoss: 0.582589\n",
      "Train Epoch: 18 [2080/2566 (81%)]\tLoss: 0.589645\n",
      "Train Epoch: 18 [2120/2566 (83%)]\tLoss: 1.110228\n",
      "Train Epoch: 18 [2160/2566 (84%)]\tLoss: 0.233266\n",
      "Train Epoch: 18 [2200/2566 (86%)]\tLoss: 0.568151\n",
      "Train Epoch: 18 [2240/2566 (87%)]\tLoss: 0.584772\n",
      "Train Epoch: 18 [2280/2566 (89%)]\tLoss: 0.204499\n",
      "Train Epoch: 18 [2320/2566 (90%)]\tLoss: 0.679375\n",
      "Train Epoch: 18 [2360/2566 (92%)]\tLoss: 0.495060\n",
      "Train Epoch: 18 [2400/2566 (93%)]\tLoss: 0.409931\n",
      "Train Epoch: 18 [2440/2566 (95%)]\tLoss: 0.505837\n",
      "Train Epoch: 18 [2480/2566 (97%)]\tLoss: 0.109617\n",
      "Train Epoch: 18 [2520/2566 (98%)]\tLoss: 0.457227\n",
      "Train Epoch: 18 [2560/2566 (100%)]\tLoss: 0.288260\n",
      "epoch:18,loss:0.5544966589253268\n",
      "Train set: Average loss: 0.4949, Accuracy: 2102/2566 (82%)\n",
      "Val set: Average loss: 0.6548, Accuracy: 247/327 (76%)\n",
      "Train Epoch: 19 [40/2566 (2%)]\tLoss: 0.282372\n",
      "Train Epoch: 19 [80/2566 (3%)]\tLoss: 0.993329\n",
      "Train Epoch: 19 [120/2566 (5%)]\tLoss: 0.560093\n",
      "Train Epoch: 19 [160/2566 (6%)]\tLoss: 0.682227\n",
      "Train Epoch: 19 [200/2566 (8%)]\tLoss: 0.605436\n",
      "Train Epoch: 19 [240/2566 (9%)]\tLoss: 0.471398\n",
      "Train Epoch: 19 [280/2566 (11%)]\tLoss: 1.048909\n",
      "Train Epoch: 19 [320/2566 (12%)]\tLoss: 0.241974\n",
      "Train Epoch: 19 [360/2566 (14%)]\tLoss: 0.900194\n",
      "Train Epoch: 19 [400/2566 (16%)]\tLoss: 0.534936\n",
      "Train Epoch: 19 [440/2566 (17%)]\tLoss: 1.099009\n",
      "Train Epoch: 19 [480/2566 (19%)]\tLoss: 0.172871\n",
      "Train Epoch: 19 [520/2566 (20%)]\tLoss: 0.753123\n",
      "Train Epoch: 19 [560/2566 (22%)]\tLoss: 0.186821\n",
      "Train Epoch: 19 [600/2566 (23%)]\tLoss: 1.411148\n",
      "Train Epoch: 19 [640/2566 (25%)]\tLoss: 0.297224\n",
      "Train Epoch: 19 [680/2566 (26%)]\tLoss: 0.556619\n",
      "Train Epoch: 19 [720/2566 (28%)]\tLoss: 0.739113\n",
      "Train Epoch: 19 [760/2566 (30%)]\tLoss: 0.592054\n",
      "Train Epoch: 19 [800/2566 (31%)]\tLoss: 0.817534\n",
      "Train Epoch: 19 [840/2566 (33%)]\tLoss: 0.581649\n",
      "Train Epoch: 19 [880/2566 (34%)]\tLoss: 0.558890\n",
      "Train Epoch: 19 [920/2566 (36%)]\tLoss: 1.230796\n",
      "Train Epoch: 19 [960/2566 (37%)]\tLoss: 0.582363\n",
      "Train Epoch: 19 [1000/2566 (39%)]\tLoss: 0.491232\n",
      "Train Epoch: 19 [1040/2566 (40%)]\tLoss: 1.563731\n",
      "Train Epoch: 19 [1080/2566 (42%)]\tLoss: 0.203759\n",
      "Train Epoch: 19 [1120/2566 (44%)]\tLoss: 0.552486\n",
      "Train Epoch: 19 [1160/2566 (45%)]\tLoss: 0.782301\n",
      "Train Epoch: 19 [1200/2566 (47%)]\tLoss: 0.422689\n",
      "Train Epoch: 19 [1240/2566 (48%)]\tLoss: 0.434366\n",
      "Train Epoch: 19 [1280/2566 (50%)]\tLoss: 0.632503\n",
      "Train Epoch: 19 [1320/2566 (51%)]\tLoss: 0.787568\n",
      "Train Epoch: 19 [1360/2566 (53%)]\tLoss: 0.192296\n",
      "Train Epoch: 19 [1400/2566 (55%)]\tLoss: 0.529270\n",
      "Train Epoch: 19 [1440/2566 (56%)]\tLoss: 0.482590\n",
      "Train Epoch: 19 [1480/2566 (58%)]\tLoss: 1.018256\n",
      "Train Epoch: 19 [1520/2566 (59%)]\tLoss: 0.263460\n",
      "Train Epoch: 19 [1560/2566 (61%)]\tLoss: 0.276502\n",
      "Train Epoch: 19 [1600/2566 (62%)]\tLoss: 0.214330\n",
      "Train Epoch: 19 [1640/2566 (64%)]\tLoss: 0.537048\n",
      "Train Epoch: 19 [1680/2566 (65%)]\tLoss: 0.448665\n",
      "Train Epoch: 19 [1720/2566 (67%)]\tLoss: 1.569023\n",
      "Train Epoch: 19 [1760/2566 (69%)]\tLoss: 0.702777\n",
      "Train Epoch: 19 [1800/2566 (70%)]\tLoss: 0.586473\n",
      "Train Epoch: 19 [1840/2566 (72%)]\tLoss: 0.594025\n",
      "Train Epoch: 19 [1880/2566 (73%)]\tLoss: 0.370410\n",
      "Train Epoch: 19 [1920/2566 (75%)]\tLoss: 0.069829\n",
      "Train Epoch: 19 [1960/2566 (76%)]\tLoss: 0.674338\n",
      "Train Epoch: 19 [2000/2566 (78%)]\tLoss: 0.383028\n",
      "Train Epoch: 19 [2040/2566 (79%)]\tLoss: 0.400428\n",
      "Train Epoch: 19 [2080/2566 (81%)]\tLoss: 0.942780\n",
      "Train Epoch: 19 [2120/2566 (83%)]\tLoss: 0.192222\n",
      "Train Epoch: 19 [2160/2566 (84%)]\tLoss: 0.857343\n",
      "Train Epoch: 19 [2200/2566 (86%)]\tLoss: 0.525965\n",
      "Train Epoch: 19 [2240/2566 (87%)]\tLoss: 0.655287\n",
      "Train Epoch: 19 [2280/2566 (89%)]\tLoss: 0.598977\n",
      "Train Epoch: 19 [2320/2566 (90%)]\tLoss: 0.587133\n",
      "Train Epoch: 19 [2360/2566 (92%)]\tLoss: 0.452133\n",
      "Train Epoch: 19 [2400/2566 (93%)]\tLoss: 0.670887\n",
      "Train Epoch: 19 [2440/2566 (95%)]\tLoss: 0.573253\n",
      "Train Epoch: 19 [2480/2566 (97%)]\tLoss: 0.698789\n",
      "Train Epoch: 19 [2520/2566 (98%)]\tLoss: 1.136475\n",
      "Train Epoch: 19 [2560/2566 (100%)]\tLoss: 0.862617\n",
      "epoch:19,loss:0.5476196604864991\n",
      "Train set: Average loss: 0.4992, Accuracy: 2086/2566 (81%)\n",
      "Val set: Average loss: 0.6439, Accuracy: 247/327 (76%)\n",
      "Train Epoch: 20 [40/2566 (2%)]\tLoss: 0.247088\n",
      "Train Epoch: 20 [80/2566 (3%)]\tLoss: 0.090160\n",
      "Train Epoch: 20 [120/2566 (5%)]\tLoss: 0.569803\n",
      "Train Epoch: 20 [160/2566 (6%)]\tLoss: 0.306844\n",
      "Train Epoch: 20 [200/2566 (8%)]\tLoss: 0.670198\n",
      "Train Epoch: 20 [240/2566 (9%)]\tLoss: 0.364349\n",
      "Train Epoch: 20 [280/2566 (11%)]\tLoss: 0.191339\n",
      "Train Epoch: 20 [320/2566 (12%)]\tLoss: 0.640015\n",
      "Train Epoch: 20 [360/2566 (14%)]\tLoss: 0.249062\n",
      "Train Epoch: 20 [400/2566 (16%)]\tLoss: 0.613442\n",
      "Train Epoch: 20 [440/2566 (17%)]\tLoss: 0.114610\n",
      "Train Epoch: 20 [480/2566 (19%)]\tLoss: 0.309925\n",
      "Train Epoch: 20 [520/2566 (20%)]\tLoss: 0.775932\n",
      "Train Epoch: 20 [560/2566 (22%)]\tLoss: 1.117897\n",
      "Train Epoch: 20 [600/2566 (23%)]\tLoss: 0.301561\n",
      "Train Epoch: 20 [640/2566 (25%)]\tLoss: 0.411027\n",
      "Train Epoch: 20 [680/2566 (26%)]\tLoss: 0.469331\n",
      "Train Epoch: 20 [720/2566 (28%)]\tLoss: 0.286799\n",
      "Train Epoch: 20 [760/2566 (30%)]\tLoss: 0.542476\n",
      "Train Epoch: 20 [800/2566 (31%)]\tLoss: 0.888770\n",
      "Train Epoch: 20 [840/2566 (33%)]\tLoss: 0.417574\n",
      "Train Epoch: 20 [880/2566 (34%)]\tLoss: 0.597771\n",
      "Train Epoch: 20 [920/2566 (36%)]\tLoss: 0.548133\n",
      "Train Epoch: 20 [960/2566 (37%)]\tLoss: 0.552554\n",
      "Train Epoch: 20 [1000/2566 (39%)]\tLoss: 0.516948\n",
      "Train Epoch: 20 [1040/2566 (40%)]\tLoss: 0.803749\n",
      "Train Epoch: 20 [1080/2566 (42%)]\tLoss: 0.776789\n",
      "Train Epoch: 20 [1120/2566 (44%)]\tLoss: 0.568760\n",
      "Train Epoch: 20 [1160/2566 (45%)]\tLoss: 0.729547\n",
      "Train Epoch: 20 [1200/2566 (47%)]\tLoss: 0.173112\n",
      "Train Epoch: 20 [1240/2566 (48%)]\tLoss: 0.622763\n",
      "Train Epoch: 20 [1280/2566 (50%)]\tLoss: 0.797758\n",
      "Train Epoch: 20 [1320/2566 (51%)]\tLoss: 0.156377\n",
      "Train Epoch: 20 [1360/2566 (53%)]\tLoss: 0.599272\n",
      "Train Epoch: 20 [1400/2566 (55%)]\tLoss: 0.548431\n",
      "Train Epoch: 20 [1440/2566 (56%)]\tLoss: 0.622846\n",
      "Train Epoch: 20 [1480/2566 (58%)]\tLoss: 0.316265\n",
      "Train Epoch: 20 [1520/2566 (59%)]\tLoss: 0.483028\n",
      "Train Epoch: 20 [1560/2566 (61%)]\tLoss: 0.269061\n",
      "Train Epoch: 20 [1600/2566 (62%)]\tLoss: 1.214636\n",
      "Train Epoch: 20 [1640/2566 (64%)]\tLoss: 0.245651\n",
      "Train Epoch: 20 [1680/2566 (65%)]\tLoss: 0.314963\n",
      "Train Epoch: 20 [1720/2566 (67%)]\tLoss: 0.644265\n",
      "Train Epoch: 20 [1760/2566 (69%)]\tLoss: 0.234572\n",
      "Train Epoch: 20 [1800/2566 (70%)]\tLoss: 0.856588\n",
      "Train Epoch: 20 [1840/2566 (72%)]\tLoss: 0.396323\n",
      "Train Epoch: 20 [1880/2566 (73%)]\tLoss: 0.415124\n",
      "Train Epoch: 20 [1920/2566 (75%)]\tLoss: 0.965086\n",
      "Train Epoch: 20 [1960/2566 (76%)]\tLoss: 0.322362\n",
      "Train Epoch: 20 [2000/2566 (78%)]\tLoss: 0.487549\n",
      "Train Epoch: 20 [2040/2566 (79%)]\tLoss: 0.437847\n",
      "Train Epoch: 20 [2080/2566 (81%)]\tLoss: 0.552011\n",
      "Train Epoch: 20 [2120/2566 (83%)]\tLoss: 0.538837\n",
      "Train Epoch: 20 [2160/2566 (84%)]\tLoss: 0.705175\n",
      "Train Epoch: 20 [2200/2566 (86%)]\tLoss: 0.560597\n",
      "Train Epoch: 20 [2240/2566 (87%)]\tLoss: 0.599422\n",
      "Train Epoch: 20 [2280/2566 (89%)]\tLoss: 0.484575\n",
      "Train Epoch: 20 [2320/2566 (90%)]\tLoss: 0.561314\n",
      "Train Epoch: 20 [2360/2566 (92%)]\tLoss: 0.450067\n",
      "Train Epoch: 20 [2400/2566 (93%)]\tLoss: 0.329550\n",
      "Train Epoch: 20 [2440/2566 (95%)]\tLoss: 0.244981\n",
      "Train Epoch: 20 [2480/2566 (97%)]\tLoss: 1.276837\n",
      "Train Epoch: 20 [2520/2566 (98%)]\tLoss: 0.293012\n",
      "Train Epoch: 20 [2560/2566 (100%)]\tLoss: 0.579448\n",
      "epoch:20,loss:0.5144092138868255\n",
      "Train set: Average loss: 0.4697, Accuracy: 2142/2566 (83%)\n",
      "Val set: Average loss: 0.6379, Accuracy: 248/327 (76%)\n",
      "Train Epoch: 21 [40/2566 (2%)]\tLoss: 0.483416\n",
      "Train Epoch: 21 [80/2566 (3%)]\tLoss: 0.665526\n",
      "Train Epoch: 21 [120/2566 (5%)]\tLoss: 0.888568\n",
      "Train Epoch: 21 [160/2566 (6%)]\tLoss: 0.142793\n",
      "Train Epoch: 21 [200/2566 (8%)]\tLoss: 0.670582\n",
      "Train Epoch: 21 [240/2566 (9%)]\tLoss: 0.413153\n",
      "Train Epoch: 21 [280/2566 (11%)]\tLoss: 0.675167\n",
      "Train Epoch: 21 [320/2566 (12%)]\tLoss: 0.587815\n",
      "Train Epoch: 21 [360/2566 (14%)]\tLoss: 0.624429\n",
      "Train Epoch: 21 [400/2566 (16%)]\tLoss: 0.537366\n",
      "Train Epoch: 21 [440/2566 (17%)]\tLoss: 0.568742\n",
      "Train Epoch: 21 [480/2566 (19%)]\tLoss: 0.490972\n",
      "Train Epoch: 21 [520/2566 (20%)]\tLoss: 0.360947\n",
      "Train Epoch: 21 [560/2566 (22%)]\tLoss: 0.734735\n",
      "Train Epoch: 21 [600/2566 (23%)]\tLoss: 0.658406\n",
      "Train Epoch: 21 [640/2566 (25%)]\tLoss: 0.566164\n",
      "Train Epoch: 21 [680/2566 (26%)]\tLoss: 0.130211\n",
      "Train Epoch: 21 [720/2566 (28%)]\tLoss: 0.697428\n",
      "Train Epoch: 21 [760/2566 (30%)]\tLoss: 0.350145\n",
      "Train Epoch: 21 [800/2566 (31%)]\tLoss: 0.456514\n",
      "Train Epoch: 21 [840/2566 (33%)]\tLoss: 0.586803\n",
      "Train Epoch: 21 [880/2566 (34%)]\tLoss: 0.331186\n",
      "Train Epoch: 21 [920/2566 (36%)]\tLoss: 0.854470\n",
      "Train Epoch: 21 [960/2566 (37%)]\tLoss: 0.695922\n",
      "Train Epoch: 21 [1000/2566 (39%)]\tLoss: 0.492559\n",
      "Train Epoch: 21 [1040/2566 (40%)]\tLoss: 0.342049\n",
      "Train Epoch: 21 [1080/2566 (42%)]\tLoss: 0.321757\n",
      "Train Epoch: 21 [1120/2566 (44%)]\tLoss: 1.420038\n",
      "Train Epoch: 21 [1160/2566 (45%)]\tLoss: 0.711254\n",
      "Train Epoch: 21 [1200/2566 (47%)]\tLoss: 0.187551\n",
      "Train Epoch: 21 [1240/2566 (48%)]\tLoss: 0.310660\n",
      "Train Epoch: 21 [1280/2566 (50%)]\tLoss: 0.453779\n",
      "Train Epoch: 21 [1320/2566 (51%)]\tLoss: 0.520187\n",
      "Train Epoch: 21 [1360/2566 (53%)]\tLoss: 0.340955\n",
      "Train Epoch: 21 [1400/2566 (55%)]\tLoss: 0.300413\n",
      "Train Epoch: 21 [1440/2566 (56%)]\tLoss: 0.629893\n",
      "Train Epoch: 21 [1480/2566 (58%)]\tLoss: 0.258034\n",
      "Train Epoch: 21 [1520/2566 (59%)]\tLoss: 0.386607\n",
      "Train Epoch: 21 [1560/2566 (61%)]\tLoss: 0.821933\n",
      "Train Epoch: 21 [1600/2566 (62%)]\tLoss: 0.972141\n",
      "Train Epoch: 21 [1640/2566 (64%)]\tLoss: 0.583078\n",
      "Train Epoch: 21 [1680/2566 (65%)]\tLoss: 0.762977\n",
      "Train Epoch: 21 [1720/2566 (67%)]\tLoss: 0.636464\n",
      "Train Epoch: 21 [1760/2566 (69%)]\tLoss: 0.729866\n",
      "Train Epoch: 21 [1800/2566 (70%)]\tLoss: 1.071638\n",
      "Train Epoch: 21 [1840/2566 (72%)]\tLoss: 0.568018\n",
      "Train Epoch: 21 [1880/2566 (73%)]\tLoss: 0.448503\n",
      "Train Epoch: 21 [1920/2566 (75%)]\tLoss: 0.501597\n",
      "Train Epoch: 21 [1960/2566 (76%)]\tLoss: 0.149175\n",
      "Train Epoch: 21 [2000/2566 (78%)]\tLoss: 0.384936\n",
      "Train Epoch: 21 [2040/2566 (79%)]\tLoss: 0.341596\n",
      "Train Epoch: 21 [2080/2566 (81%)]\tLoss: 0.149819\n",
      "Train Epoch: 21 [2120/2566 (83%)]\tLoss: 0.690314\n",
      "Train Epoch: 21 [2160/2566 (84%)]\tLoss: 0.842685\n",
      "Train Epoch: 21 [2200/2566 (86%)]\tLoss: 0.531826\n",
      "Train Epoch: 21 [2240/2566 (87%)]\tLoss: 0.466846\n",
      "Train Epoch: 21 [2280/2566 (89%)]\tLoss: 0.335255\n",
      "Train Epoch: 21 [2320/2566 (90%)]\tLoss: 0.102384\n",
      "Train Epoch: 21 [2360/2566 (92%)]\tLoss: 0.326363\n",
      "Train Epoch: 21 [2400/2566 (93%)]\tLoss: 0.489678\n",
      "Train Epoch: 21 [2440/2566 (95%)]\tLoss: 0.184688\n",
      "Train Epoch: 21 [2480/2566 (97%)]\tLoss: 0.545687\n",
      "Train Epoch: 21 [2520/2566 (98%)]\tLoss: 0.562851\n",
      "Train Epoch: 21 [2560/2566 (100%)]\tLoss: 0.132313\n",
      "epoch:21,loss:0.5075489577845992\n",
      "Train set: Average loss: 0.4152, Accuracy: 2198/2566 (86%)\n",
      "Val set: Average loss: 0.5892, Accuracy: 259/327 (79%)\n",
      "Train Epoch: 22 [40/2566 (2%)]\tLoss: 0.074841\n",
      "Train Epoch: 22 [80/2566 (3%)]\tLoss: 0.689580\n",
      "Train Epoch: 22 [120/2566 (5%)]\tLoss: 0.697815\n",
      "Train Epoch: 22 [160/2566 (6%)]\tLoss: 0.222898\n",
      "Train Epoch: 22 [200/2566 (8%)]\tLoss: 0.757964\n",
      "Train Epoch: 22 [240/2566 (9%)]\tLoss: 0.164227\n",
      "Train Epoch: 22 [280/2566 (11%)]\tLoss: 0.131436\n",
      "Train Epoch: 22 [320/2566 (12%)]\tLoss: 1.113854\n",
      "Train Epoch: 22 [360/2566 (14%)]\tLoss: 0.981671\n",
      "Train Epoch: 22 [400/2566 (16%)]\tLoss: 0.223688\n",
      "Train Epoch: 22 [440/2566 (17%)]\tLoss: 1.361317\n",
      "Train Epoch: 22 [480/2566 (19%)]\tLoss: 0.339971\n",
      "Train Epoch: 22 [520/2566 (20%)]\tLoss: 0.514348\n",
      "Train Epoch: 22 [560/2566 (22%)]\tLoss: 0.514705\n",
      "Train Epoch: 22 [600/2566 (23%)]\tLoss: 0.792981\n",
      "Train Epoch: 22 [640/2566 (25%)]\tLoss: 0.531030\n",
      "Train Epoch: 22 [680/2566 (26%)]\tLoss: 0.306177\n",
      "Train Epoch: 22 [720/2566 (28%)]\tLoss: 0.165852\n",
      "Train Epoch: 22 [760/2566 (30%)]\tLoss: 0.583633\n",
      "Train Epoch: 22 [800/2566 (31%)]\tLoss: 0.433522\n",
      "Train Epoch: 22 [840/2566 (33%)]\tLoss: 0.171839\n",
      "Train Epoch: 22 [880/2566 (34%)]\tLoss: 0.358312\n",
      "Train Epoch: 22 [920/2566 (36%)]\tLoss: 0.434397\n",
      "Train Epoch: 22 [960/2566 (37%)]\tLoss: 0.449674\n",
      "Train Epoch: 22 [1000/2566 (39%)]\tLoss: 0.115462\n",
      "Train Epoch: 22 [1040/2566 (40%)]\tLoss: 0.814128\n",
      "Train Epoch: 22 [1080/2566 (42%)]\tLoss: 0.182592\n",
      "Train Epoch: 22 [1120/2566 (44%)]\tLoss: 0.427285\n",
      "Train Epoch: 22 [1160/2566 (45%)]\tLoss: 0.663794\n",
      "Train Epoch: 22 [1200/2566 (47%)]\tLoss: 0.254646\n",
      "Train Epoch: 22 [1240/2566 (48%)]\tLoss: 0.378405\n",
      "Train Epoch: 22 [1280/2566 (50%)]\tLoss: 0.357578\n",
      "Train Epoch: 22 [1320/2566 (51%)]\tLoss: 0.603668\n",
      "Train Epoch: 22 [1360/2566 (53%)]\tLoss: 0.422246\n",
      "Train Epoch: 22 [1400/2566 (55%)]\tLoss: 0.661189\n",
      "Train Epoch: 22 [1440/2566 (56%)]\tLoss: 0.794401\n",
      "Train Epoch: 22 [1480/2566 (58%)]\tLoss: 0.627097\n",
      "Train Epoch: 22 [1520/2566 (59%)]\tLoss: 0.608740\n",
      "Train Epoch: 22 [1560/2566 (61%)]\tLoss: 0.193147\n",
      "Train Epoch: 22 [1600/2566 (62%)]\tLoss: 0.271454\n",
      "Train Epoch: 22 [1640/2566 (64%)]\tLoss: 0.143080\n",
      "Train Epoch: 22 [1680/2566 (65%)]\tLoss: 0.369409\n",
      "Train Epoch: 22 [1720/2566 (67%)]\tLoss: 0.771618\n",
      "Train Epoch: 22 [1760/2566 (69%)]\tLoss: 0.325789\n",
      "Train Epoch: 22 [1800/2566 (70%)]\tLoss: 0.430591\n",
      "Train Epoch: 22 [1840/2566 (72%)]\tLoss: 0.238857\n",
      "Train Epoch: 22 [1880/2566 (73%)]\tLoss: 0.520380\n",
      "Train Epoch: 22 [1920/2566 (75%)]\tLoss: 0.612218\n",
      "Train Epoch: 22 [1960/2566 (76%)]\tLoss: 0.420379\n",
      "Train Epoch: 22 [2000/2566 (78%)]\tLoss: 0.683397\n",
      "Train Epoch: 22 [2040/2566 (79%)]\tLoss: 0.876324\n",
      "Train Epoch: 22 [2080/2566 (81%)]\tLoss: 0.473224\n",
      "Train Epoch: 22 [2120/2566 (83%)]\tLoss: 0.645974\n",
      "Train Epoch: 22 [2160/2566 (84%)]\tLoss: 0.239163\n",
      "Train Epoch: 22 [2200/2566 (86%)]\tLoss: 0.450619\n",
      "Train Epoch: 22 [2240/2566 (87%)]\tLoss: 0.404022\n",
      "Train Epoch: 22 [2280/2566 (89%)]\tLoss: 0.287833\n",
      "Train Epoch: 22 [2320/2566 (90%)]\tLoss: 0.216892\n",
      "Train Epoch: 22 [2360/2566 (92%)]\tLoss: 0.460572\n",
      "Train Epoch: 22 [2400/2566 (93%)]\tLoss: 0.324528\n",
      "Train Epoch: 22 [2440/2566 (95%)]\tLoss: 0.646174\n",
      "Train Epoch: 22 [2480/2566 (97%)]\tLoss: 0.705685\n",
      "Train Epoch: 22 [2520/2566 (98%)]\tLoss: 0.240826\n",
      "Train Epoch: 22 [2560/2566 (100%)]\tLoss: 0.721465\n",
      "epoch:22,loss:0.4769577494709291\n",
      "Train set: Average loss: 0.4669, Accuracy: 2105/2566 (82%)\n",
      "Val set: Average loss: 0.6914, Accuracy: 246/327 (75%)\n",
      "Train Epoch: 23 [40/2566 (2%)]\tLoss: 0.527202\n",
      "Train Epoch: 23 [80/2566 (3%)]\tLoss: 0.423495\n",
      "Train Epoch: 23 [120/2566 (5%)]\tLoss: 0.435298\n",
      "Train Epoch: 23 [160/2566 (6%)]\tLoss: 0.630038\n",
      "Train Epoch: 23 [200/2566 (8%)]\tLoss: 0.255970\n",
      "Train Epoch: 23 [240/2566 (9%)]\tLoss: 0.455448\n",
      "Train Epoch: 23 [280/2566 (11%)]\tLoss: 0.522589\n",
      "Train Epoch: 23 [320/2566 (12%)]\tLoss: 0.883270\n",
      "Train Epoch: 23 [360/2566 (14%)]\tLoss: 0.246028\n",
      "Train Epoch: 23 [400/2566 (16%)]\tLoss: 0.349271\n",
      "Train Epoch: 23 [440/2566 (17%)]\tLoss: 0.726987\n",
      "Train Epoch: 23 [480/2566 (19%)]\tLoss: 0.326929\n",
      "Train Epoch: 23 [520/2566 (20%)]\tLoss: 0.204064\n",
      "Train Epoch: 23 [560/2566 (22%)]\tLoss: 0.380365\n",
      "Train Epoch: 23 [600/2566 (23%)]\tLoss: 0.279355\n",
      "Train Epoch: 23 [640/2566 (25%)]\tLoss: 0.605305\n",
      "Train Epoch: 23 [680/2566 (26%)]\tLoss: 0.162556\n",
      "Train Epoch: 23 [720/2566 (28%)]\tLoss: 0.059210\n",
      "Train Epoch: 23 [760/2566 (30%)]\tLoss: 0.297579\n",
      "Train Epoch: 23 [800/2566 (31%)]\tLoss: 0.794148\n",
      "Train Epoch: 23 [840/2566 (33%)]\tLoss: 0.538542\n",
      "Train Epoch: 23 [880/2566 (34%)]\tLoss: 0.314677\n",
      "Train Epoch: 23 [920/2566 (36%)]\tLoss: 0.492821\n",
      "Train Epoch: 23 [960/2566 (37%)]\tLoss: 0.432952\n",
      "Train Epoch: 23 [1000/2566 (39%)]\tLoss: 0.381727\n",
      "Train Epoch: 23 [1040/2566 (40%)]\tLoss: 0.454683\n",
      "Train Epoch: 23 [1080/2566 (42%)]\tLoss: 0.538629\n",
      "Train Epoch: 23 [1120/2566 (44%)]\tLoss: 0.264846\n",
      "Train Epoch: 23 [1160/2566 (45%)]\tLoss: 0.409314\n",
      "Train Epoch: 23 [1200/2566 (47%)]\tLoss: 0.229483\n",
      "Train Epoch: 23 [1240/2566 (48%)]\tLoss: 0.627855\n",
      "Train Epoch: 23 [1280/2566 (50%)]\tLoss: 0.553370\n",
      "Train Epoch: 23 [1320/2566 (51%)]\tLoss: 0.504237\n",
      "Train Epoch: 23 [1360/2566 (53%)]\tLoss: 0.555563\n",
      "Train Epoch: 23 [1400/2566 (55%)]\tLoss: 0.134742\n",
      "Train Epoch: 23 [1440/2566 (56%)]\tLoss: 0.645848\n",
      "Train Epoch: 23 [1480/2566 (58%)]\tLoss: 0.356782\n",
      "Train Epoch: 23 [1520/2566 (59%)]\tLoss: 0.600129\n",
      "Train Epoch: 23 [1560/2566 (61%)]\tLoss: 0.590070\n",
      "Train Epoch: 23 [1600/2566 (62%)]\tLoss: 0.466621\n",
      "Train Epoch: 23 [1640/2566 (64%)]\tLoss: 0.358456\n",
      "Train Epoch: 23 [1680/2566 (65%)]\tLoss: 0.304819\n",
      "Train Epoch: 23 [1720/2566 (67%)]\tLoss: 0.360203\n",
      "Train Epoch: 23 [1760/2566 (69%)]\tLoss: 0.420309\n",
      "Train Epoch: 23 [1800/2566 (70%)]\tLoss: 0.246609\n",
      "Train Epoch: 23 [1840/2566 (72%)]\tLoss: 0.459445\n",
      "Train Epoch: 23 [1880/2566 (73%)]\tLoss: 0.255696\n",
      "Train Epoch: 23 [1920/2566 (75%)]\tLoss: 0.529002\n",
      "Train Epoch: 23 [1960/2566 (76%)]\tLoss: 0.535474\n",
      "Train Epoch: 23 [2000/2566 (78%)]\tLoss: 0.509572\n",
      "Train Epoch: 23 [2040/2566 (79%)]\tLoss: 0.377739\n",
      "Train Epoch: 23 [2080/2566 (81%)]\tLoss: 0.703365\n",
      "Train Epoch: 23 [2120/2566 (83%)]\tLoss: 0.250814\n",
      "Train Epoch: 23 [2160/2566 (84%)]\tLoss: 0.190243\n",
      "Train Epoch: 23 [2200/2566 (86%)]\tLoss: 1.535326\n",
      "Train Epoch: 23 [2240/2566 (87%)]\tLoss: 0.653206\n",
      "Train Epoch: 23 [2280/2566 (89%)]\tLoss: 0.701280\n",
      "Train Epoch: 23 [2320/2566 (90%)]\tLoss: 1.271091\n",
      "Train Epoch: 23 [2360/2566 (92%)]\tLoss: 0.462309\n",
      "Train Epoch: 23 [2400/2566 (93%)]\tLoss: 0.346344\n",
      "Train Epoch: 23 [2440/2566 (95%)]\tLoss: 0.618250\n",
      "Train Epoch: 23 [2480/2566 (97%)]\tLoss: 1.153669\n",
      "Train Epoch: 23 [2520/2566 (98%)]\tLoss: 0.301253\n",
      "Train Epoch: 23 [2560/2566 (100%)]\tLoss: 0.360273\n",
      "epoch:23,loss:0.4619082193763642\n",
      "Train set: Average loss: 0.4053, Accuracy: 2184/2566 (85%)\n",
      "Val set: Average loss: 0.5948, Accuracy: 257/327 (79%)\n",
      "Train Epoch: 24 [40/2566 (2%)]\tLoss: 0.175889\n",
      "Train Epoch: 24 [80/2566 (3%)]\tLoss: 0.259696\n",
      "Train Epoch: 24 [120/2566 (5%)]\tLoss: 0.400478\n",
      "Train Epoch: 24 [160/2566 (6%)]\tLoss: 0.583097\n",
      "Train Epoch: 24 [200/2566 (8%)]\tLoss: 0.587095\n",
      "Train Epoch: 24 [240/2566 (9%)]\tLoss: 0.173435\n",
      "Train Epoch: 24 [280/2566 (11%)]\tLoss: 0.258779\n",
      "Train Epoch: 24 [320/2566 (12%)]\tLoss: 0.520771\n",
      "Train Epoch: 24 [360/2566 (14%)]\tLoss: 0.081850\n",
      "Train Epoch: 24 [400/2566 (16%)]\tLoss: 0.524902\n",
      "Train Epoch: 24 [440/2566 (17%)]\tLoss: 0.140962\n",
      "Train Epoch: 24 [480/2566 (19%)]\tLoss: 0.394317\n",
      "Train Epoch: 24 [520/2566 (20%)]\tLoss: 0.799342\n",
      "Train Epoch: 24 [560/2566 (22%)]\tLoss: 0.506684\n",
      "Train Epoch: 24 [600/2566 (23%)]\tLoss: 0.158795\n",
      "Train Epoch: 24 [640/2566 (25%)]\tLoss: 0.992141\n",
      "Train Epoch: 24 [680/2566 (26%)]\tLoss: 0.967964\n",
      "Train Epoch: 24 [720/2566 (28%)]\tLoss: 0.577322\n",
      "Train Epoch: 24 [760/2566 (30%)]\tLoss: 0.533839\n",
      "Train Epoch: 24 [800/2566 (31%)]\tLoss: 0.464189\n",
      "Train Epoch: 24 [840/2566 (33%)]\tLoss: 0.395465\n",
      "Train Epoch: 24 [880/2566 (34%)]\tLoss: 0.403806\n",
      "Train Epoch: 24 [920/2566 (36%)]\tLoss: 0.398726\n",
      "Train Epoch: 24 [960/2566 (37%)]\tLoss: 0.267416\n",
      "Train Epoch: 24 [1000/2566 (39%)]\tLoss: 0.566790\n",
      "Train Epoch: 24 [1040/2566 (40%)]\tLoss: 0.329611\n",
      "Train Epoch: 24 [1080/2566 (42%)]\tLoss: 0.146917\n",
      "Train Epoch: 24 [1120/2566 (44%)]\tLoss: 0.817620\n",
      "Train Epoch: 24 [1160/2566 (45%)]\tLoss: 0.480376\n",
      "Train Epoch: 24 [1200/2566 (47%)]\tLoss: 0.546943\n",
      "Train Epoch: 24 [1240/2566 (48%)]\tLoss: 0.436007\n",
      "Train Epoch: 24 [1280/2566 (50%)]\tLoss: 0.454340\n",
      "Train Epoch: 24 [1320/2566 (51%)]\tLoss: 0.350185\n",
      "Train Epoch: 24 [1360/2566 (53%)]\tLoss: 0.229844\n",
      "Train Epoch: 24 [1400/2566 (55%)]\tLoss: 0.665286\n",
      "Train Epoch: 24 [1440/2566 (56%)]\tLoss: 0.421917\n",
      "Train Epoch: 24 [1480/2566 (58%)]\tLoss: 0.572603\n",
      "Train Epoch: 24 [1520/2566 (59%)]\tLoss: 0.369175\n",
      "Train Epoch: 24 [1560/2566 (61%)]\tLoss: 0.486239\n",
      "Train Epoch: 24 [1600/2566 (62%)]\tLoss: 1.278304\n",
      "Train Epoch: 24 [1640/2566 (64%)]\tLoss: 0.295341\n",
      "Train Epoch: 24 [1680/2566 (65%)]\tLoss: 0.385177\n",
      "Train Epoch: 24 [1720/2566 (67%)]\tLoss: 0.759833\n",
      "Train Epoch: 24 [1760/2566 (69%)]\tLoss: 0.405353\n",
      "Train Epoch: 24 [1800/2566 (70%)]\tLoss: 0.205238\n",
      "Train Epoch: 24 [1840/2566 (72%)]\tLoss: 0.402108\n",
      "Train Epoch: 24 [1880/2566 (73%)]\tLoss: 0.350116\n",
      "Train Epoch: 24 [1920/2566 (75%)]\tLoss: 0.488877\n",
      "Train Epoch: 24 [1960/2566 (76%)]\tLoss: 0.361156\n",
      "Train Epoch: 24 [2000/2566 (78%)]\tLoss: 0.202261\n",
      "Train Epoch: 24 [2040/2566 (79%)]\tLoss: 0.229280\n",
      "Train Epoch: 24 [2080/2566 (81%)]\tLoss: 0.584325\n",
      "Train Epoch: 24 [2120/2566 (83%)]\tLoss: 0.198589\n",
      "Train Epoch: 24 [2160/2566 (84%)]\tLoss: 0.506046\n",
      "Train Epoch: 24 [2200/2566 (86%)]\tLoss: 0.180972\n",
      "Train Epoch: 24 [2240/2566 (87%)]\tLoss: 0.573099\n",
      "Train Epoch: 24 [2280/2566 (89%)]\tLoss: 0.973534\n",
      "Train Epoch: 24 [2320/2566 (90%)]\tLoss: 0.137305\n",
      "Train Epoch: 24 [2360/2566 (92%)]\tLoss: 0.769495\n",
      "Train Epoch: 24 [2400/2566 (93%)]\tLoss: 0.541910\n",
      "Train Epoch: 24 [2440/2566 (95%)]\tLoss: 0.425563\n",
      "Train Epoch: 24 [2480/2566 (97%)]\tLoss: 0.042655\n",
      "Train Epoch: 24 [2520/2566 (98%)]\tLoss: 0.280314\n",
      "Train Epoch: 24 [2560/2566 (100%)]\tLoss: 0.335821\n",
      "epoch:24,loss:0.43544507480438255\n",
      "Train set: Average loss: 0.3613, Accuracy: 2271/2566 (89%)\n",
      "Val set: Average loss: 0.5944, Accuracy: 255/327 (78%)\n",
      "Train Epoch: 25 [40/2566 (2%)]\tLoss: 0.465616\n",
      "Train Epoch: 25 [80/2566 (3%)]\tLoss: 0.387760\n",
      "Train Epoch: 25 [120/2566 (5%)]\tLoss: 0.706496\n",
      "Train Epoch: 25 [160/2566 (6%)]\tLoss: 0.228223\n",
      "Train Epoch: 25 [200/2566 (8%)]\tLoss: 0.142496\n",
      "Train Epoch: 25 [240/2566 (9%)]\tLoss: 0.613762\n",
      "Train Epoch: 25 [280/2566 (11%)]\tLoss: 0.163612\n",
      "Train Epoch: 25 [320/2566 (12%)]\tLoss: 0.118291\n",
      "Train Epoch: 25 [360/2566 (14%)]\tLoss: 0.547173\n",
      "Train Epoch: 25 [400/2566 (16%)]\tLoss: 0.342900\n",
      "Train Epoch: 25 [440/2566 (17%)]\tLoss: 0.208269\n",
      "Train Epoch: 25 [480/2566 (19%)]\tLoss: 0.251130\n",
      "Train Epoch: 25 [520/2566 (20%)]\tLoss: 0.916702\n",
      "Train Epoch: 25 [560/2566 (22%)]\tLoss: 0.330950\n",
      "Train Epoch: 25 [600/2566 (23%)]\tLoss: 0.384779\n",
      "Train Epoch: 25 [640/2566 (25%)]\tLoss: 0.345563\n",
      "Train Epoch: 25 [680/2566 (26%)]\tLoss: 0.053719\n",
      "Train Epoch: 25 [720/2566 (28%)]\tLoss: 0.590490\n",
      "Train Epoch: 25 [760/2566 (30%)]\tLoss: 0.715823\n",
      "Train Epoch: 25 [800/2566 (31%)]\tLoss: 0.643060\n",
      "Train Epoch: 25 [840/2566 (33%)]\tLoss: 0.190025\n",
      "Train Epoch: 25 [880/2566 (34%)]\tLoss: 0.467060\n",
      "Train Epoch: 25 [920/2566 (36%)]\tLoss: 0.176524\n",
      "Train Epoch: 25 [960/2566 (37%)]\tLoss: 1.151714\n",
      "Train Epoch: 25 [1000/2566 (39%)]\tLoss: 0.527978\n",
      "Train Epoch: 25 [1040/2566 (40%)]\tLoss: 0.369840\n",
      "Train Epoch: 25 [1080/2566 (42%)]\tLoss: 0.237326\n",
      "Train Epoch: 25 [1120/2566 (44%)]\tLoss: 0.641356\n",
      "Train Epoch: 25 [1160/2566 (45%)]\tLoss: 0.367290\n",
      "Train Epoch: 25 [1200/2566 (47%)]\tLoss: 0.451405\n",
      "Train Epoch: 25 [1240/2566 (48%)]\tLoss: 0.560081\n",
      "Train Epoch: 25 [1280/2566 (50%)]\tLoss: 0.309304\n",
      "Train Epoch: 25 [1320/2566 (51%)]\tLoss: 0.511023\n",
      "Train Epoch: 25 [1360/2566 (53%)]\tLoss: 0.249789\n",
      "Train Epoch: 25 [1400/2566 (55%)]\tLoss: 0.552852\n",
      "Train Epoch: 25 [1440/2566 (56%)]\tLoss: 0.197116\n",
      "Train Epoch: 25 [1480/2566 (58%)]\tLoss: 0.425030\n",
      "Train Epoch: 25 [1520/2566 (59%)]\tLoss: 0.032049\n",
      "Train Epoch: 25 [1560/2566 (61%)]\tLoss: 0.550399\n",
      "Train Epoch: 25 [1600/2566 (62%)]\tLoss: 0.233348\n",
      "Train Epoch: 25 [1640/2566 (64%)]\tLoss: 0.098786\n",
      "Train Epoch: 25 [1680/2566 (65%)]\tLoss: 0.341283\n",
      "Train Epoch: 25 [1720/2566 (67%)]\tLoss: 0.409182\n",
      "Train Epoch: 25 [1760/2566 (69%)]\tLoss: 0.416623\n",
      "Train Epoch: 25 [1800/2566 (70%)]\tLoss: 0.269075\n",
      "Train Epoch: 25 [1840/2566 (72%)]\tLoss: 0.409149\n",
      "Train Epoch: 25 [1880/2566 (73%)]\tLoss: 0.578832\n",
      "Train Epoch: 25 [1920/2566 (75%)]\tLoss: 0.642221\n",
      "Train Epoch: 25 [1960/2566 (76%)]\tLoss: 0.402194\n",
      "Train Epoch: 25 [2000/2566 (78%)]\tLoss: 0.348001\n",
      "Train Epoch: 25 [2040/2566 (79%)]\tLoss: 0.479638\n",
      "Train Epoch: 25 [2080/2566 (81%)]\tLoss: 0.256793\n",
      "Train Epoch: 25 [2120/2566 (83%)]\tLoss: 0.861273\n",
      "Train Epoch: 25 [2160/2566 (84%)]\tLoss: 0.207601\n",
      "Train Epoch: 25 [2200/2566 (86%)]\tLoss: 0.767810\n",
      "Train Epoch: 25 [2240/2566 (87%)]\tLoss: 0.225318\n",
      "Train Epoch: 25 [2280/2566 (89%)]\tLoss: 0.373634\n",
      "Train Epoch: 25 [2320/2566 (90%)]\tLoss: 0.268593\n",
      "Train Epoch: 25 [2360/2566 (92%)]\tLoss: 0.462018\n",
      "Train Epoch: 25 [2400/2566 (93%)]\tLoss: 0.233308\n",
      "Train Epoch: 25 [2440/2566 (95%)]\tLoss: 0.441069\n",
      "Train Epoch: 25 [2480/2566 (97%)]\tLoss: 0.380702\n",
      "Train Epoch: 25 [2520/2566 (98%)]\tLoss: 0.163718\n",
      "Train Epoch: 25 [2560/2566 (100%)]\tLoss: 0.308437\n",
      "epoch:25,loss:0.41394510954572034\n",
      "Train set: Average loss: 0.3486, Accuracy: 2239/2566 (87%)\n",
      "Val set: Average loss: 0.6011, Accuracy: 257/327 (79%)\n",
      "Train Epoch: 26 [40/2566 (2%)]\tLoss: 0.630084\n",
      "Train Epoch: 26 [80/2566 (3%)]\tLoss: 0.266549\n",
      "Train Epoch: 26 [120/2566 (5%)]\tLoss: 0.148594\n",
      "Train Epoch: 26 [160/2566 (6%)]\tLoss: 0.321025\n",
      "Train Epoch: 26 [200/2566 (8%)]\tLoss: 0.159583\n",
      "Train Epoch: 26 [240/2566 (9%)]\tLoss: 0.628227\n",
      "Train Epoch: 26 [280/2566 (11%)]\tLoss: 0.423940\n",
      "Train Epoch: 26 [320/2566 (12%)]\tLoss: 0.517251\n",
      "Train Epoch: 26 [360/2566 (14%)]\tLoss: 0.390029\n",
      "Train Epoch: 26 [400/2566 (16%)]\tLoss: 0.152559\n",
      "Train Epoch: 26 [440/2566 (17%)]\tLoss: 0.133786\n",
      "Train Epoch: 26 [480/2566 (19%)]\tLoss: 0.180284\n",
      "Train Epoch: 26 [520/2566 (20%)]\tLoss: 0.582790\n",
      "Train Epoch: 26 [560/2566 (22%)]\tLoss: 0.268824\n",
      "Train Epoch: 26 [600/2566 (23%)]\tLoss: 0.823407\n",
      "Train Epoch: 26 [640/2566 (25%)]\tLoss: 0.320590\n",
      "Train Epoch: 26 [680/2566 (26%)]\tLoss: 0.291914\n",
      "Train Epoch: 26 [720/2566 (28%)]\tLoss: 0.247276\n",
      "Train Epoch: 26 [760/2566 (30%)]\tLoss: 0.179534\n",
      "Train Epoch: 26 [800/2566 (31%)]\tLoss: 0.466756\n",
      "Train Epoch: 26 [840/2566 (33%)]\tLoss: 0.870571\n",
      "Train Epoch: 26 [880/2566 (34%)]\tLoss: 0.230010\n",
      "Train Epoch: 26 [920/2566 (36%)]\tLoss: 0.438829\n",
      "Train Epoch: 26 [960/2566 (37%)]\tLoss: 0.091276\n",
      "Train Epoch: 26 [1000/2566 (39%)]\tLoss: 0.310847\n",
      "Train Epoch: 26 [1040/2566 (40%)]\tLoss: 0.262489\n",
      "Train Epoch: 26 [1080/2566 (42%)]\tLoss: 1.350312\n",
      "Train Epoch: 26 [1120/2566 (44%)]\tLoss: 0.603317\n",
      "Train Epoch: 26 [1160/2566 (45%)]\tLoss: 0.389384\n",
      "Train Epoch: 26 [1200/2566 (47%)]\tLoss: 0.968521\n",
      "Train Epoch: 26 [1240/2566 (48%)]\tLoss: 0.347459\n",
      "Train Epoch: 26 [1280/2566 (50%)]\tLoss: 0.523810\n",
      "Train Epoch: 26 [1320/2566 (51%)]\tLoss: 0.042646\n",
      "Train Epoch: 26 [1360/2566 (53%)]\tLoss: 0.242939\n",
      "Train Epoch: 26 [1400/2566 (55%)]\tLoss: 0.302824\n",
      "Train Epoch: 26 [1440/2566 (56%)]\tLoss: 0.275799\n",
      "Train Epoch: 26 [1480/2566 (58%)]\tLoss: 0.352929\n",
      "Train Epoch: 26 [1520/2566 (59%)]\tLoss: 0.245849\n",
      "Train Epoch: 26 [1560/2566 (61%)]\tLoss: 0.668420\n",
      "Train Epoch: 26 [1600/2566 (62%)]\tLoss: 0.325503\n",
      "Train Epoch: 26 [1640/2566 (64%)]\tLoss: 0.577758\n",
      "Train Epoch: 26 [1680/2566 (65%)]\tLoss: 0.213397\n",
      "Train Epoch: 26 [1720/2566 (67%)]\tLoss: 0.664353\n",
      "Train Epoch: 26 [1760/2566 (69%)]\tLoss: 0.419405\n",
      "Train Epoch: 26 [1800/2566 (70%)]\tLoss: 0.605451\n",
      "Train Epoch: 26 [1840/2566 (72%)]\tLoss: 0.259887\n",
      "Train Epoch: 26 [1880/2566 (73%)]\tLoss: 0.255629\n",
      "Train Epoch: 26 [1920/2566 (75%)]\tLoss: 0.932353\n",
      "Train Epoch: 26 [1960/2566 (76%)]\tLoss: 0.269991\n",
      "Train Epoch: 26 [2000/2566 (78%)]\tLoss: 0.184711\n",
      "Train Epoch: 26 [2040/2566 (79%)]\tLoss: 0.472343\n",
      "Train Epoch: 26 [2080/2566 (81%)]\tLoss: 0.286228\n",
      "Train Epoch: 26 [2120/2566 (83%)]\tLoss: 0.254264\n",
      "Train Epoch: 26 [2160/2566 (84%)]\tLoss: 0.532305\n",
      "Train Epoch: 26 [2200/2566 (86%)]\tLoss: 0.436558\n",
      "Train Epoch: 26 [2240/2566 (87%)]\tLoss: 0.383147\n",
      "Train Epoch: 26 [2280/2566 (89%)]\tLoss: 0.522936\n",
      "Train Epoch: 26 [2320/2566 (90%)]\tLoss: 0.862629\n",
      "Train Epoch: 26 [2360/2566 (92%)]\tLoss: 0.247325\n",
      "Train Epoch: 26 [2400/2566 (93%)]\tLoss: 0.576360\n",
      "Train Epoch: 26 [2440/2566 (95%)]\tLoss: 0.172701\n",
      "Train Epoch: 26 [2480/2566 (97%)]\tLoss: 0.166222\n",
      "Train Epoch: 26 [2520/2566 (98%)]\tLoss: 0.681433\n",
      "Train Epoch: 26 [2560/2566 (100%)]\tLoss: 0.131036\n",
      "epoch:26,loss:0.4020910672611351\n",
      "Train set: Average loss: 0.3047, Accuracy: 2301/2566 (90%)\n",
      "Val set: Average loss: 0.5645, Accuracy: 261/327 (80%)\n",
      "Train Epoch: 27 [40/2566 (2%)]\tLoss: 0.638064\n",
      "Train Epoch: 27 [80/2566 (3%)]\tLoss: 0.263595\n",
      "Train Epoch: 27 [120/2566 (5%)]\tLoss: 0.528426\n",
      "Train Epoch: 27 [160/2566 (6%)]\tLoss: 0.283647\n",
      "Train Epoch: 27 [200/2566 (8%)]\tLoss: 0.475418\n",
      "Train Epoch: 27 [240/2566 (9%)]\tLoss: 0.460326\n",
      "Train Epoch: 27 [280/2566 (11%)]\tLoss: 0.465218\n",
      "Train Epoch: 27 [320/2566 (12%)]\tLoss: 0.266584\n",
      "Train Epoch: 27 [360/2566 (14%)]\tLoss: 0.405838\n",
      "Train Epoch: 27 [400/2566 (16%)]\tLoss: 0.403312\n",
      "Train Epoch: 27 [440/2566 (17%)]\tLoss: 0.522097\n",
      "Train Epoch: 27 [480/2566 (19%)]\tLoss: 0.421924\n",
      "Train Epoch: 27 [520/2566 (20%)]\tLoss: 0.414727\n",
      "Train Epoch: 27 [560/2566 (22%)]\tLoss: 0.068737\n",
      "Train Epoch: 27 [600/2566 (23%)]\tLoss: 0.445563\n",
      "Train Epoch: 27 [640/2566 (25%)]\tLoss: 0.283881\n",
      "Train Epoch: 27 [680/2566 (26%)]\tLoss: 0.351281\n",
      "Train Epoch: 27 [720/2566 (28%)]\tLoss: 1.150115\n",
      "Train Epoch: 27 [760/2566 (30%)]\tLoss: 0.710825\n",
      "Train Epoch: 27 [800/2566 (31%)]\tLoss: 0.142105\n",
      "Train Epoch: 27 [840/2566 (33%)]\tLoss: 0.251263\n",
      "Train Epoch: 27 [880/2566 (34%)]\tLoss: 0.275221\n",
      "Train Epoch: 27 [920/2566 (36%)]\tLoss: 0.420751\n",
      "Train Epoch: 27 [960/2566 (37%)]\tLoss: 0.225456\n",
      "Train Epoch: 27 [1000/2566 (39%)]\tLoss: 0.279773\n",
      "Train Epoch: 27 [1040/2566 (40%)]\tLoss: 0.277469\n",
      "Train Epoch: 27 [1080/2566 (42%)]\tLoss: 0.257443\n",
      "Train Epoch: 27 [1120/2566 (44%)]\tLoss: 0.501472\n",
      "Train Epoch: 27 [1160/2566 (45%)]\tLoss: 0.467170\n",
      "Train Epoch: 27 [1200/2566 (47%)]\tLoss: 0.272281\n",
      "Train Epoch: 27 [1240/2566 (48%)]\tLoss: 0.179091\n",
      "Train Epoch: 27 [1280/2566 (50%)]\tLoss: 0.278262\n",
      "Train Epoch: 27 [1320/2566 (51%)]\tLoss: 0.751684\n",
      "Train Epoch: 27 [1360/2566 (53%)]\tLoss: 0.076797\n",
      "Train Epoch: 27 [1400/2566 (55%)]\tLoss: 0.274891\n",
      "Train Epoch: 27 [1440/2566 (56%)]\tLoss: 0.199584\n",
      "Train Epoch: 27 [1480/2566 (58%)]\tLoss: 0.717523\n",
      "Train Epoch: 27 [1520/2566 (59%)]\tLoss: 0.367486\n",
      "Train Epoch: 27 [1560/2566 (61%)]\tLoss: 0.122284\n",
      "Train Epoch: 27 [1600/2566 (62%)]\tLoss: 0.406104\n",
      "Train Epoch: 27 [1640/2566 (64%)]\tLoss: 0.241099\n",
      "Train Epoch: 27 [1680/2566 (65%)]\tLoss: 0.147088\n",
      "Train Epoch: 27 [1720/2566 (67%)]\tLoss: 0.210046\n",
      "Train Epoch: 27 [1760/2566 (69%)]\tLoss: 0.495305\n",
      "Train Epoch: 27 [1800/2566 (70%)]\tLoss: 0.535095\n",
      "Train Epoch: 27 [1840/2566 (72%)]\tLoss: 0.203403\n",
      "Train Epoch: 27 [1880/2566 (73%)]\tLoss: 0.536877\n",
      "Train Epoch: 27 [1920/2566 (75%)]\tLoss: 0.609944\n",
      "Train Epoch: 27 [1960/2566 (76%)]\tLoss: 0.327674\n",
      "Train Epoch: 27 [2000/2566 (78%)]\tLoss: 0.299530\n",
      "Train Epoch: 27 [2040/2566 (79%)]\tLoss: 0.263769\n",
      "Train Epoch: 27 [2080/2566 (81%)]\tLoss: 0.583056\n",
      "Train Epoch: 27 [2120/2566 (83%)]\tLoss: 0.298814\n",
      "Train Epoch: 27 [2160/2566 (84%)]\tLoss: 0.332492\n",
      "Train Epoch: 27 [2200/2566 (86%)]\tLoss: 0.474043\n",
      "Train Epoch: 27 [2240/2566 (87%)]\tLoss: 0.213075\n",
      "Train Epoch: 27 [2280/2566 (89%)]\tLoss: 0.162349\n",
      "Train Epoch: 27 [2320/2566 (90%)]\tLoss: 0.343757\n",
      "Train Epoch: 27 [2360/2566 (92%)]\tLoss: 0.108933\n",
      "Train Epoch: 27 [2400/2566 (93%)]\tLoss: 0.081345\n",
      "Train Epoch: 27 [2440/2566 (95%)]\tLoss: 0.196854\n",
      "Train Epoch: 27 [2480/2566 (97%)]\tLoss: 0.363203\n",
      "Train Epoch: 27 [2520/2566 (98%)]\tLoss: 0.067616\n",
      "Train Epoch: 27 [2560/2566 (100%)]\tLoss: 0.211500\n",
      "epoch:27,loss:0.3776816209122492\n",
      "Train set: Average loss: 0.3312, Accuracy: 2293/2566 (89%)\n",
      "Val set: Average loss: 0.5824, Accuracy: 264/327 (81%)\n",
      "Train Epoch: 28 [40/2566 (2%)]\tLoss: 0.233873\n",
      "Train Epoch: 28 [80/2566 (3%)]\tLoss: 0.326754\n",
      "Train Epoch: 28 [120/2566 (5%)]\tLoss: 0.693540\n",
      "Train Epoch: 28 [160/2566 (6%)]\tLoss: 0.054428\n",
      "Train Epoch: 28 [200/2566 (8%)]\tLoss: 0.330548\n",
      "Train Epoch: 28 [240/2566 (9%)]\tLoss: 0.070737\n",
      "Train Epoch: 28 [280/2566 (11%)]\tLoss: 0.160149\n",
      "Train Epoch: 28 [320/2566 (12%)]\tLoss: 0.419762\n",
      "Train Epoch: 28 [360/2566 (14%)]\tLoss: 1.185476\n",
      "Train Epoch: 28 [400/2566 (16%)]\tLoss: 0.742034\n",
      "Train Epoch: 28 [440/2566 (17%)]\tLoss: 0.317799\n",
      "Train Epoch: 28 [480/2566 (19%)]\tLoss: 0.322737\n",
      "Train Epoch: 28 [520/2566 (20%)]\tLoss: 0.418368\n",
      "Train Epoch: 28 [560/2566 (22%)]\tLoss: 0.385766\n",
      "Train Epoch: 28 [600/2566 (23%)]\tLoss: 0.435250\n",
      "Train Epoch: 28 [640/2566 (25%)]\tLoss: 0.331647\n",
      "Train Epoch: 28 [680/2566 (26%)]\tLoss: 0.271602\n",
      "Train Epoch: 28 [720/2566 (28%)]\tLoss: 0.187491\n",
      "Train Epoch: 28 [760/2566 (30%)]\tLoss: 0.581112\n",
      "Train Epoch: 28 [800/2566 (31%)]\tLoss: 0.360834\n",
      "Train Epoch: 28 [840/2566 (33%)]\tLoss: 0.695081\n",
      "Train Epoch: 28 [880/2566 (34%)]\tLoss: 0.559746\n",
      "Train Epoch: 28 [920/2566 (36%)]\tLoss: 0.572313\n",
      "Train Epoch: 28 [960/2566 (37%)]\tLoss: 0.427365\n",
      "Train Epoch: 28 [1000/2566 (39%)]\tLoss: 0.107219\n",
      "Train Epoch: 28 [1040/2566 (40%)]\tLoss: 0.101724\n",
      "Train Epoch: 28 [1080/2566 (42%)]\tLoss: 0.252820\n",
      "Train Epoch: 28 [1120/2566 (44%)]\tLoss: 0.375490\n",
      "Train Epoch: 28 [1160/2566 (45%)]\tLoss: 0.061225\n",
      "Train Epoch: 28 [1200/2566 (47%)]\tLoss: 0.294917\n",
      "Train Epoch: 28 [1240/2566 (48%)]\tLoss: 0.979266\n",
      "Train Epoch: 28 [1280/2566 (50%)]\tLoss: 0.360598\n",
      "Train Epoch: 28 [1320/2566 (51%)]\tLoss: 0.097660\n",
      "Train Epoch: 28 [1360/2566 (53%)]\tLoss: 0.407588\n",
      "Train Epoch: 28 [1400/2566 (55%)]\tLoss: 0.135304\n",
      "Train Epoch: 28 [1440/2566 (56%)]\tLoss: 0.670346\n",
      "Train Epoch: 28 [1480/2566 (58%)]\tLoss: 0.079088\n",
      "Train Epoch: 28 [1520/2566 (59%)]\tLoss: 0.379744\n",
      "Train Epoch: 28 [1560/2566 (61%)]\tLoss: 0.623088\n",
      "Train Epoch: 28 [1600/2566 (62%)]\tLoss: 0.338299\n",
      "Train Epoch: 28 [1640/2566 (64%)]\tLoss: 0.161094\n",
      "Train Epoch: 28 [1680/2566 (65%)]\tLoss: 0.354219\n",
      "Train Epoch: 28 [1720/2566 (67%)]\tLoss: 0.689078\n",
      "Train Epoch: 28 [1760/2566 (69%)]\tLoss: 0.261147\n",
      "Train Epoch: 28 [1800/2566 (70%)]\tLoss: 0.233166\n",
      "Train Epoch: 28 [1840/2566 (72%)]\tLoss: 0.191265\n",
      "Train Epoch: 28 [1880/2566 (73%)]\tLoss: 0.353185\n",
      "Train Epoch: 28 [1920/2566 (75%)]\tLoss: 0.535089\n",
      "Train Epoch: 28 [1960/2566 (76%)]\tLoss: 0.117452\n",
      "Train Epoch: 28 [2000/2566 (78%)]\tLoss: 0.131756\n",
      "Train Epoch: 28 [2040/2566 (79%)]\tLoss: 0.379007\n",
      "Train Epoch: 28 [2080/2566 (81%)]\tLoss: 0.416675\n",
      "Train Epoch: 28 [2120/2566 (83%)]\tLoss: 0.096588\n",
      "Train Epoch: 28 [2160/2566 (84%)]\tLoss: 0.214657\n",
      "Train Epoch: 28 [2200/2566 (86%)]\tLoss: 0.294832\n",
      "Train Epoch: 28 [2240/2566 (87%)]\tLoss: 0.213824\n",
      "Train Epoch: 28 [2280/2566 (89%)]\tLoss: 0.302621\n",
      "Train Epoch: 28 [2320/2566 (90%)]\tLoss: 0.447235\n",
      "Train Epoch: 28 [2360/2566 (92%)]\tLoss: 0.321279\n",
      "Train Epoch: 28 [2400/2566 (93%)]\tLoss: 0.142552\n",
      "Train Epoch: 28 [2440/2566 (95%)]\tLoss: 0.662191\n",
      "Train Epoch: 28 [2480/2566 (97%)]\tLoss: 0.307997\n",
      "Train Epoch: 28 [2520/2566 (98%)]\tLoss: 0.324183\n",
      "Train Epoch: 28 [2560/2566 (100%)]\tLoss: 0.263447\n",
      "epoch:28,loss:0.36120226633418756\n",
      "Train set: Average loss: 0.3025, Accuracy: 2319/2566 (90%)\n",
      "Val set: Average loss: 0.6056, Accuracy: 252/327 (77%)\n",
      "Train Epoch: 29 [40/2566 (2%)]\tLoss: 0.663223\n",
      "Train Epoch: 29 [80/2566 (3%)]\tLoss: 0.151676\n",
      "Train Epoch: 29 [120/2566 (5%)]\tLoss: 0.204310\n",
      "Train Epoch: 29 [160/2566 (6%)]\tLoss: 0.133645\n",
      "Train Epoch: 29 [200/2566 (8%)]\tLoss: 0.711701\n",
      "Train Epoch: 29 [240/2566 (9%)]\tLoss: 0.077249\n",
      "Train Epoch: 29 [280/2566 (11%)]\tLoss: 0.307116\n",
      "Train Epoch: 29 [320/2566 (12%)]\tLoss: 0.584753\n",
      "Train Epoch: 29 [360/2566 (14%)]\tLoss: 0.203042\n",
      "Train Epoch: 29 [400/2566 (16%)]\tLoss: 0.217076\n",
      "Train Epoch: 29 [440/2566 (17%)]\tLoss: 0.209830\n",
      "Train Epoch: 29 [480/2566 (19%)]\tLoss: 0.293806\n",
      "Train Epoch: 29 [520/2566 (20%)]\tLoss: 0.133405\n",
      "Train Epoch: 29 [560/2566 (22%)]\tLoss: 0.939999\n",
      "Train Epoch: 29 [600/2566 (23%)]\tLoss: 0.163675\n",
      "Train Epoch: 29 [640/2566 (25%)]\tLoss: 0.210656\n",
      "Train Epoch: 29 [680/2566 (26%)]\tLoss: 0.171700\n",
      "Train Epoch: 29 [720/2566 (28%)]\tLoss: 0.328684\n",
      "Train Epoch: 29 [760/2566 (30%)]\tLoss: 0.654343\n",
      "Train Epoch: 29 [800/2566 (31%)]\tLoss: 0.191938\n",
      "Train Epoch: 29 [840/2566 (33%)]\tLoss: 0.211146\n",
      "Train Epoch: 29 [880/2566 (34%)]\tLoss: 0.156102\n",
      "Train Epoch: 29 [920/2566 (36%)]\tLoss: 0.105604\n",
      "Train Epoch: 29 [960/2566 (37%)]\tLoss: 0.342335\n",
      "Train Epoch: 29 [1000/2566 (39%)]\tLoss: 0.573881\n",
      "Train Epoch: 29 [1040/2566 (40%)]\tLoss: 0.300554\n",
      "Train Epoch: 29 [1080/2566 (42%)]\tLoss: 0.535979\n",
      "Train Epoch: 29 [1120/2566 (44%)]\tLoss: 0.447981\n",
      "Train Epoch: 29 [1160/2566 (45%)]\tLoss: 0.589369\n",
      "Train Epoch: 29 [1200/2566 (47%)]\tLoss: 0.421984\n",
      "Train Epoch: 29 [1240/2566 (48%)]\tLoss: 0.107879\n",
      "Train Epoch: 29 [1280/2566 (50%)]\tLoss: 0.059158\n",
      "Train Epoch: 29 [1320/2566 (51%)]\tLoss: 0.455048\n",
      "Train Epoch: 29 [1360/2566 (53%)]\tLoss: 0.270091\n",
      "Train Epoch: 29 [1400/2566 (55%)]\tLoss: 0.254949\n",
      "Train Epoch: 29 [1440/2566 (56%)]\tLoss: 0.102713\n",
      "Train Epoch: 29 [1480/2566 (58%)]\tLoss: 0.212432\n",
      "Train Epoch: 29 [1520/2566 (59%)]\tLoss: 0.250220\n",
      "Train Epoch: 29 [1560/2566 (61%)]\tLoss: 0.305411\n",
      "Train Epoch: 29 [1600/2566 (62%)]\tLoss: 0.819207\n",
      "Train Epoch: 29 [1640/2566 (64%)]\tLoss: 0.334941\n",
      "Train Epoch: 29 [1680/2566 (65%)]\tLoss: 0.103087\n",
      "Train Epoch: 29 [1720/2566 (67%)]\tLoss: 0.560450\n",
      "Train Epoch: 29 [1760/2566 (69%)]\tLoss: 0.703598\n",
      "Train Epoch: 29 [1800/2566 (70%)]\tLoss: 0.287331\n",
      "Train Epoch: 29 [1840/2566 (72%)]\tLoss: 0.435284\n",
      "Train Epoch: 29 [1880/2566 (73%)]\tLoss: 0.100826\n",
      "Train Epoch: 29 [1920/2566 (75%)]\tLoss: 0.142348\n",
      "Train Epoch: 29 [1960/2566 (76%)]\tLoss: 0.468802\n",
      "Train Epoch: 29 [2000/2566 (78%)]\tLoss: 0.042674\n",
      "Train Epoch: 29 [2040/2566 (79%)]\tLoss: 0.503415\n",
      "Train Epoch: 29 [2080/2566 (81%)]\tLoss: 0.580715\n",
      "Train Epoch: 29 [2120/2566 (83%)]\tLoss: 0.426354\n",
      "Train Epoch: 29 [2160/2566 (84%)]\tLoss: 0.226486\n",
      "Train Epoch: 29 [2200/2566 (86%)]\tLoss: 0.512352\n",
      "Train Epoch: 29 [2240/2566 (87%)]\tLoss: 0.800372\n",
      "Train Epoch: 29 [2280/2566 (89%)]\tLoss: 0.644356\n",
      "Train Epoch: 29 [2320/2566 (90%)]\tLoss: 0.234344\n",
      "Train Epoch: 29 [2360/2566 (92%)]\tLoss: 0.629667\n",
      "Train Epoch: 29 [2400/2566 (93%)]\tLoss: 0.175728\n",
      "Train Epoch: 29 [2440/2566 (95%)]\tLoss: 0.114008\n",
      "Train Epoch: 29 [2480/2566 (97%)]\tLoss: 0.487473\n",
      "Train Epoch: 29 [2520/2566 (98%)]\tLoss: 0.109282\n",
      "Train Epoch: 29 [2560/2566 (100%)]\tLoss: 0.391177\n",
      "epoch:29,loss:0.34152164208090563\n",
      "Train set: Average loss: 0.2751, Accuracy: 2332/2566 (91%)\n",
      "Val set: Average loss: 0.5989, Accuracy: 255/327 (78%)\n",
      "Train Epoch: 30 [40/2566 (2%)]\tLoss: 0.105012\n",
      "Train Epoch: 30 [80/2566 (3%)]\tLoss: 0.075871\n",
      "Train Epoch: 30 [120/2566 (5%)]\tLoss: 0.255756\n",
      "Train Epoch: 30 [160/2566 (6%)]\tLoss: 0.104999\n",
      "Train Epoch: 30 [200/2566 (8%)]\tLoss: 0.093418\n",
      "Train Epoch: 30 [240/2566 (9%)]\tLoss: 0.079730\n",
      "Train Epoch: 30 [280/2566 (11%)]\tLoss: 0.092533\n",
      "Train Epoch: 30 [320/2566 (12%)]\tLoss: 0.186424\n",
      "Train Epoch: 30 [360/2566 (14%)]\tLoss: 0.275798\n",
      "Train Epoch: 30 [400/2566 (16%)]\tLoss: 0.272937\n",
      "Train Epoch: 30 [440/2566 (17%)]\tLoss: 0.098078\n",
      "Train Epoch: 30 [480/2566 (19%)]\tLoss: 0.217546\n",
      "Train Epoch: 30 [520/2566 (20%)]\tLoss: 0.049607\n",
      "Train Epoch: 30 [560/2566 (22%)]\tLoss: 0.183957\n",
      "Train Epoch: 30 [600/2566 (23%)]\tLoss: 0.176601\n",
      "Train Epoch: 30 [640/2566 (25%)]\tLoss: 0.148646\n",
      "Train Epoch: 30 [680/2566 (26%)]\tLoss: 0.313237\n",
      "Train Epoch: 30 [720/2566 (28%)]\tLoss: 0.231505\n",
      "Train Epoch: 30 [760/2566 (30%)]\tLoss: 0.144462\n",
      "Train Epoch: 30 [800/2566 (31%)]\tLoss: 0.087406\n",
      "Train Epoch: 30 [840/2566 (33%)]\tLoss: 0.743908\n",
      "Train Epoch: 30 [880/2566 (34%)]\tLoss: 0.312931\n",
      "Train Epoch: 30 [920/2566 (36%)]\tLoss: 0.170349\n",
      "Train Epoch: 30 [960/2566 (37%)]\tLoss: 0.398308\n",
      "Train Epoch: 30 [1000/2566 (39%)]\tLoss: 0.234387\n",
      "Train Epoch: 30 [1040/2566 (40%)]\tLoss: 0.159358\n",
      "Train Epoch: 30 [1080/2566 (42%)]\tLoss: 0.223502\n",
      "Train Epoch: 30 [1120/2566 (44%)]\tLoss: 0.238675\n",
      "Train Epoch: 30 [1160/2566 (45%)]\tLoss: 0.666179\n",
      "Train Epoch: 30 [1200/2566 (47%)]\tLoss: 0.189285\n",
      "Train Epoch: 30 [1240/2566 (48%)]\tLoss: 0.318452\n",
      "Train Epoch: 30 [1280/2566 (50%)]\tLoss: 0.265255\n",
      "Train Epoch: 30 [1320/2566 (51%)]\tLoss: 0.547096\n",
      "Train Epoch: 30 [1360/2566 (53%)]\tLoss: 0.301398\n",
      "Train Epoch: 30 [1400/2566 (55%)]\tLoss: 1.234291\n",
      "Train Epoch: 30 [1440/2566 (56%)]\tLoss: 0.287127\n",
      "Train Epoch: 30 [1480/2566 (58%)]\tLoss: 0.404940\n",
      "Train Epoch: 30 [1520/2566 (59%)]\tLoss: 0.094123\n",
      "Train Epoch: 30 [1560/2566 (61%)]\tLoss: 0.747962\n",
      "Train Epoch: 30 [1600/2566 (62%)]\tLoss: 0.099678\n",
      "Train Epoch: 30 [1640/2566 (64%)]\tLoss: 0.487219\n",
      "Train Epoch: 30 [1680/2566 (65%)]\tLoss: 0.563154\n",
      "Train Epoch: 30 [1720/2566 (67%)]\tLoss: 0.314733\n",
      "Train Epoch: 30 [1760/2566 (69%)]\tLoss: 0.476516\n",
      "Train Epoch: 30 [1800/2566 (70%)]\tLoss: 0.087021\n",
      "Train Epoch: 30 [1840/2566 (72%)]\tLoss: 0.195826\n",
      "Train Epoch: 30 [1880/2566 (73%)]\tLoss: 0.446230\n",
      "Train Epoch: 30 [1920/2566 (75%)]\tLoss: 0.030778\n",
      "Train Epoch: 30 [1960/2566 (76%)]\tLoss: 0.120704\n",
      "Train Epoch: 30 [2000/2566 (78%)]\tLoss: 0.477896\n",
      "Train Epoch: 30 [2040/2566 (79%)]\tLoss: 0.158443\n",
      "Train Epoch: 30 [2080/2566 (81%)]\tLoss: 0.140030\n",
      "Train Epoch: 30 [2120/2566 (83%)]\tLoss: 0.181735\n",
      "Train Epoch: 30 [2160/2566 (84%)]\tLoss: 0.366076\n",
      "Train Epoch: 30 [2200/2566 (86%)]\tLoss: 0.339123\n",
      "Train Epoch: 30 [2240/2566 (87%)]\tLoss: 0.573406\n",
      "Train Epoch: 30 [2280/2566 (89%)]\tLoss: 0.671480\n",
      "Train Epoch: 30 [2320/2566 (90%)]\tLoss: 0.022430\n",
      "Train Epoch: 30 [2360/2566 (92%)]\tLoss: 0.343060\n",
      "Train Epoch: 30 [2400/2566 (93%)]\tLoss: 0.447836\n",
      "Train Epoch: 30 [2440/2566 (95%)]\tLoss: 0.421468\n",
      "Train Epoch: 30 [2480/2566 (97%)]\tLoss: 0.090128\n",
      "Train Epoch: 30 [2520/2566 (98%)]\tLoss: 0.268441\n",
      "Train Epoch: 30 [2560/2566 (100%)]\tLoss: 0.488457\n",
      "epoch:30,loss:0.31882243515848185\n",
      "Train set: Average loss: 0.2534, Accuracy: 2354/2566 (92%)\n",
      "Val set: Average loss: 0.5888, Accuracy: 256/327 (78%)\n",
      "Train Epoch: 31 [40/2566 (2%)]\tLoss: 0.745435\n",
      "Train Epoch: 31 [80/2566 (3%)]\tLoss: 0.109390\n",
      "Train Epoch: 31 [120/2566 (5%)]\tLoss: 0.256282\n",
      "Train Epoch: 31 [160/2566 (6%)]\tLoss: 1.034775\n",
      "Train Epoch: 31 [200/2566 (8%)]\tLoss: 0.720490\n",
      "Train Epoch: 31 [240/2566 (9%)]\tLoss: 0.122933\n",
      "Train Epoch: 31 [280/2566 (11%)]\tLoss: 0.500146\n",
      "Train Epoch: 31 [320/2566 (12%)]\tLoss: 0.301815\n",
      "Train Epoch: 31 [360/2566 (14%)]\tLoss: 0.356803\n",
      "Train Epoch: 31 [400/2566 (16%)]\tLoss: 0.158852\n",
      "Train Epoch: 31 [440/2566 (17%)]\tLoss: 0.231955\n",
      "Train Epoch: 31 [480/2566 (19%)]\tLoss: 0.516653\n",
      "Train Epoch: 31 [520/2566 (20%)]\tLoss: 0.460351\n",
      "Train Epoch: 31 [560/2566 (22%)]\tLoss: 0.748034\n",
      "Train Epoch: 31 [600/2566 (23%)]\tLoss: 0.609629\n",
      "Train Epoch: 31 [640/2566 (25%)]\tLoss: 0.144425\n",
      "Train Epoch: 31 [680/2566 (26%)]\tLoss: 0.371776\n",
      "Train Epoch: 31 [720/2566 (28%)]\tLoss: 0.633657\n",
      "Train Epoch: 31 [760/2566 (30%)]\tLoss: 0.576468\n",
      "Train Epoch: 31 [800/2566 (31%)]\tLoss: 0.545012\n",
      "Train Epoch: 31 [840/2566 (33%)]\tLoss: 0.441843\n",
      "Train Epoch: 31 [880/2566 (34%)]\tLoss: 0.122839\n",
      "Train Epoch: 31 [920/2566 (36%)]\tLoss: 0.291158\n",
      "Train Epoch: 31 [960/2566 (37%)]\tLoss: 0.611455\n",
      "Train Epoch: 31 [1000/2566 (39%)]\tLoss: 0.114847\n",
      "Train Epoch: 31 [1040/2566 (40%)]\tLoss: 0.257200\n",
      "Train Epoch: 31 [1080/2566 (42%)]\tLoss: 0.499183\n",
      "Train Epoch: 31 [1120/2566 (44%)]\tLoss: 0.146645\n",
      "Train Epoch: 31 [1160/2566 (45%)]\tLoss: 0.287154\n",
      "Train Epoch: 31 [1200/2566 (47%)]\tLoss: 0.118743\n",
      "Train Epoch: 31 [1240/2566 (48%)]\tLoss: 0.334760\n",
      "Train Epoch: 31 [1280/2566 (50%)]\tLoss: 0.119502\n",
      "Train Epoch: 31 [1320/2566 (51%)]\tLoss: 0.197338\n",
      "Train Epoch: 31 [1360/2566 (53%)]\tLoss: 0.231482\n",
      "Train Epoch: 31 [1400/2566 (55%)]\tLoss: 0.127797\n",
      "Train Epoch: 31 [1440/2566 (56%)]\tLoss: 0.040103\n",
      "Train Epoch: 31 [1480/2566 (58%)]\tLoss: 0.222524\n",
      "Train Epoch: 31 [1520/2566 (59%)]\tLoss: 0.542068\n",
      "Train Epoch: 31 [1560/2566 (61%)]\tLoss: 0.133962\n",
      "Train Epoch: 31 [1600/2566 (62%)]\tLoss: 0.472264\n",
      "Train Epoch: 31 [1640/2566 (64%)]\tLoss: 0.296952\n",
      "Train Epoch: 31 [1680/2566 (65%)]\tLoss: 0.309260\n",
      "Train Epoch: 31 [1720/2566 (67%)]\tLoss: 0.270559\n",
      "Train Epoch: 31 [1760/2566 (69%)]\tLoss: 0.129899\n",
      "Train Epoch: 31 [1800/2566 (70%)]\tLoss: 0.293914\n",
      "Train Epoch: 31 [1840/2566 (72%)]\tLoss: 0.039831\n",
      "Train Epoch: 31 [1880/2566 (73%)]\tLoss: 0.677602\n",
      "Train Epoch: 31 [1920/2566 (75%)]\tLoss: 0.505755\n",
      "Train Epoch: 31 [1960/2566 (76%)]\tLoss: 0.079067\n",
      "Train Epoch: 31 [2000/2566 (78%)]\tLoss: 0.162027\n",
      "Train Epoch: 31 [2040/2566 (79%)]\tLoss: 0.280788\n",
      "Train Epoch: 31 [2080/2566 (81%)]\tLoss: 0.259681\n",
      "Train Epoch: 31 [2120/2566 (83%)]\tLoss: 0.312813\n",
      "Train Epoch: 31 [2160/2566 (84%)]\tLoss: 0.075772\n",
      "Train Epoch: 31 [2200/2566 (86%)]\tLoss: 0.678047\n",
      "Train Epoch: 31 [2240/2566 (87%)]\tLoss: 0.367895\n",
      "Train Epoch: 31 [2280/2566 (89%)]\tLoss: 0.088338\n",
      "Train Epoch: 31 [2320/2566 (90%)]\tLoss: 0.395227\n",
      "Train Epoch: 31 [2360/2566 (92%)]\tLoss: 0.076829\n",
      "Train Epoch: 31 [2400/2566 (93%)]\tLoss: 0.327044\n",
      "Train Epoch: 31 [2440/2566 (95%)]\tLoss: 0.104851\n",
      "Train Epoch: 31 [2480/2566 (97%)]\tLoss: 0.080883\n",
      "Train Epoch: 31 [2520/2566 (98%)]\tLoss: 0.149660\n",
      "Train Epoch: 31 [2560/2566 (100%)]\tLoss: 0.159082\n",
      "epoch:31,loss:0.29984990402916883\n",
      "Train set: Average loss: 0.2438, Accuracy: 2342/2566 (91%)\n",
      "Val set: Average loss: 0.6256, Accuracy: 256/327 (78%)\n",
      "Train Epoch: 32 [40/2566 (2%)]\tLoss: 0.116179\n",
      "Train Epoch: 32 [80/2566 (3%)]\tLoss: 0.089965\n",
      "Train Epoch: 32 [120/2566 (5%)]\tLoss: 0.239013\n",
      "Train Epoch: 32 [160/2566 (6%)]\tLoss: 0.133234\n",
      "Train Epoch: 32 [200/2566 (8%)]\tLoss: 0.380569\n",
      "Train Epoch: 32 [240/2566 (9%)]\tLoss: 0.086618\n",
      "Train Epoch: 32 [280/2566 (11%)]\tLoss: 0.278207\n",
      "Train Epoch: 32 [320/2566 (12%)]\tLoss: 0.241528\n",
      "Train Epoch: 32 [360/2566 (14%)]\tLoss: 0.124640\n",
      "Train Epoch: 32 [400/2566 (16%)]\tLoss: 0.323164\n",
      "Train Epoch: 32 [440/2566 (17%)]\tLoss: 0.059091\n",
      "Train Epoch: 32 [480/2566 (19%)]\tLoss: 0.745263\n",
      "Train Epoch: 32 [520/2566 (20%)]\tLoss: 0.167252\n",
      "Train Epoch: 32 [560/2566 (22%)]\tLoss: 0.243919\n",
      "Train Epoch: 32 [600/2566 (23%)]\tLoss: 0.374263\n",
      "Train Epoch: 32 [640/2566 (25%)]\tLoss: 0.524083\n",
      "Train Epoch: 32 [680/2566 (26%)]\tLoss: 0.353998\n",
      "Train Epoch: 32 [720/2566 (28%)]\tLoss: 0.052892\n",
      "Train Epoch: 32 [760/2566 (30%)]\tLoss: 0.520588\n",
      "Train Epoch: 32 [800/2566 (31%)]\tLoss: 0.158036\n",
      "Train Epoch: 32 [840/2566 (33%)]\tLoss: 0.293317\n",
      "Train Epoch: 32 [880/2566 (34%)]\tLoss: 0.576078\n",
      "Train Epoch: 32 [920/2566 (36%)]\tLoss: 0.087875\n",
      "Train Epoch: 32 [960/2566 (37%)]\tLoss: 0.319291\n",
      "Train Epoch: 32 [1000/2566 (39%)]\tLoss: 0.381560\n",
      "Train Epoch: 32 [1040/2566 (40%)]\tLoss: 0.134351\n",
      "Train Epoch: 32 [1080/2566 (42%)]\tLoss: 0.122076\n",
      "Train Epoch: 32 [1120/2566 (44%)]\tLoss: 0.086887\n",
      "Train Epoch: 32 [1160/2566 (45%)]\tLoss: 0.146513\n",
      "Train Epoch: 32 [1200/2566 (47%)]\tLoss: 0.091340\n",
      "Train Epoch: 32 [1240/2566 (48%)]\tLoss: 0.465350\n",
      "Train Epoch: 32 [1280/2566 (50%)]\tLoss: 0.246056\n",
      "Train Epoch: 32 [1320/2566 (51%)]\tLoss: 0.141891\n",
      "Train Epoch: 32 [1360/2566 (53%)]\tLoss: 0.572152\n",
      "Train Epoch: 32 [1400/2566 (55%)]\tLoss: 0.424505\n",
      "Train Epoch: 32 [1440/2566 (56%)]\tLoss: 0.560974\n",
      "Train Epoch: 32 [1480/2566 (58%)]\tLoss: 0.269894\n",
      "Train Epoch: 32 [1520/2566 (59%)]\tLoss: 0.066662\n",
      "Train Epoch: 32 [1560/2566 (61%)]\tLoss: 0.217349\n",
      "Train Epoch: 32 [1600/2566 (62%)]\tLoss: 0.202165\n",
      "Train Epoch: 32 [1640/2566 (64%)]\tLoss: 0.662674\n",
      "Train Epoch: 32 [1680/2566 (65%)]\tLoss: 0.225293\n",
      "Train Epoch: 32 [1720/2566 (67%)]\tLoss: 0.144795\n",
      "Train Epoch: 32 [1760/2566 (69%)]\tLoss: 0.328848\n",
      "Train Epoch: 32 [1800/2566 (70%)]\tLoss: 0.104422\n",
      "Train Epoch: 32 [1840/2566 (72%)]\tLoss: 0.094124\n",
      "Train Epoch: 32 [1880/2566 (73%)]\tLoss: 0.279155\n",
      "Train Epoch: 32 [1920/2566 (75%)]\tLoss: 0.330811\n",
      "Train Epoch: 32 [1960/2566 (76%)]\tLoss: 0.283365\n",
      "Train Epoch: 32 [2000/2566 (78%)]\tLoss: 0.317718\n",
      "Train Epoch: 32 [2040/2566 (79%)]\tLoss: 0.312681\n",
      "Train Epoch: 32 [2080/2566 (81%)]\tLoss: 0.284640\n",
      "Train Epoch: 32 [2120/2566 (83%)]\tLoss: 0.444748\n",
      "Train Epoch: 32 [2160/2566 (84%)]\tLoss: 0.126979\n",
      "Train Epoch: 32 [2200/2566 (86%)]\tLoss: 0.387693\n",
      "Train Epoch: 32 [2240/2566 (87%)]\tLoss: 0.407768\n",
      "Train Epoch: 32 [2280/2566 (89%)]\tLoss: 0.317000\n",
      "Train Epoch: 32 [2320/2566 (90%)]\tLoss: 0.117726\n",
      "Train Epoch: 32 [2360/2566 (92%)]\tLoss: 0.308453\n",
      "Train Epoch: 32 [2400/2566 (93%)]\tLoss: 0.227841\n",
      "Train Epoch: 32 [2440/2566 (95%)]\tLoss: 0.287413\n",
      "Train Epoch: 32 [2480/2566 (97%)]\tLoss: 0.468761\n",
      "Train Epoch: 32 [2520/2566 (98%)]\tLoss: 0.386627\n",
      "Train Epoch: 32 [2560/2566 (100%)]\tLoss: 0.129848\n",
      "epoch:32,loss:0.28965345785207464\n",
      "Train set: Average loss: 0.2212, Accuracy: 2367/2566 (92%)\n",
      "Val set: Average loss: 0.5896, Accuracy: 258/327 (79%)\n",
      "Train Epoch: 33 [40/2566 (2%)]\tLoss: 0.117488\n",
      "Train Epoch: 33 [80/2566 (3%)]\tLoss: 0.440883\n",
      "Train Epoch: 33 [120/2566 (5%)]\tLoss: 0.482926\n",
      "Train Epoch: 33 [160/2566 (6%)]\tLoss: 0.315862\n",
      "Train Epoch: 33 [200/2566 (8%)]\tLoss: 0.145705\n",
      "Train Epoch: 33 [240/2566 (9%)]\tLoss: 0.735042\n",
      "Train Epoch: 33 [280/2566 (11%)]\tLoss: 0.304032\n",
      "Train Epoch: 33 [320/2566 (12%)]\tLoss: 0.265845\n",
      "Train Epoch: 33 [360/2566 (14%)]\tLoss: 0.055181\n",
      "Train Epoch: 33 [400/2566 (16%)]\tLoss: 0.124529\n",
      "Train Epoch: 33 [440/2566 (17%)]\tLoss: 0.133528\n",
      "Train Epoch: 33 [480/2566 (19%)]\tLoss: 0.408136\n",
      "Train Epoch: 33 [520/2566 (20%)]\tLoss: 0.187807\n",
      "Train Epoch: 33 [560/2566 (22%)]\tLoss: 0.212351\n",
      "Train Epoch: 33 [600/2566 (23%)]\tLoss: 0.129529\n",
      "Train Epoch: 33 [640/2566 (25%)]\tLoss: 0.607290\n",
      "Train Epoch: 33 [680/2566 (26%)]\tLoss: 0.807732\n",
      "Train Epoch: 33 [720/2566 (28%)]\tLoss: 0.327966\n",
      "Train Epoch: 33 [760/2566 (30%)]\tLoss: 0.147471\n",
      "Train Epoch: 33 [800/2566 (31%)]\tLoss: 0.079272\n",
      "Train Epoch: 33 [840/2566 (33%)]\tLoss: 0.422141\n",
      "Train Epoch: 33 [880/2566 (34%)]\tLoss: 0.783428\n",
      "Train Epoch: 33 [920/2566 (36%)]\tLoss: 0.446637\n",
      "Train Epoch: 33 [960/2566 (37%)]\tLoss: 0.370638\n",
      "Train Epoch: 33 [1000/2566 (39%)]\tLoss: 0.129355\n",
      "Train Epoch: 33 [1040/2566 (40%)]\tLoss: 0.430338\n",
      "Train Epoch: 33 [1080/2566 (42%)]\tLoss: 0.252935\n",
      "Train Epoch: 33 [1120/2566 (44%)]\tLoss: 0.524491\n",
      "Train Epoch: 33 [1160/2566 (45%)]\tLoss: 0.175005\n",
      "Train Epoch: 33 [1200/2566 (47%)]\tLoss: 0.030902\n",
      "Train Epoch: 33 [1240/2566 (48%)]\tLoss: 0.349842\n",
      "Train Epoch: 33 [1280/2566 (50%)]\tLoss: 0.290752\n",
      "Train Epoch: 33 [1320/2566 (51%)]\tLoss: 0.456846\n",
      "Train Epoch: 33 [1360/2566 (53%)]\tLoss: 0.387418\n",
      "Train Epoch: 33 [1400/2566 (55%)]\tLoss: 0.140591\n",
      "Train Epoch: 33 [1440/2566 (56%)]\tLoss: 0.514943\n",
      "Train Epoch: 33 [1480/2566 (58%)]\tLoss: 0.441212\n",
      "Train Epoch: 33 [1520/2566 (59%)]\tLoss: 0.184996\n",
      "Train Epoch: 33 [1560/2566 (61%)]\tLoss: 0.079914\n",
      "Train Epoch: 33 [1600/2566 (62%)]\tLoss: 0.044097\n",
      "Train Epoch: 33 [1640/2566 (64%)]\tLoss: 0.158483\n",
      "Train Epoch: 33 [1680/2566 (65%)]\tLoss: 0.268194\n",
      "Train Epoch: 33 [1720/2566 (67%)]\tLoss: 0.161940\n",
      "Train Epoch: 33 [1760/2566 (69%)]\tLoss: 0.264863\n",
      "Train Epoch: 33 [1800/2566 (70%)]\tLoss: 0.071975\n",
      "Train Epoch: 33 [1840/2566 (72%)]\tLoss: 0.503863\n",
      "Train Epoch: 33 [1880/2566 (73%)]\tLoss: 0.195559\n",
      "Train Epoch: 33 [1920/2566 (75%)]\tLoss: 0.187045\n",
      "Train Epoch: 33 [1960/2566 (76%)]\tLoss: 0.043043\n",
      "Train Epoch: 33 [2000/2566 (78%)]\tLoss: 0.225004\n",
      "Train Epoch: 33 [2040/2566 (79%)]\tLoss: 0.301484\n",
      "Train Epoch: 33 [2080/2566 (81%)]\tLoss: 0.202246\n",
      "Train Epoch: 33 [2120/2566 (83%)]\tLoss: 0.668196\n",
      "Train Epoch: 33 [2160/2566 (84%)]\tLoss: 0.367127\n",
      "Train Epoch: 33 [2200/2566 (86%)]\tLoss: 0.105985\n",
      "Train Epoch: 33 [2240/2566 (87%)]\tLoss: 0.411538\n",
      "Train Epoch: 33 [2280/2566 (89%)]\tLoss: 0.063722\n",
      "Train Epoch: 33 [2320/2566 (90%)]\tLoss: 0.239625\n",
      "Train Epoch: 33 [2360/2566 (92%)]\tLoss: 0.445345\n",
      "Train Epoch: 33 [2400/2566 (93%)]\tLoss: 0.612993\n",
      "Train Epoch: 33 [2440/2566 (95%)]\tLoss: 0.141773\n",
      "Train Epoch: 33 [2480/2566 (97%)]\tLoss: 0.209582\n",
      "Train Epoch: 33 [2520/2566 (98%)]\tLoss: 0.277948\n",
      "Train Epoch: 33 [2560/2566 (100%)]\tLoss: 0.408627\n",
      "epoch:33,loss:0.27973392942914527\n",
      "Train set: Average loss: 0.3023, Accuracy: 2307/2566 (90%)\n",
      "Val set: Average loss: 0.6765, Accuracy: 244/327 (75%)\n",
      "Train Epoch: 34 [40/2566 (2%)]\tLoss: 0.181997\n",
      "Train Epoch: 34 [80/2566 (3%)]\tLoss: 0.152482\n",
      "Train Epoch: 34 [120/2566 (5%)]\tLoss: 0.494567\n",
      "Train Epoch: 34 [160/2566 (6%)]\tLoss: 0.044291\n",
      "Train Epoch: 34 [200/2566 (8%)]\tLoss: 0.319405\n",
      "Train Epoch: 34 [240/2566 (9%)]\tLoss: 0.090159\n",
      "Train Epoch: 34 [280/2566 (11%)]\tLoss: 0.052952\n",
      "Train Epoch: 34 [320/2566 (12%)]\tLoss: 0.542928\n",
      "Train Epoch: 34 [360/2566 (14%)]\tLoss: 0.210970\n",
      "Train Epoch: 34 [400/2566 (16%)]\tLoss: 0.404406\n",
      "Train Epoch: 34 [440/2566 (17%)]\tLoss: 0.117691\n",
      "Train Epoch: 34 [480/2566 (19%)]\tLoss: 0.186463\n",
      "Train Epoch: 34 [520/2566 (20%)]\tLoss: 0.053165\n",
      "Train Epoch: 34 [560/2566 (22%)]\tLoss: 0.267504\n",
      "Train Epoch: 34 [600/2566 (23%)]\tLoss: 0.179607\n",
      "Train Epoch: 34 [640/2566 (25%)]\tLoss: 0.240732\n",
      "Train Epoch: 34 [680/2566 (26%)]\tLoss: 0.045539\n",
      "Train Epoch: 34 [720/2566 (28%)]\tLoss: 0.151511\n",
      "Train Epoch: 34 [760/2566 (30%)]\tLoss: 0.083180\n",
      "Train Epoch: 34 [800/2566 (31%)]\tLoss: 0.307121\n",
      "Train Epoch: 34 [840/2566 (33%)]\tLoss: 0.497003\n",
      "Train Epoch: 34 [880/2566 (34%)]\tLoss: 0.103548\n",
      "Train Epoch: 34 [920/2566 (36%)]\tLoss: 0.368857\n",
      "Train Epoch: 34 [960/2566 (37%)]\tLoss: 0.180166\n",
      "Train Epoch: 34 [1000/2566 (39%)]\tLoss: 0.254719\n",
      "Train Epoch: 34 [1040/2566 (40%)]\tLoss: 0.123851\n",
      "Train Epoch: 34 [1080/2566 (42%)]\tLoss: 0.200515\n",
      "Train Epoch: 34 [1120/2566 (44%)]\tLoss: 0.253412\n",
      "Train Epoch: 34 [1160/2566 (45%)]\tLoss: 0.222463\n",
      "Train Epoch: 34 [1200/2566 (47%)]\tLoss: 0.281125\n",
      "Train Epoch: 34 [1240/2566 (48%)]\tLoss: 0.634339\n",
      "Train Epoch: 34 [1280/2566 (50%)]\tLoss: 0.220481\n",
      "Train Epoch: 34 [1320/2566 (51%)]\tLoss: 0.249276\n",
      "Train Epoch: 34 [1360/2566 (53%)]\tLoss: 0.243073\n",
      "Train Epoch: 34 [1400/2566 (55%)]\tLoss: 0.120754\n",
      "Train Epoch: 34 [1440/2566 (56%)]\tLoss: 0.176146\n",
      "Train Epoch: 34 [1480/2566 (58%)]\tLoss: 0.415271\n",
      "Train Epoch: 34 [1520/2566 (59%)]\tLoss: 0.603293\n",
      "Train Epoch: 34 [1560/2566 (61%)]\tLoss: 0.347647\n",
      "Train Epoch: 34 [1600/2566 (62%)]\tLoss: 0.180892\n",
      "Train Epoch: 34 [1640/2566 (64%)]\tLoss: 0.344088\n",
      "Train Epoch: 34 [1680/2566 (65%)]\tLoss: 0.279538\n",
      "Train Epoch: 34 [1720/2566 (67%)]\tLoss: 0.149564\n",
      "Train Epoch: 34 [1760/2566 (69%)]\tLoss: 0.204353\n",
      "Train Epoch: 34 [1800/2566 (70%)]\tLoss: 0.031456\n",
      "Train Epoch: 34 [1840/2566 (72%)]\tLoss: 0.240154\n",
      "Train Epoch: 34 [1880/2566 (73%)]\tLoss: 0.066572\n",
      "Train Epoch: 34 [1920/2566 (75%)]\tLoss: 0.166709\n",
      "Train Epoch: 34 [1960/2566 (76%)]\tLoss: 0.309873\n",
      "Train Epoch: 34 [2000/2566 (78%)]\tLoss: 0.469232\n",
      "Train Epoch: 34 [2040/2566 (79%)]\tLoss: 0.209992\n",
      "Train Epoch: 34 [2080/2566 (81%)]\tLoss: 0.275510\n",
      "Train Epoch: 34 [2120/2566 (83%)]\tLoss: 0.051798\n",
      "Train Epoch: 34 [2160/2566 (84%)]\tLoss: 0.080179\n",
      "Train Epoch: 34 [2200/2566 (86%)]\tLoss: 0.199603\n",
      "Train Epoch: 34 [2240/2566 (87%)]\tLoss: 0.409836\n",
      "Train Epoch: 34 [2280/2566 (89%)]\tLoss: 0.310543\n",
      "Train Epoch: 34 [2320/2566 (90%)]\tLoss: 0.488407\n",
      "Train Epoch: 34 [2360/2566 (92%)]\tLoss: 0.146207\n",
      "Train Epoch: 34 [2400/2566 (93%)]\tLoss: 0.483451\n",
      "Train Epoch: 34 [2440/2566 (95%)]\tLoss: 0.204261\n",
      "Train Epoch: 34 [2480/2566 (97%)]\tLoss: 0.076063\n",
      "Train Epoch: 34 [2520/2566 (98%)]\tLoss: 0.338463\n",
      "Train Epoch: 34 [2560/2566 (100%)]\tLoss: 0.082886\n",
      "epoch:34,loss:0.25331053216695043\n",
      "Train set: Average loss: 0.2149, Accuracy: 2355/2566 (92%)\n",
      "Val set: Average loss: 0.6729, Accuracy: 260/327 (80%)\n",
      "Train Epoch: 35 [40/2566 (2%)]\tLoss: 0.477473\n",
      "Train Epoch: 35 [80/2566 (3%)]\tLoss: 0.258211\n",
      "Train Epoch: 35 [120/2566 (5%)]\tLoss: 0.339178\n",
      "Train Epoch: 35 [160/2566 (6%)]\tLoss: 0.332447\n",
      "Train Epoch: 35 [200/2566 (8%)]\tLoss: 0.064490\n",
      "Train Epoch: 35 [240/2566 (9%)]\tLoss: 0.087590\n",
      "Train Epoch: 35 [280/2566 (11%)]\tLoss: 0.222451\n",
      "Train Epoch: 35 [320/2566 (12%)]\tLoss: 0.484114\n",
      "Train Epoch: 35 [360/2566 (14%)]\tLoss: 0.235598\n",
      "Train Epoch: 35 [400/2566 (16%)]\tLoss: 0.270585\n",
      "Train Epoch: 35 [440/2566 (17%)]\tLoss: 0.433889\n",
      "Train Epoch: 35 [480/2566 (19%)]\tLoss: 0.090797\n",
      "Train Epoch: 35 [520/2566 (20%)]\tLoss: 0.072990\n",
      "Train Epoch: 35 [560/2566 (22%)]\tLoss: 0.042041\n",
      "Train Epoch: 35 [600/2566 (23%)]\tLoss: 0.142713\n",
      "Train Epoch: 35 [640/2566 (25%)]\tLoss: 0.218944\n",
      "Train Epoch: 35 [680/2566 (26%)]\tLoss: 0.171048\n",
      "Train Epoch: 35 [720/2566 (28%)]\tLoss: 0.040062\n",
      "Train Epoch: 35 [760/2566 (30%)]\tLoss: 0.069893\n",
      "Train Epoch: 35 [800/2566 (31%)]\tLoss: 0.034864\n",
      "Train Epoch: 35 [840/2566 (33%)]\tLoss: 0.037112\n",
      "Train Epoch: 35 [880/2566 (34%)]\tLoss: 0.257924\n",
      "Train Epoch: 35 [920/2566 (36%)]\tLoss: 0.374442\n",
      "Train Epoch: 35 [960/2566 (37%)]\tLoss: 0.183327\n",
      "Train Epoch: 35 [1000/2566 (39%)]\tLoss: 0.406180\n",
      "Train Epoch: 35 [1040/2566 (40%)]\tLoss: 0.064890\n",
      "Train Epoch: 35 [1080/2566 (42%)]\tLoss: 0.057578\n",
      "Train Epoch: 35 [1120/2566 (44%)]\tLoss: 0.805170\n",
      "Train Epoch: 35 [1160/2566 (45%)]\tLoss: 0.077969\n",
      "Train Epoch: 35 [1200/2566 (47%)]\tLoss: 0.267028\n",
      "Train Epoch: 35 [1240/2566 (48%)]\tLoss: 0.077475\n",
      "Train Epoch: 35 [1280/2566 (50%)]\tLoss: 0.072810\n",
      "Train Epoch: 35 [1320/2566 (51%)]\tLoss: 0.054588\n",
      "Train Epoch: 35 [1360/2566 (53%)]\tLoss: 0.044902\n",
      "Train Epoch: 35 [1400/2566 (55%)]\tLoss: 0.266314\n",
      "Train Epoch: 35 [1440/2566 (56%)]\tLoss: 0.079657\n",
      "Train Epoch: 35 [1480/2566 (58%)]\tLoss: 0.326665\n",
      "Train Epoch: 35 [1520/2566 (59%)]\tLoss: 0.237570\n",
      "Train Epoch: 35 [1560/2566 (61%)]\tLoss: 0.158315\n",
      "Train Epoch: 35 [1600/2566 (62%)]\tLoss: 0.119267\n",
      "Train Epoch: 35 [1640/2566 (64%)]\tLoss: 0.298862\n",
      "Train Epoch: 35 [1680/2566 (65%)]\tLoss: 0.058452\n",
      "Train Epoch: 35 [1720/2566 (67%)]\tLoss: 0.074554\n",
      "Train Epoch: 35 [1760/2566 (69%)]\tLoss: 0.364443\n",
      "Train Epoch: 35 [1800/2566 (70%)]\tLoss: 0.116284\n",
      "Train Epoch: 35 [1840/2566 (72%)]\tLoss: 0.456257\n",
      "Train Epoch: 35 [1880/2566 (73%)]\tLoss: 0.145449\n",
      "Train Epoch: 35 [1920/2566 (75%)]\tLoss: 0.294688\n",
      "Train Epoch: 35 [1960/2566 (76%)]\tLoss: 0.224242\n",
      "Train Epoch: 35 [2000/2566 (78%)]\tLoss: 0.711369\n",
      "Train Epoch: 35 [2040/2566 (79%)]\tLoss: 0.407908\n",
      "Train Epoch: 35 [2080/2566 (81%)]\tLoss: 0.503565\n",
      "Train Epoch: 35 [2120/2566 (83%)]\tLoss: 0.124175\n",
      "Train Epoch: 35 [2160/2566 (84%)]\tLoss: 0.172027\n",
      "Train Epoch: 35 [2200/2566 (86%)]\tLoss: 0.077126\n",
      "Train Epoch: 35 [2240/2566 (87%)]\tLoss: 0.044232\n",
      "Train Epoch: 35 [2280/2566 (89%)]\tLoss: 0.175307\n",
      "Train Epoch: 35 [2320/2566 (90%)]\tLoss: 0.146826\n",
      "Train Epoch: 35 [2360/2566 (92%)]\tLoss: 0.135515\n",
      "Train Epoch: 35 [2400/2566 (93%)]\tLoss: 0.407050\n",
      "Train Epoch: 35 [2440/2566 (95%)]\tLoss: 0.165767\n",
      "Train Epoch: 35 [2480/2566 (97%)]\tLoss: 0.533287\n",
      "Train Epoch: 35 [2520/2566 (98%)]\tLoss: 0.074098\n",
      "Train Epoch: 35 [2560/2566 (100%)]\tLoss: 0.052070\n",
      "epoch:35,loss:0.24063059128240633\n",
      "Train set: Average loss: 0.1634, Accuracy: 2440/2566 (95%)\n",
      "Val set: Average loss: 0.6128, Accuracy: 257/327 (79%)\n",
      "Train Epoch: 36 [40/2566 (2%)]\tLoss: 0.186092\n",
      "Train Epoch: 36 [80/2566 (3%)]\tLoss: 0.190065\n",
      "Train Epoch: 36 [120/2566 (5%)]\tLoss: 0.075077\n",
      "Train Epoch: 36 [160/2566 (6%)]\tLoss: 0.185089\n",
      "Train Epoch: 36 [200/2566 (8%)]\tLoss: 0.238582\n",
      "Train Epoch: 36 [240/2566 (9%)]\tLoss: 0.339796\n",
      "Train Epoch: 36 [280/2566 (11%)]\tLoss: 0.091060\n",
      "Train Epoch: 36 [320/2566 (12%)]\tLoss: 0.263509\n",
      "Train Epoch: 36 [360/2566 (14%)]\tLoss: 0.161160\n",
      "Train Epoch: 36 [400/2566 (16%)]\tLoss: 0.073337\n",
      "Train Epoch: 36 [440/2566 (17%)]\tLoss: 0.190822\n",
      "Train Epoch: 36 [480/2566 (19%)]\tLoss: 0.168721\n",
      "Train Epoch: 36 [520/2566 (20%)]\tLoss: 0.097511\n",
      "Train Epoch: 36 [560/2566 (22%)]\tLoss: 0.093955\n",
      "Train Epoch: 36 [600/2566 (23%)]\tLoss: 0.110692\n",
      "Train Epoch: 36 [640/2566 (25%)]\tLoss: 0.063315\n",
      "Train Epoch: 36 [680/2566 (26%)]\tLoss: 0.361015\n",
      "Train Epoch: 36 [720/2566 (28%)]\tLoss: 0.164190\n",
      "Train Epoch: 36 [760/2566 (30%)]\tLoss: 0.389360\n",
      "Train Epoch: 36 [800/2566 (31%)]\tLoss: 0.092988\n",
      "Train Epoch: 36 [840/2566 (33%)]\tLoss: 0.306777\n",
      "Train Epoch: 36 [880/2566 (34%)]\tLoss: 0.671302\n",
      "Train Epoch: 36 [920/2566 (36%)]\tLoss: 0.087643\n",
      "Train Epoch: 36 [960/2566 (37%)]\tLoss: 0.313311\n",
      "Train Epoch: 36 [1000/2566 (39%)]\tLoss: 0.230274\n",
      "Train Epoch: 36 [1040/2566 (40%)]\tLoss: 0.030927\n",
      "Train Epoch: 36 [1080/2566 (42%)]\tLoss: 0.051762\n",
      "Train Epoch: 36 [1120/2566 (44%)]\tLoss: 0.068807\n",
      "Train Epoch: 36 [1160/2566 (45%)]\tLoss: 0.449812\n",
      "Train Epoch: 36 [1200/2566 (47%)]\tLoss: 0.082790\n",
      "Train Epoch: 36 [1240/2566 (48%)]\tLoss: 0.174348\n",
      "Train Epoch: 36 [1280/2566 (50%)]\tLoss: 0.143384\n",
      "Train Epoch: 36 [1320/2566 (51%)]\tLoss: 0.096214\n",
      "Train Epoch: 36 [1360/2566 (53%)]\tLoss: 0.152747\n",
      "Train Epoch: 36 [1400/2566 (55%)]\tLoss: 0.157109\n",
      "Train Epoch: 36 [1440/2566 (56%)]\tLoss: 0.407234\n",
      "Train Epoch: 36 [1480/2566 (58%)]\tLoss: 0.207618\n",
      "Train Epoch: 36 [1520/2566 (59%)]\tLoss: 0.156027\n",
      "Train Epoch: 36 [1560/2566 (61%)]\tLoss: 0.439085\n",
      "Train Epoch: 36 [1600/2566 (62%)]\tLoss: 0.136964\n",
      "Train Epoch: 36 [1640/2566 (64%)]\tLoss: 0.061294\n",
      "Train Epoch: 36 [1680/2566 (65%)]\tLoss: 0.150150\n",
      "Train Epoch: 36 [1720/2566 (67%)]\tLoss: 0.373399\n",
      "Train Epoch: 36 [1760/2566 (69%)]\tLoss: 0.207080\n",
      "Train Epoch: 36 [1800/2566 (70%)]\tLoss: 0.065226\n",
      "Train Epoch: 36 [1840/2566 (72%)]\tLoss: 0.275920\n",
      "Train Epoch: 36 [1880/2566 (73%)]\tLoss: 0.133076\n",
      "Train Epoch: 36 [1920/2566 (75%)]\tLoss: 0.174210\n",
      "Train Epoch: 36 [1960/2566 (76%)]\tLoss: 0.528215\n",
      "Train Epoch: 36 [2000/2566 (78%)]\tLoss: 0.399937\n",
      "Train Epoch: 36 [2040/2566 (79%)]\tLoss: 0.040996\n",
      "Train Epoch: 36 [2080/2566 (81%)]\tLoss: 0.104757\n",
      "Train Epoch: 36 [2120/2566 (83%)]\tLoss: 0.246316\n",
      "Train Epoch: 36 [2160/2566 (84%)]\tLoss: 0.488995\n",
      "Train Epoch: 36 [2200/2566 (86%)]\tLoss: 0.405984\n",
      "Train Epoch: 36 [2240/2566 (87%)]\tLoss: 0.084205\n",
      "Train Epoch: 36 [2280/2566 (89%)]\tLoss: 0.169324\n",
      "Train Epoch: 36 [2320/2566 (90%)]\tLoss: 0.695439\n",
      "Train Epoch: 36 [2360/2566 (92%)]\tLoss: 0.345912\n",
      "Train Epoch: 36 [2400/2566 (93%)]\tLoss: 0.130984\n",
      "Train Epoch: 36 [2440/2566 (95%)]\tLoss: 0.352619\n",
      "Train Epoch: 36 [2480/2566 (97%)]\tLoss: 0.068009\n",
      "Train Epoch: 36 [2520/2566 (98%)]\tLoss: 0.645796\n",
      "Train Epoch: 36 [2560/2566 (100%)]\tLoss: 0.364478\n",
      "epoch:36,loss:0.21829148483494545\n",
      "Train set: Average loss: 0.1568, Accuracy: 2464/2566 (96%)\n",
      "Val set: Average loss: 0.5676, Accuracy: 262/327 (80%)\n",
      "Train Epoch: 37 [40/2566 (2%)]\tLoss: 0.170383\n",
      "Train Epoch: 37 [80/2566 (3%)]\tLoss: 0.154920\n",
      "Train Epoch: 37 [120/2566 (5%)]\tLoss: 0.102740\n",
      "Train Epoch: 37 [160/2566 (6%)]\tLoss: 0.280685\n",
      "Train Epoch: 37 [200/2566 (8%)]\tLoss: 0.154145\n",
      "Train Epoch: 37 [240/2566 (9%)]\tLoss: 0.311655\n",
      "Train Epoch: 37 [280/2566 (11%)]\tLoss: 0.099815\n",
      "Train Epoch: 37 [320/2566 (12%)]\tLoss: 0.651160\n",
      "Train Epoch: 37 [360/2566 (14%)]\tLoss: 0.060695\n",
      "Train Epoch: 37 [400/2566 (16%)]\tLoss: 0.018120\n",
      "Train Epoch: 37 [440/2566 (17%)]\tLoss: 0.228591\n",
      "Train Epoch: 37 [480/2566 (19%)]\tLoss: 0.111684\n",
      "Train Epoch: 37 [520/2566 (20%)]\tLoss: 0.207959\n",
      "Train Epoch: 37 [560/2566 (22%)]\tLoss: 0.225868\n",
      "Train Epoch: 37 [600/2566 (23%)]\tLoss: 0.072444\n",
      "Train Epoch: 37 [640/2566 (25%)]\tLoss: 0.120680\n",
      "Train Epoch: 37 [680/2566 (26%)]\tLoss: 0.113954\n",
      "Train Epoch: 37 [720/2566 (28%)]\tLoss: 0.159122\n",
      "Train Epoch: 37 [760/2566 (30%)]\tLoss: 0.209256\n",
      "Train Epoch: 37 [800/2566 (31%)]\tLoss: 0.308160\n",
      "Train Epoch: 37 [840/2566 (33%)]\tLoss: 0.303668\n",
      "Train Epoch: 37 [880/2566 (34%)]\tLoss: 0.153909\n",
      "Train Epoch: 37 [920/2566 (36%)]\tLoss: 0.039490\n",
      "Train Epoch: 37 [960/2566 (37%)]\tLoss: 0.151034\n",
      "Train Epoch: 37 [1000/2566 (39%)]\tLoss: 0.250201\n",
      "Train Epoch: 37 [1040/2566 (40%)]\tLoss: 0.110109\n",
      "Train Epoch: 37 [1080/2566 (42%)]\tLoss: 0.471493\n",
      "Train Epoch: 37 [1120/2566 (44%)]\tLoss: 0.402997\n",
      "Train Epoch: 37 [1160/2566 (45%)]\tLoss: 0.112148\n",
      "Train Epoch: 37 [1200/2566 (47%)]\tLoss: 0.019026\n",
      "Train Epoch: 37 [1240/2566 (48%)]\tLoss: 0.026960\n",
      "Train Epoch: 37 [1280/2566 (50%)]\tLoss: 0.264651\n",
      "Train Epoch: 37 [1320/2566 (51%)]\tLoss: 0.055653\n",
      "Train Epoch: 37 [1360/2566 (53%)]\tLoss: 0.082472\n",
      "Train Epoch: 37 [1400/2566 (55%)]\tLoss: 0.382336\n",
      "Train Epoch: 37 [1440/2566 (56%)]\tLoss: 0.143752\n",
      "Train Epoch: 37 [1480/2566 (58%)]\tLoss: 0.059020\n",
      "Train Epoch: 37 [1520/2566 (59%)]\tLoss: 0.549635\n",
      "Train Epoch: 37 [1560/2566 (61%)]\tLoss: 0.320990\n",
      "Train Epoch: 37 [1600/2566 (62%)]\tLoss: 0.463432\n",
      "Train Epoch: 37 [1640/2566 (64%)]\tLoss: 0.231994\n",
      "Train Epoch: 37 [1680/2566 (65%)]\tLoss: 0.089759\n",
      "Train Epoch: 37 [1720/2566 (67%)]\tLoss: 0.384114\n",
      "Train Epoch: 37 [1760/2566 (69%)]\tLoss: 0.279372\n",
      "Train Epoch: 37 [1800/2566 (70%)]\tLoss: 0.055867\n",
      "Train Epoch: 37 [1840/2566 (72%)]\tLoss: 0.118724\n",
      "Train Epoch: 37 [1880/2566 (73%)]\tLoss: 0.475009\n",
      "Train Epoch: 37 [1920/2566 (75%)]\tLoss: 0.168223\n",
      "Train Epoch: 37 [1960/2566 (76%)]\tLoss: 0.102031\n",
      "Train Epoch: 37 [2000/2566 (78%)]\tLoss: 0.291746\n",
      "Train Epoch: 37 [2040/2566 (79%)]\tLoss: 0.064477\n",
      "Train Epoch: 37 [2080/2566 (81%)]\tLoss: 0.058681\n",
      "Train Epoch: 37 [2120/2566 (83%)]\tLoss: 0.176710\n",
      "Train Epoch: 37 [2160/2566 (84%)]\tLoss: 0.440452\n",
      "Train Epoch: 37 [2200/2566 (86%)]\tLoss: 0.049746\n",
      "Train Epoch: 37 [2240/2566 (87%)]\tLoss: 0.218986\n",
      "Train Epoch: 37 [2280/2566 (89%)]\tLoss: 0.658842\n",
      "Train Epoch: 37 [2320/2566 (90%)]\tLoss: 0.033747\n",
      "Train Epoch: 37 [2360/2566 (92%)]\tLoss: 0.178988\n",
      "Train Epoch: 37 [2400/2566 (93%)]\tLoss: 0.151570\n",
      "Train Epoch: 37 [2440/2566 (95%)]\tLoss: 0.104229\n",
      "Train Epoch: 37 [2480/2566 (97%)]\tLoss: 0.653925\n",
      "Train Epoch: 37 [2520/2566 (98%)]\tLoss: 0.021785\n",
      "Train Epoch: 37 [2560/2566 (100%)]\tLoss: 0.146117\n",
      "epoch:37,loss:0.2182111292729311\n",
      "Train set: Average loss: 0.1503, Accuracy: 2455/2566 (96%)\n",
      "Val set: Average loss: 0.6430, Accuracy: 258/327 (79%)\n",
      "Train Epoch: 38 [40/2566 (2%)]\tLoss: 0.247711\n",
      "Train Epoch: 38 [80/2566 (3%)]\tLoss: 0.259057\n",
      "Train Epoch: 38 [120/2566 (5%)]\tLoss: 0.059159\n",
      "Train Epoch: 38 [160/2566 (6%)]\tLoss: 0.335978\n",
      "Train Epoch: 38 [200/2566 (8%)]\tLoss: 0.275935\n",
      "Train Epoch: 38 [240/2566 (9%)]\tLoss: 0.193268\n",
      "Train Epoch: 38 [280/2566 (11%)]\tLoss: 0.436381\n",
      "Train Epoch: 38 [320/2566 (12%)]\tLoss: 0.496957\n",
      "Train Epoch: 38 [360/2566 (14%)]\tLoss: 0.159152\n",
      "Train Epoch: 38 [400/2566 (16%)]\tLoss: 0.261353\n",
      "Train Epoch: 38 [440/2566 (17%)]\tLoss: 0.254352\n",
      "Train Epoch: 38 [480/2566 (19%)]\tLoss: 0.075109\n",
      "Train Epoch: 38 [520/2566 (20%)]\tLoss: 0.027530\n",
      "Train Epoch: 38 [560/2566 (22%)]\tLoss: 0.337415\n",
      "Train Epoch: 38 [600/2566 (23%)]\tLoss: 0.259529\n",
      "Train Epoch: 38 [640/2566 (25%)]\tLoss: 0.105086\n",
      "Train Epoch: 38 [680/2566 (26%)]\tLoss: 0.207635\n",
      "Train Epoch: 38 [720/2566 (28%)]\tLoss: 0.070795\n",
      "Train Epoch: 38 [760/2566 (30%)]\tLoss: 0.168694\n",
      "Train Epoch: 38 [800/2566 (31%)]\tLoss: 0.292668\n",
      "Train Epoch: 38 [840/2566 (33%)]\tLoss: 0.062649\n",
      "Train Epoch: 38 [880/2566 (34%)]\tLoss: 0.025165\n",
      "Train Epoch: 38 [920/2566 (36%)]\tLoss: 0.164150\n",
      "Train Epoch: 38 [960/2566 (37%)]\tLoss: 0.223599\n",
      "Train Epoch: 38 [1000/2566 (39%)]\tLoss: 0.083377\n",
      "Train Epoch: 38 [1040/2566 (40%)]\tLoss: 0.359408\n",
      "Train Epoch: 38 [1080/2566 (42%)]\tLoss: 0.305383\n",
      "Train Epoch: 38 [1120/2566 (44%)]\tLoss: 0.081159\n",
      "Train Epoch: 38 [1160/2566 (45%)]\tLoss: 0.123563\n",
      "Train Epoch: 38 [1200/2566 (47%)]\tLoss: 0.211650\n",
      "Train Epoch: 38 [1240/2566 (48%)]\tLoss: 0.217833\n",
      "Train Epoch: 38 [1280/2566 (50%)]\tLoss: 0.123967\n",
      "Train Epoch: 38 [1320/2566 (51%)]\tLoss: 0.177192\n",
      "Train Epoch: 38 [1360/2566 (53%)]\tLoss: 0.094852\n",
      "Train Epoch: 38 [1400/2566 (55%)]\tLoss: 0.041039\n",
      "Train Epoch: 38 [1440/2566 (56%)]\tLoss: 0.061658\n",
      "Train Epoch: 38 [1480/2566 (58%)]\tLoss: 0.182043\n",
      "Train Epoch: 38 [1520/2566 (59%)]\tLoss: 0.204700\n",
      "Train Epoch: 38 [1560/2566 (61%)]\tLoss: 0.104131\n",
      "Train Epoch: 38 [1600/2566 (62%)]\tLoss: 0.124600\n",
      "Train Epoch: 38 [1640/2566 (64%)]\tLoss: 0.073980\n",
      "Train Epoch: 38 [1680/2566 (65%)]\tLoss: 0.030530\n",
      "Train Epoch: 38 [1720/2566 (67%)]\tLoss: 0.063383\n",
      "Train Epoch: 38 [1760/2566 (69%)]\tLoss: 0.601226\n",
      "Train Epoch: 38 [1800/2566 (70%)]\tLoss: 0.122619\n",
      "Train Epoch: 38 [1840/2566 (72%)]\tLoss: 0.036755\n",
      "Train Epoch: 38 [1880/2566 (73%)]\tLoss: 0.501103\n",
      "Train Epoch: 38 [1920/2566 (75%)]\tLoss: 0.069494\n",
      "Train Epoch: 38 [1960/2566 (76%)]\tLoss: 0.053048\n",
      "Train Epoch: 38 [2000/2566 (78%)]\tLoss: 0.031400\n",
      "Train Epoch: 38 [2040/2566 (79%)]\tLoss: 0.071486\n",
      "Train Epoch: 38 [2080/2566 (81%)]\tLoss: 0.184579\n",
      "Train Epoch: 38 [2120/2566 (83%)]\tLoss: 0.080246\n",
      "Train Epoch: 38 [2160/2566 (84%)]\tLoss: 0.073027\n",
      "Train Epoch: 38 [2200/2566 (86%)]\tLoss: 0.671866\n",
      "Train Epoch: 38 [2240/2566 (87%)]\tLoss: 0.315277\n",
      "Train Epoch: 38 [2280/2566 (89%)]\tLoss: 0.068043\n",
      "Train Epoch: 38 [2320/2566 (90%)]\tLoss: 0.036272\n",
      "Train Epoch: 38 [2360/2566 (92%)]\tLoss: 0.251276\n",
      "Train Epoch: 38 [2400/2566 (93%)]\tLoss: 0.207985\n",
      "Train Epoch: 38 [2440/2566 (95%)]\tLoss: 0.722541\n",
      "Train Epoch: 38 [2480/2566 (97%)]\tLoss: 0.152836\n",
      "Train Epoch: 38 [2520/2566 (98%)]\tLoss: 0.108316\n",
      "Train Epoch: 38 [2560/2566 (100%)]\tLoss: 0.226189\n",
      "epoch:38,loss:0.19297491405101208\n",
      "Train set: Average loss: 0.1783, Accuracy: 2417/2566 (94%)\n",
      "Val set: Average loss: 0.6931, Accuracy: 260/327 (80%)\n",
      "Train Epoch: 39 [40/2566 (2%)]\tLoss: 0.199100\n",
      "Train Epoch: 39 [80/2566 (3%)]\tLoss: 0.018950\n",
      "Train Epoch: 39 [120/2566 (5%)]\tLoss: 0.172300\n",
      "Train Epoch: 39 [160/2566 (6%)]\tLoss: 0.136040\n",
      "Train Epoch: 39 [200/2566 (8%)]\tLoss: 0.022664\n",
      "Train Epoch: 39 [240/2566 (9%)]\tLoss: 0.125929\n",
      "Train Epoch: 39 [280/2566 (11%)]\tLoss: 0.066934\n",
      "Train Epoch: 39 [320/2566 (12%)]\tLoss: 0.194972\n",
      "Train Epoch: 39 [360/2566 (14%)]\tLoss: 0.125212\n",
      "Train Epoch: 39 [400/2566 (16%)]\tLoss: 0.458022\n",
      "Train Epoch: 39 [440/2566 (17%)]\tLoss: 0.100061\n",
      "Train Epoch: 39 [480/2566 (19%)]\tLoss: 0.110139\n",
      "Train Epoch: 39 [520/2566 (20%)]\tLoss: 0.320837\n",
      "Train Epoch: 39 [560/2566 (22%)]\tLoss: 0.310831\n",
      "Train Epoch: 39 [600/2566 (23%)]\tLoss: 0.180755\n",
      "Train Epoch: 39 [640/2566 (25%)]\tLoss: 0.112384\n",
      "Train Epoch: 39 [680/2566 (26%)]\tLoss: 0.039930\n",
      "Train Epoch: 39 [720/2566 (28%)]\tLoss: 0.126008\n",
      "Train Epoch: 39 [760/2566 (30%)]\tLoss: 0.055714\n",
      "Train Epoch: 39 [800/2566 (31%)]\tLoss: 0.118408\n",
      "Train Epoch: 39 [840/2566 (33%)]\tLoss: 0.396372\n",
      "Train Epoch: 39 [880/2566 (34%)]\tLoss: 0.098442\n",
      "Train Epoch: 39 [920/2566 (36%)]\tLoss: 0.038494\n",
      "Train Epoch: 39 [960/2566 (37%)]\tLoss: 0.094828\n",
      "Train Epoch: 39 [1000/2566 (39%)]\tLoss: 0.400183\n",
      "Train Epoch: 39 [1040/2566 (40%)]\tLoss: 0.568325\n",
      "Train Epoch: 39 [1080/2566 (42%)]\tLoss: 0.045116\n",
      "Train Epoch: 39 [1120/2566 (44%)]\tLoss: 0.040774\n",
      "Train Epoch: 39 [1160/2566 (45%)]\tLoss: 0.225978\n",
      "Train Epoch: 39 [1200/2566 (47%)]\tLoss: 0.128307\n",
      "Train Epoch: 39 [1240/2566 (48%)]\tLoss: 0.104198\n",
      "Train Epoch: 39 [1280/2566 (50%)]\tLoss: 0.247035\n",
      "Train Epoch: 39 [1320/2566 (51%)]\tLoss: 0.090933\n",
      "Train Epoch: 39 [1360/2566 (53%)]\tLoss: 0.152975\n",
      "Train Epoch: 39 [1400/2566 (55%)]\tLoss: 0.110401\n",
      "Train Epoch: 39 [1440/2566 (56%)]\tLoss: 0.298572\n",
      "Train Epoch: 39 [1480/2566 (58%)]\tLoss: 0.419650\n",
      "Train Epoch: 39 [1520/2566 (59%)]\tLoss: 0.098674\n",
      "Train Epoch: 39 [1560/2566 (61%)]\tLoss: 0.469798\n",
      "Train Epoch: 39 [1600/2566 (62%)]\tLoss: 0.080413\n",
      "Train Epoch: 39 [1640/2566 (64%)]\tLoss: 0.042840\n",
      "Train Epoch: 39 [1680/2566 (65%)]\tLoss: 0.015201\n",
      "Train Epoch: 39 [1720/2566 (67%)]\tLoss: 0.120014\n",
      "Train Epoch: 39 [1760/2566 (69%)]\tLoss: 0.349351\n",
      "Train Epoch: 39 [1800/2566 (70%)]\tLoss: 0.221919\n",
      "Train Epoch: 39 [1840/2566 (72%)]\tLoss: 0.454241\n",
      "Train Epoch: 39 [1880/2566 (73%)]\tLoss: 0.224725\n",
      "Train Epoch: 39 [1920/2566 (75%)]\tLoss: 0.019777\n",
      "Train Epoch: 39 [1960/2566 (76%)]\tLoss: 0.198488\n",
      "Train Epoch: 39 [2000/2566 (78%)]\tLoss: 0.159277\n",
      "Train Epoch: 39 [2040/2566 (79%)]\tLoss: 0.041479\n",
      "Train Epoch: 39 [2080/2566 (81%)]\tLoss: 0.110174\n",
      "Train Epoch: 39 [2120/2566 (83%)]\tLoss: 0.351843\n",
      "Train Epoch: 39 [2160/2566 (84%)]\tLoss: 0.052046\n",
      "Train Epoch: 39 [2200/2566 (86%)]\tLoss: 0.218402\n",
      "Train Epoch: 39 [2240/2566 (87%)]\tLoss: 0.062657\n",
      "Train Epoch: 39 [2280/2566 (89%)]\tLoss: 0.210183\n",
      "Train Epoch: 39 [2320/2566 (90%)]\tLoss: 0.063703\n",
      "Train Epoch: 39 [2360/2566 (92%)]\tLoss: 0.016197\n",
      "Train Epoch: 39 [2400/2566 (93%)]\tLoss: 0.162438\n",
      "Train Epoch: 39 [2440/2566 (95%)]\tLoss: 0.151946\n",
      "Train Epoch: 39 [2480/2566 (97%)]\tLoss: 0.454337\n",
      "Train Epoch: 39 [2520/2566 (98%)]\tLoss: 0.200425\n",
      "Train Epoch: 39 [2560/2566 (100%)]\tLoss: 0.139328\n",
      "epoch:39,loss:0.1863444177752993\n",
      "Train set: Average loss: 0.2364, Accuracy: 2389/2566 (93%)\n",
      "Val set: Average loss: 0.8224, Accuracy: 242/327 (74%)\n",
      "Train Epoch: 40 [40/2566 (2%)]\tLoss: 0.185709\n",
      "Train Epoch: 40 [80/2566 (3%)]\tLoss: 0.114025\n",
      "Train Epoch: 40 [120/2566 (5%)]\tLoss: 0.187283\n",
      "Train Epoch: 40 [160/2566 (6%)]\tLoss: 0.027457\n",
      "Train Epoch: 40 [200/2566 (8%)]\tLoss: 0.137765\n",
      "Train Epoch: 40 [240/2566 (9%)]\tLoss: 0.176312\n",
      "Train Epoch: 40 [280/2566 (11%)]\tLoss: 0.051595\n",
      "Train Epoch: 40 [320/2566 (12%)]\tLoss: 0.100758\n",
      "Train Epoch: 40 [360/2566 (14%)]\tLoss: 0.071240\n",
      "Train Epoch: 40 [400/2566 (16%)]\tLoss: 0.038407\n",
      "Train Epoch: 40 [440/2566 (17%)]\tLoss: 0.140102\n",
      "Train Epoch: 40 [480/2566 (19%)]\tLoss: 0.346720\n",
      "Train Epoch: 40 [520/2566 (20%)]\tLoss: 0.070637\n",
      "Train Epoch: 40 [560/2566 (22%)]\tLoss: 0.018887\n",
      "Train Epoch: 40 [600/2566 (23%)]\tLoss: 0.036293\n",
      "Train Epoch: 40 [640/2566 (25%)]\tLoss: 0.328712\n",
      "Train Epoch: 40 [680/2566 (26%)]\tLoss: 0.556071\n",
      "Train Epoch: 40 [720/2566 (28%)]\tLoss: 0.020333\n",
      "Train Epoch: 40 [760/2566 (30%)]\tLoss: 0.035610\n",
      "Train Epoch: 40 [800/2566 (31%)]\tLoss: 0.365531\n",
      "Train Epoch: 40 [840/2566 (33%)]\tLoss: 0.792481\n",
      "Train Epoch: 40 [880/2566 (34%)]\tLoss: 0.109642\n",
      "Train Epoch: 40 [920/2566 (36%)]\tLoss: 0.022859\n",
      "Train Epoch: 40 [960/2566 (37%)]\tLoss: 0.307142\n",
      "Train Epoch: 40 [1000/2566 (39%)]\tLoss: 0.087454\n",
      "Train Epoch: 40 [1040/2566 (40%)]\tLoss: 0.025880\n",
      "Train Epoch: 40 [1080/2566 (42%)]\tLoss: 0.470886\n",
      "Train Epoch: 40 [1120/2566 (44%)]\tLoss: 0.052002\n",
      "Train Epoch: 40 [1160/2566 (45%)]\tLoss: 0.024369\n",
      "Train Epoch: 40 [1200/2566 (47%)]\tLoss: 0.169323\n",
      "Train Epoch: 40 [1240/2566 (48%)]\tLoss: 0.162040\n",
      "Train Epoch: 40 [1280/2566 (50%)]\tLoss: 0.038973\n",
      "Train Epoch: 40 [1320/2566 (51%)]\tLoss: 0.144456\n",
      "Train Epoch: 40 [1360/2566 (53%)]\tLoss: 0.060738\n",
      "Train Epoch: 40 [1400/2566 (55%)]\tLoss: 0.296156\n",
      "Train Epoch: 40 [1440/2566 (56%)]\tLoss: 0.107470\n",
      "Train Epoch: 40 [1480/2566 (58%)]\tLoss: 0.024790\n",
      "Train Epoch: 40 [1520/2566 (59%)]\tLoss: 0.390993\n",
      "Train Epoch: 40 [1560/2566 (61%)]\tLoss: 0.058775\n",
      "Train Epoch: 40 [1600/2566 (62%)]\tLoss: 0.199840\n",
      "Train Epoch: 40 [1640/2566 (64%)]\tLoss: 0.051330\n",
      "Train Epoch: 40 [1680/2566 (65%)]\tLoss: 0.030239\n",
      "Train Epoch: 40 [1720/2566 (67%)]\tLoss: 0.030717\n",
      "Train Epoch: 40 [1760/2566 (69%)]\tLoss: 0.262083\n",
      "Train Epoch: 40 [1800/2566 (70%)]\tLoss: 0.056916\n",
      "Train Epoch: 40 [1840/2566 (72%)]\tLoss: 0.044872\n",
      "Train Epoch: 40 [1880/2566 (73%)]\tLoss: 0.190588\n",
      "Train Epoch: 40 [1920/2566 (75%)]\tLoss: 0.018833\n",
      "Train Epoch: 40 [1960/2566 (76%)]\tLoss: 0.163821\n",
      "Train Epoch: 40 [2000/2566 (78%)]\tLoss: 0.119531\n",
      "Train Epoch: 40 [2040/2566 (79%)]\tLoss: 0.046393\n",
      "Train Epoch: 40 [2080/2566 (81%)]\tLoss: 0.172382\n",
      "Train Epoch: 40 [2120/2566 (83%)]\tLoss: 0.083901\n",
      "Train Epoch: 40 [2160/2566 (84%)]\tLoss: 0.070401\n",
      "Train Epoch: 40 [2200/2566 (86%)]\tLoss: 0.117353\n",
      "Train Epoch: 40 [2240/2566 (87%)]\tLoss: 0.037041\n",
      "Train Epoch: 40 [2280/2566 (89%)]\tLoss: 0.631362\n",
      "Train Epoch: 40 [2320/2566 (90%)]\tLoss: 0.132406\n",
      "Train Epoch: 40 [2360/2566 (92%)]\tLoss: 0.145573\n",
      "Train Epoch: 40 [2400/2566 (93%)]\tLoss: 0.090877\n",
      "Train Epoch: 40 [2440/2566 (95%)]\tLoss: 0.550807\n",
      "Train Epoch: 40 [2480/2566 (97%)]\tLoss: 0.168876\n",
      "Train Epoch: 40 [2520/2566 (98%)]\tLoss: 0.025963\n",
      "Train Epoch: 40 [2560/2566 (100%)]\tLoss: 0.114731\n",
      "epoch:40,loss:0.17504845974775396\n",
      "Train set: Average loss: 0.1803, Accuracy: 2432/2566 (95%)\n",
      "Val set: Average loss: 0.7057, Accuracy: 251/327 (77%)\n",
      "Train Epoch: 41 [40/2566 (2%)]\tLoss: 0.099784\n",
      "Train Epoch: 41 [80/2566 (3%)]\tLoss: 0.052251\n",
      "Train Epoch: 41 [120/2566 (5%)]\tLoss: 0.707211\n",
      "Train Epoch: 41 [160/2566 (6%)]\tLoss: 0.223119\n",
      "Train Epoch: 41 [200/2566 (8%)]\tLoss: 0.019772\n",
      "Train Epoch: 41 [240/2566 (9%)]\tLoss: 0.094701\n",
      "Train Epoch: 41 [280/2566 (11%)]\tLoss: 0.024703\n",
      "Train Epoch: 41 [320/2566 (12%)]\tLoss: 0.135441\n",
      "Train Epoch: 41 [360/2566 (14%)]\tLoss: 0.055581\n",
      "Train Epoch: 41 [400/2566 (16%)]\tLoss: 0.102430\n",
      "Train Epoch: 41 [440/2566 (17%)]\tLoss: 0.096491\n",
      "Train Epoch: 41 [480/2566 (19%)]\tLoss: 0.092219\n",
      "Train Epoch: 41 [520/2566 (20%)]\tLoss: 0.536373\n",
      "Train Epoch: 41 [560/2566 (22%)]\tLoss: 0.141576\n",
      "Train Epoch: 41 [600/2566 (23%)]\tLoss: 0.334379\n",
      "Train Epoch: 41 [640/2566 (25%)]\tLoss: 0.128800\n",
      "Train Epoch: 41 [680/2566 (26%)]\tLoss: 0.058083\n",
      "Train Epoch: 41 [720/2566 (28%)]\tLoss: 0.070401\n",
      "Train Epoch: 41 [760/2566 (30%)]\tLoss: 0.087693\n",
      "Train Epoch: 41 [800/2566 (31%)]\tLoss: 0.032036\n",
      "Train Epoch: 41 [840/2566 (33%)]\tLoss: 0.035070\n",
      "Train Epoch: 41 [880/2566 (34%)]\tLoss: 1.240030\n",
      "Train Epoch: 41 [920/2566 (36%)]\tLoss: 0.241911\n",
      "Train Epoch: 41 [960/2566 (37%)]\tLoss: 0.045908\n",
      "Train Epoch: 41 [1000/2566 (39%)]\tLoss: 0.191191\n",
      "Train Epoch: 41 [1040/2566 (40%)]\tLoss: 0.285286\n",
      "Train Epoch: 41 [1080/2566 (42%)]\tLoss: 0.022846\n",
      "Train Epoch: 41 [1120/2566 (44%)]\tLoss: 0.217054\n",
      "Train Epoch: 41 [1160/2566 (45%)]\tLoss: 0.350168\n",
      "Train Epoch: 41 [1200/2566 (47%)]\tLoss: 0.048073\n",
      "Train Epoch: 41 [1240/2566 (48%)]\tLoss: 0.240750\n",
      "Train Epoch: 41 [1280/2566 (50%)]\tLoss: 0.118584\n",
      "Train Epoch: 41 [1320/2566 (51%)]\tLoss: 0.041420\n",
      "Train Epoch: 41 [1360/2566 (53%)]\tLoss: 0.031835\n",
      "Train Epoch: 41 [1400/2566 (55%)]\tLoss: 0.181207\n",
      "Train Epoch: 41 [1440/2566 (56%)]\tLoss: 0.273912\n",
      "Train Epoch: 41 [1480/2566 (58%)]\tLoss: 0.026587\n",
      "Train Epoch: 41 [1520/2566 (59%)]\tLoss: 0.028095\n",
      "Train Epoch: 41 [1560/2566 (61%)]\tLoss: 0.343631\n",
      "Train Epoch: 41 [1600/2566 (62%)]\tLoss: 0.115032\n",
      "Train Epoch: 41 [1640/2566 (64%)]\tLoss: 0.024668\n",
      "Train Epoch: 41 [1680/2566 (65%)]\tLoss: 0.111740\n",
      "Train Epoch: 41 [1720/2566 (67%)]\tLoss: 0.062521\n",
      "Train Epoch: 41 [1760/2566 (69%)]\tLoss: 0.066442\n",
      "Train Epoch: 41 [1800/2566 (70%)]\tLoss: 0.047240\n",
      "Train Epoch: 41 [1840/2566 (72%)]\tLoss: 0.076866\n",
      "Train Epoch: 41 [1880/2566 (73%)]\tLoss: 0.026668\n",
      "Train Epoch: 41 [1920/2566 (75%)]\tLoss: 0.261251\n",
      "Train Epoch: 41 [1960/2566 (76%)]\tLoss: 0.090163\n",
      "Train Epoch: 41 [2000/2566 (78%)]\tLoss: 0.068265\n",
      "Train Epoch: 41 [2040/2566 (79%)]\tLoss: 0.153833\n",
      "Train Epoch: 41 [2080/2566 (81%)]\tLoss: 0.375779\n",
      "Train Epoch: 41 [2120/2566 (83%)]\tLoss: 0.097270\n",
      "Train Epoch: 41 [2160/2566 (84%)]\tLoss: 0.126977\n",
      "Train Epoch: 41 [2200/2566 (86%)]\tLoss: 0.164195\n",
      "Train Epoch: 41 [2240/2566 (87%)]\tLoss: 0.263343\n",
      "Train Epoch: 41 [2280/2566 (89%)]\tLoss: 0.066981\n",
      "Train Epoch: 41 [2320/2566 (90%)]\tLoss: 0.087038\n",
      "Train Epoch: 41 [2360/2566 (92%)]\tLoss: 0.078811\n",
      "Train Epoch: 41 [2400/2566 (93%)]\tLoss: 0.030764\n",
      "Train Epoch: 41 [2440/2566 (95%)]\tLoss: 0.209254\n",
      "Train Epoch: 41 [2480/2566 (97%)]\tLoss: 0.050772\n",
      "Train Epoch: 41 [2520/2566 (98%)]\tLoss: 0.021008\n",
      "Train Epoch: 41 [2560/2566 (100%)]\tLoss: 0.584939\n",
      "epoch:41,loss:0.14881609045433292\n",
      "Train set: Average loss: 0.1104, Accuracy: 2501/2566 (97%)\n",
      "Val set: Average loss: 0.6665, Accuracy: 262/327 (80%)\n",
      "Train Epoch: 42 [40/2566 (2%)]\tLoss: 0.077826\n",
      "Train Epoch: 42 [80/2566 (3%)]\tLoss: 0.013136\n",
      "Train Epoch: 42 [120/2566 (5%)]\tLoss: 0.159109\n",
      "Train Epoch: 42 [160/2566 (6%)]\tLoss: 0.070771\n",
      "Train Epoch: 42 [200/2566 (8%)]\tLoss: 0.377991\n",
      "Train Epoch: 42 [240/2566 (9%)]\tLoss: 0.120920\n",
      "Train Epoch: 42 [280/2566 (11%)]\tLoss: 0.063302\n",
      "Train Epoch: 42 [320/2566 (12%)]\tLoss: 0.341005\n",
      "Train Epoch: 42 [360/2566 (14%)]\tLoss: 0.057752\n",
      "Train Epoch: 42 [400/2566 (16%)]\tLoss: 0.082703\n",
      "Train Epoch: 42 [440/2566 (17%)]\tLoss: 0.042219\n",
      "Train Epoch: 42 [480/2566 (19%)]\tLoss: 0.178463\n",
      "Train Epoch: 42 [520/2566 (20%)]\tLoss: 0.150116\n",
      "Train Epoch: 42 [560/2566 (22%)]\tLoss: 0.013540\n",
      "Train Epoch: 42 [600/2566 (23%)]\tLoss: 0.034842\n",
      "Train Epoch: 42 [640/2566 (25%)]\tLoss: 0.300415\n",
      "Train Epoch: 42 [680/2566 (26%)]\tLoss: 0.150227\n",
      "Train Epoch: 42 [720/2566 (28%)]\tLoss: 0.089180\n",
      "Train Epoch: 42 [760/2566 (30%)]\tLoss: 0.034453\n",
      "Train Epoch: 42 [800/2566 (31%)]\tLoss: 0.044934\n",
      "Train Epoch: 42 [840/2566 (33%)]\tLoss: 0.347044\n",
      "Train Epoch: 42 [880/2566 (34%)]\tLoss: 0.077322\n",
      "Train Epoch: 42 [920/2566 (36%)]\tLoss: 0.059658\n",
      "Train Epoch: 42 [960/2566 (37%)]\tLoss: 0.096419\n",
      "Train Epoch: 42 [1000/2566 (39%)]\tLoss: 0.050938\n",
      "Train Epoch: 42 [1040/2566 (40%)]\tLoss: 0.046567\n",
      "Train Epoch: 42 [1080/2566 (42%)]\tLoss: 0.160284\n",
      "Train Epoch: 42 [1120/2566 (44%)]\tLoss: 0.031919\n",
      "Train Epoch: 42 [1160/2566 (45%)]\tLoss: 0.278223\n",
      "Train Epoch: 42 [1200/2566 (47%)]\tLoss: 0.021709\n",
      "Train Epoch: 42 [1240/2566 (48%)]\tLoss: 0.115965\n",
      "Train Epoch: 42 [1280/2566 (50%)]\tLoss: 0.034476\n",
      "Train Epoch: 42 [1320/2566 (51%)]\tLoss: 0.222577\n",
      "Train Epoch: 42 [1360/2566 (53%)]\tLoss: 0.453027\n",
      "Train Epoch: 42 [1400/2566 (55%)]\tLoss: 0.023653\n",
      "Train Epoch: 42 [1440/2566 (56%)]\tLoss: 0.211587\n",
      "Train Epoch: 42 [1480/2566 (58%)]\tLoss: 0.024707\n",
      "Train Epoch: 42 [1520/2566 (59%)]\tLoss: 0.089076\n",
      "Train Epoch: 42 [1560/2566 (61%)]\tLoss: 0.023037\n",
      "Train Epoch: 42 [1600/2566 (62%)]\tLoss: 0.021816\n",
      "Train Epoch: 42 [1640/2566 (64%)]\tLoss: 0.086798\n",
      "Train Epoch: 42 [1680/2566 (65%)]\tLoss: 0.072264\n",
      "Train Epoch: 42 [1720/2566 (67%)]\tLoss: 0.037809\n",
      "Train Epoch: 42 [1760/2566 (69%)]\tLoss: 0.280470\n",
      "Train Epoch: 42 [1800/2566 (70%)]\tLoss: 0.417613\n",
      "Train Epoch: 42 [1840/2566 (72%)]\tLoss: 0.018761\n",
      "Train Epoch: 42 [1880/2566 (73%)]\tLoss: 0.242475\n",
      "Train Epoch: 42 [1920/2566 (75%)]\tLoss: 0.042717\n",
      "Train Epoch: 42 [1960/2566 (76%)]\tLoss: 0.189769\n",
      "Train Epoch: 42 [2000/2566 (78%)]\tLoss: 0.221498\n",
      "Train Epoch: 42 [2040/2566 (79%)]\tLoss: 0.054684\n",
      "Train Epoch: 42 [2080/2566 (81%)]\tLoss: 0.018633\n",
      "Train Epoch: 42 [2120/2566 (83%)]\tLoss: 0.050441\n",
      "Train Epoch: 42 [2160/2566 (84%)]\tLoss: 0.025441\n",
      "Train Epoch: 42 [2200/2566 (86%)]\tLoss: 0.029293\n",
      "Train Epoch: 42 [2240/2566 (87%)]\tLoss: 0.310994\n",
      "Train Epoch: 42 [2280/2566 (89%)]\tLoss: 0.184924\n",
      "Train Epoch: 42 [2320/2566 (90%)]\tLoss: 0.120305\n",
      "Train Epoch: 42 [2360/2566 (92%)]\tLoss: 0.080978\n",
      "Train Epoch: 42 [2400/2566 (93%)]\tLoss: 0.162459\n",
      "Train Epoch: 42 [2440/2566 (95%)]\tLoss: 0.108044\n",
      "Train Epoch: 42 [2480/2566 (97%)]\tLoss: 0.089288\n",
      "Train Epoch: 42 [2520/2566 (98%)]\tLoss: 0.048492\n",
      "Train Epoch: 42 [2560/2566 (100%)]\tLoss: 0.116349\n",
      "epoch:42,loss:0.1425011135808938\n",
      "Train set: Average loss: 0.1270, Accuracy: 2475/2566 (96%)\n",
      "Val set: Average loss: 0.7457, Accuracy: 254/327 (78%)\n",
      "Train Epoch: 43 [40/2566 (2%)]\tLoss: 0.097376\n",
      "Train Epoch: 43 [80/2566 (3%)]\tLoss: 0.015958\n",
      "Train Epoch: 43 [120/2566 (5%)]\tLoss: 0.060605\n",
      "Train Epoch: 43 [160/2566 (6%)]\tLoss: 0.253232\n",
      "Train Epoch: 43 [200/2566 (8%)]\tLoss: 0.213919\n",
      "Train Epoch: 43 [240/2566 (9%)]\tLoss: 0.058219\n",
      "Train Epoch: 43 [280/2566 (11%)]\tLoss: 0.569058\n",
      "Train Epoch: 43 [320/2566 (12%)]\tLoss: 0.091296\n",
      "Train Epoch: 43 [360/2566 (14%)]\tLoss: 0.336698\n",
      "Train Epoch: 43 [400/2566 (16%)]\tLoss: 0.053073\n",
      "Train Epoch: 43 [440/2566 (17%)]\tLoss: 0.090183\n",
      "Train Epoch: 43 [480/2566 (19%)]\tLoss: 0.047717\n",
      "Train Epoch: 43 [520/2566 (20%)]\tLoss: 0.322786\n",
      "Train Epoch: 43 [560/2566 (22%)]\tLoss: 0.473397\n",
      "Train Epoch: 43 [600/2566 (23%)]\tLoss: 0.175360\n",
      "Train Epoch: 43 [640/2566 (25%)]\tLoss: 0.131862\n",
      "Train Epoch: 43 [680/2566 (26%)]\tLoss: 0.137527\n",
      "Train Epoch: 43 [720/2566 (28%)]\tLoss: 0.096704\n",
      "Train Epoch: 43 [760/2566 (30%)]\tLoss: 0.138743\n",
      "Train Epoch: 43 [800/2566 (31%)]\tLoss: 0.101144\n",
      "Train Epoch: 43 [840/2566 (33%)]\tLoss: 0.123815\n",
      "Train Epoch: 43 [880/2566 (34%)]\tLoss: 0.137423\n",
      "Train Epoch: 43 [920/2566 (36%)]\tLoss: 0.038665\n",
      "Train Epoch: 43 [960/2566 (37%)]\tLoss: 0.049384\n",
      "Train Epoch: 43 [1000/2566 (39%)]\tLoss: 0.257149\n",
      "Train Epoch: 43 [1040/2566 (40%)]\tLoss: 0.034663\n",
      "Train Epoch: 43 [1080/2566 (42%)]\tLoss: 0.087438\n",
      "Train Epoch: 43 [1120/2566 (44%)]\tLoss: 0.301083\n",
      "Train Epoch: 43 [1160/2566 (45%)]\tLoss: 0.059107\n",
      "Train Epoch: 43 [1200/2566 (47%)]\tLoss: 0.243853\n",
      "Train Epoch: 43 [1240/2566 (48%)]\tLoss: 0.027717\n",
      "Train Epoch: 43 [1280/2566 (50%)]\tLoss: 0.222023\n",
      "Train Epoch: 43 [1320/2566 (51%)]\tLoss: 0.276300\n",
      "Train Epoch: 43 [1360/2566 (53%)]\tLoss: 0.097597\n",
      "Train Epoch: 43 [1400/2566 (55%)]\tLoss: 0.202928\n",
      "Train Epoch: 43 [1440/2566 (56%)]\tLoss: 0.196061\n",
      "Train Epoch: 43 [1480/2566 (58%)]\tLoss: 0.023831\n",
      "Train Epoch: 43 [1520/2566 (59%)]\tLoss: 0.188287\n",
      "Train Epoch: 43 [1560/2566 (61%)]\tLoss: 0.180685\n",
      "Train Epoch: 43 [1600/2566 (62%)]\tLoss: 0.036900\n",
      "Train Epoch: 43 [1640/2566 (64%)]\tLoss: 0.019297\n",
      "Train Epoch: 43 [1680/2566 (65%)]\tLoss: 0.060374\n",
      "Train Epoch: 43 [1720/2566 (67%)]\tLoss: 0.236418\n",
      "Train Epoch: 43 [1760/2566 (69%)]\tLoss: 0.038885\n",
      "Train Epoch: 43 [1800/2566 (70%)]\tLoss: 0.095200\n",
      "Train Epoch: 43 [1840/2566 (72%)]\tLoss: 0.022314\n",
      "Train Epoch: 43 [1880/2566 (73%)]\tLoss: 0.154870\n",
      "Train Epoch: 43 [1920/2566 (75%)]\tLoss: 0.062821\n",
      "Train Epoch: 43 [1960/2566 (76%)]\tLoss: 0.593663\n",
      "Train Epoch: 43 [2000/2566 (78%)]\tLoss: 0.174110\n",
      "Train Epoch: 43 [2040/2566 (79%)]\tLoss: 0.181805\n",
      "Train Epoch: 43 [2080/2566 (81%)]\tLoss: 0.125890\n",
      "Train Epoch: 43 [2120/2566 (83%)]\tLoss: 0.107686\n",
      "Train Epoch: 43 [2160/2566 (84%)]\tLoss: 0.117370\n",
      "Train Epoch: 43 [2200/2566 (86%)]\tLoss: 0.827965\n",
      "Train Epoch: 43 [2240/2566 (87%)]\tLoss: 0.071377\n",
      "Train Epoch: 43 [2280/2566 (89%)]\tLoss: 0.053457\n",
      "Train Epoch: 43 [2320/2566 (90%)]\tLoss: 0.216417\n",
      "Train Epoch: 43 [2360/2566 (92%)]\tLoss: 0.077077\n",
      "Train Epoch: 43 [2400/2566 (93%)]\tLoss: 0.094940\n",
      "Train Epoch: 43 [2440/2566 (95%)]\tLoss: 0.039911\n",
      "Train Epoch: 43 [2480/2566 (97%)]\tLoss: 0.309372\n",
      "Train Epoch: 43 [2520/2566 (98%)]\tLoss: 0.115404\n",
      "Train Epoch: 43 [2560/2566 (100%)]\tLoss: 0.043081\n",
      "epoch:43,loss:0.14456244461482096\n",
      "Train set: Average loss: 0.1499, Accuracy: 2439/2566 (95%)\n",
      "Val set: Average loss: 0.8084, Accuracy: 251/327 (77%)\n",
      "Train Epoch: 44 [40/2566 (2%)]\tLoss: 0.013651\n",
      "Train Epoch: 44 [80/2566 (3%)]\tLoss: 0.288558\n",
      "Train Epoch: 44 [120/2566 (5%)]\tLoss: 0.019590\n",
      "Train Epoch: 44 [160/2566 (6%)]\tLoss: 0.059070\n",
      "Train Epoch: 44 [200/2566 (8%)]\tLoss: 0.226063\n",
      "Train Epoch: 44 [240/2566 (9%)]\tLoss: 0.089635\n",
      "Train Epoch: 44 [280/2566 (11%)]\tLoss: 0.292266\n",
      "Train Epoch: 44 [320/2566 (12%)]\tLoss: 0.089120\n",
      "Train Epoch: 44 [360/2566 (14%)]\tLoss: 0.073355\n",
      "Train Epoch: 44 [400/2566 (16%)]\tLoss: 0.030913\n",
      "Train Epoch: 44 [440/2566 (17%)]\tLoss: 0.068158\n",
      "Train Epoch: 44 [480/2566 (19%)]\tLoss: 0.079054\n",
      "Train Epoch: 44 [520/2566 (20%)]\tLoss: 0.162121\n",
      "Train Epoch: 44 [560/2566 (22%)]\tLoss: 0.021061\n",
      "Train Epoch: 44 [600/2566 (23%)]\tLoss: 0.491065\n",
      "Train Epoch: 44 [640/2566 (25%)]\tLoss: 0.130642\n",
      "Train Epoch: 44 [680/2566 (26%)]\tLoss: 0.142983\n",
      "Train Epoch: 44 [720/2566 (28%)]\tLoss: 0.110952\n",
      "Train Epoch: 44 [760/2566 (30%)]\tLoss: 0.050996\n",
      "Train Epoch: 44 [800/2566 (31%)]\tLoss: 0.065737\n",
      "Train Epoch: 44 [840/2566 (33%)]\tLoss: 0.023315\n",
      "Train Epoch: 44 [880/2566 (34%)]\tLoss: 0.078183\n",
      "Train Epoch: 44 [920/2566 (36%)]\tLoss: 0.028617\n",
      "Train Epoch: 44 [960/2566 (37%)]\tLoss: 0.034603\n",
      "Train Epoch: 44 [1000/2566 (39%)]\tLoss: 0.240021\n",
      "Train Epoch: 44 [1040/2566 (40%)]\tLoss: 0.620171\n",
      "Train Epoch: 44 [1080/2566 (42%)]\tLoss: 0.287401\n",
      "Train Epoch: 44 [1120/2566 (44%)]\tLoss: 0.427542\n",
      "Train Epoch: 44 [1160/2566 (45%)]\tLoss: 0.020015\n",
      "Train Epoch: 44 [1200/2566 (47%)]\tLoss: 0.363932\n",
      "Train Epoch: 44 [1240/2566 (48%)]\tLoss: 0.121605\n",
      "Train Epoch: 44 [1280/2566 (50%)]\tLoss: 0.263915\n",
      "Train Epoch: 44 [1320/2566 (51%)]\tLoss: 0.026115\n",
      "Train Epoch: 44 [1360/2566 (53%)]\tLoss: 0.124086\n",
      "Train Epoch: 44 [1400/2566 (55%)]\tLoss: 0.058401\n",
      "Train Epoch: 44 [1440/2566 (56%)]\tLoss: 0.030268\n",
      "Train Epoch: 44 [1480/2566 (58%)]\tLoss: 0.021527\n",
      "Train Epoch: 44 [1520/2566 (59%)]\tLoss: 0.145549\n",
      "Train Epoch: 44 [1560/2566 (61%)]\tLoss: 0.058417\n",
      "Train Epoch: 44 [1600/2566 (62%)]\tLoss: 0.196293\n",
      "Train Epoch: 44 [1640/2566 (64%)]\tLoss: 0.034634\n",
      "Train Epoch: 44 [1680/2566 (65%)]\tLoss: 0.144412\n",
      "Train Epoch: 44 [1720/2566 (67%)]\tLoss: 0.191778\n",
      "Train Epoch: 44 [1760/2566 (69%)]\tLoss: 0.071530\n",
      "Train Epoch: 44 [1800/2566 (70%)]\tLoss: 0.175235\n",
      "Train Epoch: 44 [1840/2566 (72%)]\tLoss: 0.155359\n",
      "Train Epoch: 44 [1880/2566 (73%)]\tLoss: 0.072699\n",
      "Train Epoch: 44 [1920/2566 (75%)]\tLoss: 0.082822\n",
      "Train Epoch: 44 [1960/2566 (76%)]\tLoss: 0.180569\n",
      "Train Epoch: 44 [2000/2566 (78%)]\tLoss: 0.049507\n",
      "Train Epoch: 44 [2040/2566 (79%)]\tLoss: 0.545621\n",
      "Train Epoch: 44 [2080/2566 (81%)]\tLoss: 0.103541\n",
      "Train Epoch: 44 [2120/2566 (83%)]\tLoss: 0.012929\n",
      "Train Epoch: 44 [2160/2566 (84%)]\tLoss: 0.211104\n",
      "Train Epoch: 44 [2200/2566 (86%)]\tLoss: 0.093569\n",
      "Train Epoch: 44 [2240/2566 (87%)]\tLoss: 0.012542\n",
      "Train Epoch: 44 [2280/2566 (89%)]\tLoss: 0.060351\n",
      "Train Epoch: 44 [2320/2566 (90%)]\tLoss: 0.051189\n",
      "Train Epoch: 44 [2360/2566 (92%)]\tLoss: 0.025026\n",
      "Train Epoch: 44 [2400/2566 (93%)]\tLoss: 0.064278\n",
      "Train Epoch: 44 [2440/2566 (95%)]\tLoss: 0.057525\n",
      "Train Epoch: 44 [2480/2566 (97%)]\tLoss: 0.011131\n",
      "Train Epoch: 44 [2520/2566 (98%)]\tLoss: 0.016350\n",
      "Train Epoch: 44 [2560/2566 (100%)]\tLoss: 0.070663\n",
      "epoch:44,loss:0.14148910955389574\n",
      "Train set: Average loss: 0.1694, Accuracy: 2458/2566 (96%)\n",
      "Val set: Average loss: 0.7957, Accuracy: 246/327 (75%)\n",
      "Train Epoch: 45 [40/2566 (2%)]\tLoss: 0.052911\n",
      "Train Epoch: 45 [80/2566 (3%)]\tLoss: 0.016443\n",
      "Train Epoch: 45 [120/2566 (5%)]\tLoss: 0.072815\n",
      "Train Epoch: 45 [160/2566 (6%)]\tLoss: 0.051831\n",
      "Train Epoch: 45 [200/2566 (8%)]\tLoss: 0.088292\n",
      "Train Epoch: 45 [240/2566 (9%)]\tLoss: 0.053673\n",
      "Train Epoch: 45 [280/2566 (11%)]\tLoss: 0.095944\n",
      "Train Epoch: 45 [320/2566 (12%)]\tLoss: 0.127631\n",
      "Train Epoch: 45 [360/2566 (14%)]\tLoss: 0.045771\n",
      "Train Epoch: 45 [400/2566 (16%)]\tLoss: 0.109833\n",
      "Train Epoch: 45 [440/2566 (17%)]\tLoss: 0.083466\n",
      "Train Epoch: 45 [480/2566 (19%)]\tLoss: 0.177274\n",
      "Train Epoch: 45 [520/2566 (20%)]\tLoss: 0.161065\n",
      "Train Epoch: 45 [560/2566 (22%)]\tLoss: 0.023235\n",
      "Train Epoch: 45 [600/2566 (23%)]\tLoss: 0.042983\n",
      "Train Epoch: 45 [640/2566 (25%)]\tLoss: 0.006521\n",
      "Train Epoch: 45 [680/2566 (26%)]\tLoss: 0.161486\n",
      "Train Epoch: 45 [720/2566 (28%)]\tLoss: 0.046785\n",
      "Train Epoch: 45 [760/2566 (30%)]\tLoss: 0.038520\n",
      "Train Epoch: 45 [800/2566 (31%)]\tLoss: 0.074377\n",
      "Train Epoch: 45 [840/2566 (33%)]\tLoss: 0.074877\n",
      "Train Epoch: 45 [880/2566 (34%)]\tLoss: 0.137155\n",
      "Train Epoch: 45 [920/2566 (36%)]\tLoss: 0.070115\n",
      "Train Epoch: 45 [960/2566 (37%)]\tLoss: 0.034634\n",
      "Train Epoch: 45 [1000/2566 (39%)]\tLoss: 0.247248\n",
      "Train Epoch: 45 [1040/2566 (40%)]\tLoss: 0.084494\n",
      "Train Epoch: 45 [1080/2566 (42%)]\tLoss: 0.052441\n",
      "Train Epoch: 45 [1120/2566 (44%)]\tLoss: 0.030909\n",
      "Train Epoch: 45 [1160/2566 (45%)]\tLoss: 0.035360\n",
      "Train Epoch: 45 [1200/2566 (47%)]\tLoss: 0.280125\n",
      "Train Epoch: 45 [1240/2566 (48%)]\tLoss: 0.096269\n",
      "Train Epoch: 45 [1280/2566 (50%)]\tLoss: 0.017533\n",
      "Train Epoch: 45 [1320/2566 (51%)]\tLoss: 0.516339\n",
      "Train Epoch: 45 [1360/2566 (53%)]\tLoss: 0.034240\n",
      "Train Epoch: 45 [1400/2566 (55%)]\tLoss: 0.137409\n",
      "Train Epoch: 45 [1440/2566 (56%)]\tLoss: 0.025878\n",
      "Train Epoch: 45 [1480/2566 (58%)]\tLoss: 0.029562\n",
      "Train Epoch: 45 [1520/2566 (59%)]\tLoss: 0.115327\n",
      "Train Epoch: 45 [1560/2566 (61%)]\tLoss: 0.088122\n",
      "Train Epoch: 45 [1600/2566 (62%)]\tLoss: 0.205771\n",
      "Train Epoch: 45 [1640/2566 (64%)]\tLoss: 0.028033\n",
      "Train Epoch: 45 [1680/2566 (65%)]\tLoss: 0.114916\n",
      "Train Epoch: 45 [1720/2566 (67%)]\tLoss: 0.069229\n",
      "Train Epoch: 45 [1760/2566 (69%)]\tLoss: 0.093153\n",
      "Train Epoch: 45 [1800/2566 (70%)]\tLoss: 0.268166\n",
      "Train Epoch: 45 [1840/2566 (72%)]\tLoss: 0.091897\n",
      "Train Epoch: 45 [1880/2566 (73%)]\tLoss: 0.122821\n",
      "Train Epoch: 45 [1920/2566 (75%)]\tLoss: 0.026296\n",
      "Train Epoch: 45 [1960/2566 (76%)]\tLoss: 0.108205\n",
      "Train Epoch: 45 [2000/2566 (78%)]\tLoss: 0.069154\n",
      "Train Epoch: 45 [2040/2566 (79%)]\tLoss: 0.084629\n",
      "Train Epoch: 45 [2080/2566 (81%)]\tLoss: 0.102395\n",
      "Train Epoch: 45 [2120/2566 (83%)]\tLoss: 0.152463\n",
      "Train Epoch: 45 [2160/2566 (84%)]\tLoss: 0.105209\n",
      "Train Epoch: 45 [2200/2566 (86%)]\tLoss: 0.035507\n",
      "Train Epoch: 45 [2240/2566 (87%)]\tLoss: 0.165717\n",
      "Train Epoch: 45 [2280/2566 (89%)]\tLoss: 0.087453\n",
      "Train Epoch: 45 [2320/2566 (90%)]\tLoss: 0.072538\n",
      "Train Epoch: 45 [2360/2566 (92%)]\tLoss: 0.084923\n",
      "Train Epoch: 45 [2400/2566 (93%)]\tLoss: 0.025150\n",
      "Train Epoch: 45 [2440/2566 (95%)]\tLoss: 0.789583\n",
      "Train Epoch: 45 [2480/2566 (97%)]\tLoss: 0.114869\n",
      "Train Epoch: 45 [2520/2566 (98%)]\tLoss: 0.058767\n",
      "Train Epoch: 45 [2560/2566 (100%)]\tLoss: 0.282874\n",
      "epoch:45,loss:0.13036108675856736\n",
      "Train set: Average loss: 0.0907, Accuracy: 2516/2566 (98%)\n",
      "Val set: Average loss: 0.7023, Accuracy: 262/327 (80%)\n",
      "Train Epoch: 46 [40/2566 (2%)]\tLoss: 0.048178\n",
      "Train Epoch: 46 [80/2566 (3%)]\tLoss: 0.066187\n",
      "Train Epoch: 46 [120/2566 (5%)]\tLoss: 0.086174\n",
      "Train Epoch: 46 [160/2566 (6%)]\tLoss: 0.061543\n",
      "Train Epoch: 46 [200/2566 (8%)]\tLoss: 0.041166\n",
      "Train Epoch: 46 [240/2566 (9%)]\tLoss: 0.140834\n",
      "Train Epoch: 46 [280/2566 (11%)]\tLoss: 0.101462\n",
      "Train Epoch: 46 [320/2566 (12%)]\tLoss: 0.080200\n",
      "Train Epoch: 46 [360/2566 (14%)]\tLoss: 0.550826\n",
      "Train Epoch: 46 [400/2566 (16%)]\tLoss: 0.175512\n",
      "Train Epoch: 46 [440/2566 (17%)]\tLoss: 0.066066\n",
      "Train Epoch: 46 [480/2566 (19%)]\tLoss: 0.024480\n",
      "Train Epoch: 46 [520/2566 (20%)]\tLoss: 0.142441\n",
      "Train Epoch: 46 [560/2566 (22%)]\tLoss: 0.034723\n",
      "Train Epoch: 46 [600/2566 (23%)]\tLoss: 0.088412\n",
      "Train Epoch: 46 [640/2566 (25%)]\tLoss: 0.075125\n",
      "Train Epoch: 46 [680/2566 (26%)]\tLoss: 0.025072\n",
      "Train Epoch: 46 [720/2566 (28%)]\tLoss: 0.090315\n",
      "Train Epoch: 46 [760/2566 (30%)]\tLoss: 0.014322\n",
      "Train Epoch: 46 [800/2566 (31%)]\tLoss: 0.098633\n",
      "Train Epoch: 46 [840/2566 (33%)]\tLoss: 0.032694\n",
      "Train Epoch: 46 [880/2566 (34%)]\tLoss: 0.033879\n",
      "Train Epoch: 46 [920/2566 (36%)]\tLoss: 0.195858\n",
      "Train Epoch: 46 [960/2566 (37%)]\tLoss: 0.023752\n",
      "Train Epoch: 46 [1000/2566 (39%)]\tLoss: 0.105753\n",
      "Train Epoch: 46 [1040/2566 (40%)]\tLoss: 0.034078\n",
      "Train Epoch: 46 [1080/2566 (42%)]\tLoss: 0.015651\n",
      "Train Epoch: 46 [1120/2566 (44%)]\tLoss: 0.081893\n",
      "Train Epoch: 46 [1160/2566 (45%)]\tLoss: 0.070392\n",
      "Train Epoch: 46 [1200/2566 (47%)]\tLoss: 0.117109\n",
      "Train Epoch: 46 [1240/2566 (48%)]\tLoss: 0.217201\n",
      "Train Epoch: 46 [1280/2566 (50%)]\tLoss: 0.014941\n",
      "Train Epoch: 46 [1320/2566 (51%)]\tLoss: 0.068004\n",
      "Train Epoch: 46 [1360/2566 (53%)]\tLoss: 0.369932\n",
      "Train Epoch: 46 [1400/2566 (55%)]\tLoss: 0.053080\n",
      "Train Epoch: 46 [1440/2566 (56%)]\tLoss: 0.180334\n",
      "Train Epoch: 46 [1480/2566 (58%)]\tLoss: 0.092275\n",
      "Train Epoch: 46 [1520/2566 (59%)]\tLoss: 0.362244\n",
      "Train Epoch: 46 [1560/2566 (61%)]\tLoss: 0.056326\n",
      "Train Epoch: 46 [1600/2566 (62%)]\tLoss: 0.056237\n",
      "Train Epoch: 46 [1640/2566 (64%)]\tLoss: 0.104773\n",
      "Train Epoch: 46 [1680/2566 (65%)]\tLoss: 0.039675\n",
      "Train Epoch: 46 [1720/2566 (67%)]\tLoss: 0.012873\n",
      "Train Epoch: 46 [1760/2566 (69%)]\tLoss: 0.195788\n",
      "Train Epoch: 46 [1800/2566 (70%)]\tLoss: 0.484784\n",
      "Train Epoch: 46 [1840/2566 (72%)]\tLoss: 0.065750\n",
      "Train Epoch: 46 [1880/2566 (73%)]\tLoss: 0.021375\n",
      "Train Epoch: 46 [1920/2566 (75%)]\tLoss: 0.116677\n",
      "Train Epoch: 46 [1960/2566 (76%)]\tLoss: 0.035399\n",
      "Train Epoch: 46 [2000/2566 (78%)]\tLoss: 0.278923\n",
      "Train Epoch: 46 [2040/2566 (79%)]\tLoss: 0.056327\n",
      "Train Epoch: 46 [2080/2566 (81%)]\tLoss: 0.046959\n",
      "Train Epoch: 46 [2120/2566 (83%)]\tLoss: 0.022705\n",
      "Train Epoch: 46 [2160/2566 (84%)]\tLoss: 0.133091\n",
      "Train Epoch: 46 [2200/2566 (86%)]\tLoss: 0.456182\n",
      "Train Epoch: 46 [2240/2566 (87%)]\tLoss: 0.112673\n",
      "Train Epoch: 46 [2280/2566 (89%)]\tLoss: 0.254161\n",
      "Train Epoch: 46 [2320/2566 (90%)]\tLoss: 0.089252\n",
      "Train Epoch: 46 [2360/2566 (92%)]\tLoss: 0.010641\n",
      "Train Epoch: 46 [2400/2566 (93%)]\tLoss: 0.032375\n",
      "Train Epoch: 46 [2440/2566 (95%)]\tLoss: 0.153035\n",
      "Train Epoch: 46 [2480/2566 (97%)]\tLoss: 0.104249\n",
      "Train Epoch: 46 [2520/2566 (98%)]\tLoss: 0.111957\n",
      "Train Epoch: 46 [2560/2566 (100%)]\tLoss: 0.274150\n",
      "epoch:46,loss:0.12518680498085197\n",
      "Train set: Average loss: 0.0736, Accuracy: 2530/2566 (99%)\n",
      "Val set: Average loss: 0.7670, Accuracy: 254/327 (78%)\n",
      "Train Epoch: 47 [40/2566 (2%)]\tLoss: 0.022529\n",
      "Train Epoch: 47 [80/2566 (3%)]\tLoss: 0.047881\n",
      "Train Epoch: 47 [120/2566 (5%)]\tLoss: 0.184208\n",
      "Train Epoch: 47 [160/2566 (6%)]\tLoss: 0.024774\n",
      "Train Epoch: 47 [200/2566 (8%)]\tLoss: 0.035549\n",
      "Train Epoch: 47 [240/2566 (9%)]\tLoss: 0.214240\n",
      "Train Epoch: 47 [280/2566 (11%)]\tLoss: 0.097442\n",
      "Train Epoch: 47 [320/2566 (12%)]\tLoss: 0.105445\n",
      "Train Epoch: 47 [360/2566 (14%)]\tLoss: 0.132168\n",
      "Train Epoch: 47 [400/2566 (16%)]\tLoss: 0.061004\n",
      "Train Epoch: 47 [440/2566 (17%)]\tLoss: 0.304939\n",
      "Train Epoch: 47 [480/2566 (19%)]\tLoss: 0.034728\n",
      "Train Epoch: 47 [520/2566 (20%)]\tLoss: 0.007070\n",
      "Train Epoch: 47 [560/2566 (22%)]\tLoss: 0.019756\n",
      "Train Epoch: 47 [600/2566 (23%)]\tLoss: 0.084585\n",
      "Train Epoch: 47 [640/2566 (25%)]\tLoss: 0.032495\n",
      "Train Epoch: 47 [680/2566 (26%)]\tLoss: 0.013437\n",
      "Train Epoch: 47 [720/2566 (28%)]\tLoss: 0.107166\n",
      "Train Epoch: 47 [760/2566 (30%)]\tLoss: 0.045633\n",
      "Train Epoch: 47 [800/2566 (31%)]\tLoss: 0.325265\n",
      "Train Epoch: 47 [840/2566 (33%)]\tLoss: 0.008507\n",
      "Train Epoch: 47 [880/2566 (34%)]\tLoss: 0.112812\n",
      "Train Epoch: 47 [920/2566 (36%)]\tLoss: 0.080541\n",
      "Train Epoch: 47 [960/2566 (37%)]\tLoss: 0.315754\n",
      "Train Epoch: 47 [1000/2566 (39%)]\tLoss: 0.346966\n",
      "Train Epoch: 47 [1040/2566 (40%)]\tLoss: 0.030254\n",
      "Train Epoch: 47 [1080/2566 (42%)]\tLoss: 0.046762\n",
      "Train Epoch: 47 [1120/2566 (44%)]\tLoss: 0.161876\n",
      "Train Epoch: 47 [1160/2566 (45%)]\tLoss: 0.111865\n",
      "Train Epoch: 47 [1200/2566 (47%)]\tLoss: 0.021962\n",
      "Train Epoch: 47 [1240/2566 (48%)]\tLoss: 0.044830\n",
      "Train Epoch: 47 [1280/2566 (50%)]\tLoss: 0.092249\n",
      "Train Epoch: 47 [1320/2566 (51%)]\tLoss: 0.013077\n",
      "Train Epoch: 47 [1360/2566 (53%)]\tLoss: 0.563737\n",
      "Train Epoch: 47 [1400/2566 (55%)]\tLoss: 0.097250\n",
      "Train Epoch: 47 [1440/2566 (56%)]\tLoss: 0.057561\n",
      "Train Epoch: 47 [1480/2566 (58%)]\tLoss: 0.078570\n",
      "Train Epoch: 47 [1520/2566 (59%)]\tLoss: 0.455824\n",
      "Train Epoch: 47 [1560/2566 (61%)]\tLoss: 0.382880\n",
      "Train Epoch: 47 [1600/2566 (62%)]\tLoss: 0.016960\n",
      "Train Epoch: 47 [1640/2566 (64%)]\tLoss: 0.086388\n",
      "Train Epoch: 47 [1680/2566 (65%)]\tLoss: 0.287733\n",
      "Train Epoch: 47 [1720/2566 (67%)]\tLoss: 0.035703\n",
      "Train Epoch: 47 [1760/2566 (69%)]\tLoss: 0.020025\n",
      "Train Epoch: 47 [1800/2566 (70%)]\tLoss: 0.031261\n",
      "Train Epoch: 47 [1840/2566 (72%)]\tLoss: 0.192852\n",
      "Train Epoch: 47 [1880/2566 (73%)]\tLoss: 0.077059\n",
      "Train Epoch: 47 [1920/2566 (75%)]\tLoss: 0.094016\n",
      "Train Epoch: 47 [1960/2566 (76%)]\tLoss: 0.393172\n",
      "Train Epoch: 47 [2000/2566 (78%)]\tLoss: 0.015784\n",
      "Train Epoch: 47 [2040/2566 (79%)]\tLoss: 0.018525\n",
      "Train Epoch: 47 [2080/2566 (81%)]\tLoss: 0.029599\n",
      "Train Epoch: 47 [2120/2566 (83%)]\tLoss: 0.903203\n",
      "Train Epoch: 47 [2160/2566 (84%)]\tLoss: 0.021244\n",
      "Train Epoch: 47 [2200/2566 (86%)]\tLoss: 0.088489\n",
      "Train Epoch: 47 [2240/2566 (87%)]\tLoss: 0.113870\n",
      "Train Epoch: 47 [2280/2566 (89%)]\tLoss: 0.017460\n",
      "Train Epoch: 47 [2320/2566 (90%)]\tLoss: 0.053613\n",
      "Train Epoch: 47 [2360/2566 (92%)]\tLoss: 0.250385\n",
      "Train Epoch: 47 [2400/2566 (93%)]\tLoss: 0.014565\n",
      "Train Epoch: 47 [2440/2566 (95%)]\tLoss: 0.046620\n",
      "Train Epoch: 47 [2480/2566 (97%)]\tLoss: 0.039902\n",
      "Train Epoch: 47 [2520/2566 (98%)]\tLoss: 0.117977\n",
      "Train Epoch: 47 [2560/2566 (100%)]\tLoss: 0.090872\n",
      "epoch:47,loss:0.10969237777088243\n",
      "Train set: Average loss: 0.0922, Accuracy: 2510/2566 (98%)\n",
      "Val set: Average loss: 0.7348, Accuracy: 255/327 (78%)\n",
      "Train Epoch: 48 [40/2566 (2%)]\tLoss: 0.045035\n",
      "Train Epoch: 48 [80/2566 (3%)]\tLoss: 0.379896\n",
      "Train Epoch: 48 [120/2566 (5%)]\tLoss: 0.063118\n",
      "Train Epoch: 48 [160/2566 (6%)]\tLoss: 0.204491\n",
      "Train Epoch: 48 [200/2566 (8%)]\tLoss: 0.058254\n",
      "Train Epoch: 48 [240/2566 (9%)]\tLoss: 0.059066\n",
      "Train Epoch: 48 [280/2566 (11%)]\tLoss: 0.069018\n",
      "Train Epoch: 48 [320/2566 (12%)]\tLoss: 0.031263\n",
      "Train Epoch: 48 [360/2566 (14%)]\tLoss: 0.026732\n",
      "Train Epoch: 48 [400/2566 (16%)]\tLoss: 0.530275\n",
      "Train Epoch: 48 [440/2566 (17%)]\tLoss: 0.070923\n",
      "Train Epoch: 48 [480/2566 (19%)]\tLoss: 0.088134\n",
      "Train Epoch: 48 [520/2566 (20%)]\tLoss: 0.039674\n",
      "Train Epoch: 48 [560/2566 (22%)]\tLoss: 0.023400\n",
      "Train Epoch: 48 [600/2566 (23%)]\tLoss: 0.254231\n",
      "Train Epoch: 48 [640/2566 (25%)]\tLoss: 0.040294\n",
      "Train Epoch: 48 [680/2566 (26%)]\tLoss: 0.087988\n",
      "Train Epoch: 48 [720/2566 (28%)]\tLoss: 0.253745\n",
      "Train Epoch: 48 [760/2566 (30%)]\tLoss: 0.040881\n",
      "Train Epoch: 48 [800/2566 (31%)]\tLoss: 0.063169\n",
      "Train Epoch: 48 [840/2566 (33%)]\tLoss: 0.238399\n",
      "Train Epoch: 48 [880/2566 (34%)]\tLoss: 0.191356\n",
      "Train Epoch: 48 [920/2566 (36%)]\tLoss: 0.007321\n",
      "Train Epoch: 48 [960/2566 (37%)]\tLoss: 0.036004\n",
      "Train Epoch: 48 [1000/2566 (39%)]\tLoss: 0.119863\n",
      "Train Epoch: 48 [1040/2566 (40%)]\tLoss: 0.013021\n",
      "Train Epoch: 48 [1080/2566 (42%)]\tLoss: 0.010307\n",
      "Train Epoch: 48 [1120/2566 (44%)]\tLoss: 0.058424\n",
      "Train Epoch: 48 [1160/2566 (45%)]\tLoss: 0.026734\n",
      "Train Epoch: 48 [1200/2566 (47%)]\tLoss: 0.168502\n",
      "Train Epoch: 48 [1240/2566 (48%)]\tLoss: 0.009747\n",
      "Train Epoch: 48 [1280/2566 (50%)]\tLoss: 0.026743\n",
      "Train Epoch: 48 [1320/2566 (51%)]\tLoss: 0.028386\n",
      "Train Epoch: 48 [1360/2566 (53%)]\tLoss: 0.042307\n",
      "Train Epoch: 48 [1400/2566 (55%)]\tLoss: 0.352040\n",
      "Train Epoch: 48 [1440/2566 (56%)]\tLoss: 0.011599\n",
      "Train Epoch: 48 [1480/2566 (58%)]\tLoss: 0.155822\n",
      "Train Epoch: 48 [1520/2566 (59%)]\tLoss: 0.059998\n",
      "Train Epoch: 48 [1560/2566 (61%)]\tLoss: 0.123610\n",
      "Train Epoch: 48 [1600/2566 (62%)]\tLoss: 0.021695\n",
      "Train Epoch: 48 [1640/2566 (64%)]\tLoss: 0.036282\n",
      "Train Epoch: 48 [1680/2566 (65%)]\tLoss: 0.039985\n",
      "Train Epoch: 48 [1720/2566 (67%)]\tLoss: 0.020464\n",
      "Train Epoch: 48 [1760/2566 (69%)]\tLoss: 0.115896\n",
      "Train Epoch: 48 [1800/2566 (70%)]\tLoss: 0.010154\n",
      "Train Epoch: 48 [1840/2566 (72%)]\tLoss: 0.042900\n",
      "Train Epoch: 48 [1880/2566 (73%)]\tLoss: 0.026086\n",
      "Train Epoch: 48 [1920/2566 (75%)]\tLoss: 0.030946\n",
      "Train Epoch: 48 [1960/2566 (76%)]\tLoss: 0.098229\n",
      "Train Epoch: 48 [2000/2566 (78%)]\tLoss: 0.026749\n",
      "Train Epoch: 48 [2040/2566 (79%)]\tLoss: 0.022900\n",
      "Train Epoch: 48 [2080/2566 (81%)]\tLoss: 0.072446\n",
      "Train Epoch: 48 [2120/2566 (83%)]\tLoss: 0.324030\n",
      "Train Epoch: 48 [2160/2566 (84%)]\tLoss: 0.099600\n",
      "Train Epoch: 48 [2200/2566 (86%)]\tLoss: 0.027617\n",
      "Train Epoch: 48 [2240/2566 (87%)]\tLoss: 0.055716\n",
      "Train Epoch: 48 [2280/2566 (89%)]\tLoss: 1.047373\n",
      "Train Epoch: 48 [2320/2566 (90%)]\tLoss: 0.300312\n",
      "Train Epoch: 48 [2360/2566 (92%)]\tLoss: 0.111776\n",
      "Train Epoch: 48 [2400/2566 (93%)]\tLoss: 0.126839\n",
      "Train Epoch: 48 [2440/2566 (95%)]\tLoss: 0.289047\n",
      "Train Epoch: 48 [2480/2566 (97%)]\tLoss: 0.116556\n",
      "Train Epoch: 48 [2520/2566 (98%)]\tLoss: 0.074197\n",
      "Train Epoch: 48 [2560/2566 (100%)]\tLoss: 0.039649\n",
      "epoch:48,loss:0.1082079517466489\n",
      "Train set: Average loss: 0.1553, Accuracy: 2429/2566 (95%)\n",
      "Val set: Average loss: 0.8459, Accuracy: 252/327 (77%)\n",
      "Train Epoch: 49 [40/2566 (2%)]\tLoss: 0.016256\n",
      "Train Epoch: 49 [80/2566 (3%)]\tLoss: 0.010646\n",
      "Train Epoch: 49 [120/2566 (5%)]\tLoss: 0.028171\n",
      "Train Epoch: 49 [160/2566 (6%)]\tLoss: 0.031243\n",
      "Train Epoch: 49 [200/2566 (8%)]\tLoss: 0.006912\n",
      "Train Epoch: 49 [240/2566 (9%)]\tLoss: 0.147875\n",
      "Train Epoch: 49 [280/2566 (11%)]\tLoss: 0.024147\n",
      "Train Epoch: 49 [320/2566 (12%)]\tLoss: 0.032062\n",
      "Train Epoch: 49 [360/2566 (14%)]\tLoss: 0.122466\n",
      "Train Epoch: 49 [400/2566 (16%)]\tLoss: 0.118312\n",
      "Train Epoch: 49 [440/2566 (17%)]\tLoss: 0.042157\n",
      "Train Epoch: 49 [480/2566 (19%)]\tLoss: 0.570055\n",
      "Train Epoch: 49 [520/2566 (20%)]\tLoss: 0.005030\n",
      "Train Epoch: 49 [560/2566 (22%)]\tLoss: 0.035968\n",
      "Train Epoch: 49 [600/2566 (23%)]\tLoss: 0.053429\n",
      "Train Epoch: 49 [640/2566 (25%)]\tLoss: 0.061749\n",
      "Train Epoch: 49 [680/2566 (26%)]\tLoss: 0.034360\n",
      "Train Epoch: 49 [720/2566 (28%)]\tLoss: 0.370535\n",
      "Train Epoch: 49 [760/2566 (30%)]\tLoss: 0.114625\n",
      "Train Epoch: 49 [800/2566 (31%)]\tLoss: 0.033433\n",
      "Train Epoch: 49 [840/2566 (33%)]\tLoss: 0.032760\n",
      "Train Epoch: 49 [880/2566 (34%)]\tLoss: 0.187449\n",
      "Train Epoch: 49 [920/2566 (36%)]\tLoss: 0.039260\n",
      "Train Epoch: 49 [960/2566 (37%)]\tLoss: 0.191165\n",
      "Train Epoch: 49 [1000/2566 (39%)]\tLoss: 0.077018\n",
      "Train Epoch: 49 [1040/2566 (40%)]\tLoss: 0.441076\n",
      "Train Epoch: 49 [1080/2566 (42%)]\tLoss: 0.062017\n",
      "Train Epoch: 49 [1120/2566 (44%)]\tLoss: 0.170308\n",
      "Train Epoch: 49 [1160/2566 (45%)]\tLoss: 0.339972\n",
      "Train Epoch: 49 [1200/2566 (47%)]\tLoss: 0.009079\n",
      "Train Epoch: 49 [1240/2566 (48%)]\tLoss: 0.013769\n",
      "Train Epoch: 49 [1280/2566 (50%)]\tLoss: 0.142431\n",
      "Train Epoch: 49 [1320/2566 (51%)]\tLoss: 0.022352\n",
      "Train Epoch: 49 [1360/2566 (53%)]\tLoss: 0.079304\n",
      "Train Epoch: 49 [1400/2566 (55%)]\tLoss: 0.038820\n",
      "Train Epoch: 49 [1440/2566 (56%)]\tLoss: 0.145373\n",
      "Train Epoch: 49 [1480/2566 (58%)]\tLoss: 0.260272\n",
      "Train Epoch: 49 [1520/2566 (59%)]\tLoss: 0.201200\n",
      "Train Epoch: 49 [1560/2566 (61%)]\tLoss: 0.054607\n",
      "Train Epoch: 49 [1600/2566 (62%)]\tLoss: 0.023105\n",
      "Train Epoch: 49 [1640/2566 (64%)]\tLoss: 0.065516\n",
      "Train Epoch: 49 [1680/2566 (65%)]\tLoss: 0.015118\n",
      "Train Epoch: 49 [1720/2566 (67%)]\tLoss: 0.818374\n",
      "Train Epoch: 49 [1760/2566 (69%)]\tLoss: 0.020267\n",
      "Train Epoch: 49 [1800/2566 (70%)]\tLoss: 0.118350\n",
      "Train Epoch: 49 [1840/2566 (72%)]\tLoss: 0.031758\n",
      "Train Epoch: 49 [1880/2566 (73%)]\tLoss: 0.073666\n",
      "Train Epoch: 49 [1920/2566 (75%)]\tLoss: 0.084737\n",
      "Train Epoch: 49 [1960/2566 (76%)]\tLoss: 0.049188\n",
      "Train Epoch: 49 [2000/2566 (78%)]\tLoss: 0.051069\n",
      "Train Epoch: 49 [2040/2566 (79%)]\tLoss: 0.864374\n",
      "Train Epoch: 49 [2080/2566 (81%)]\tLoss: 0.006157\n",
      "Train Epoch: 49 [2120/2566 (83%)]\tLoss: 0.099021\n",
      "Train Epoch: 49 [2160/2566 (84%)]\tLoss: 0.153174\n",
      "Train Epoch: 49 [2200/2566 (86%)]\tLoss: 0.135570\n",
      "Train Epoch: 49 [2240/2566 (87%)]\tLoss: 0.020459\n",
      "Train Epoch: 49 [2280/2566 (89%)]\tLoss: 0.025189\n",
      "Train Epoch: 49 [2320/2566 (90%)]\tLoss: 0.257847\n",
      "Train Epoch: 49 [2360/2566 (92%)]\tLoss: 0.005214\n",
      "Train Epoch: 49 [2400/2566 (93%)]\tLoss: 0.111702\n",
      "Train Epoch: 49 [2440/2566 (95%)]\tLoss: 0.032861\n",
      "Train Epoch: 49 [2480/2566 (97%)]\tLoss: 0.077063\n",
      "Train Epoch: 49 [2520/2566 (98%)]\tLoss: 0.031390\n",
      "Train Epoch: 49 [2560/2566 (100%)]\tLoss: 0.016486\n",
      "epoch:49,loss:0.11044924356757063\n",
      "Train set: Average loss: 0.0607, Accuracy: 2529/2566 (99%)\n",
      "Val set: Average loss: 0.7202, Accuracy: 257/327 (79%)\n",
      "Train Epoch: 50 [40/2566 (2%)]\tLoss: 0.035097\n",
      "Train Epoch: 50 [80/2566 (3%)]\tLoss: 0.062743\n",
      "Train Epoch: 50 [120/2566 (5%)]\tLoss: 0.036789\n",
      "Train Epoch: 50 [160/2566 (6%)]\tLoss: 0.177519\n",
      "Train Epoch: 50 [200/2566 (8%)]\tLoss: 0.208369\n",
      "Train Epoch: 50 [240/2566 (9%)]\tLoss: 0.005462\n",
      "Train Epoch: 50 [280/2566 (11%)]\tLoss: 0.036708\n",
      "Train Epoch: 50 [320/2566 (12%)]\tLoss: 0.027276\n",
      "Train Epoch: 50 [360/2566 (14%)]\tLoss: 0.139265\n",
      "Train Epoch: 50 [400/2566 (16%)]\tLoss: 0.011311\n",
      "Train Epoch: 50 [440/2566 (17%)]\tLoss: 0.011250\n",
      "Train Epoch: 50 [480/2566 (19%)]\tLoss: 0.011260\n",
      "Train Epoch: 50 [520/2566 (20%)]\tLoss: 0.003815\n",
      "Train Epoch: 50 [560/2566 (22%)]\tLoss: 0.057068\n",
      "Train Epoch: 50 [600/2566 (23%)]\tLoss: 0.020445\n",
      "Train Epoch: 50 [640/2566 (25%)]\tLoss: 0.010706\n",
      "Train Epoch: 50 [680/2566 (26%)]\tLoss: 0.008566\n",
      "Train Epoch: 50 [720/2566 (28%)]\tLoss: 0.266652\n",
      "Train Epoch: 50 [760/2566 (30%)]\tLoss: 0.138281\n",
      "Train Epoch: 50 [800/2566 (31%)]\tLoss: 0.011701\n",
      "Train Epoch: 50 [840/2566 (33%)]\tLoss: 0.007391\n",
      "Train Epoch: 50 [880/2566 (34%)]\tLoss: 0.083559\n",
      "Train Epoch: 50 [920/2566 (36%)]\tLoss: 0.002268\n",
      "Train Epoch: 50 [960/2566 (37%)]\tLoss: 0.017449\n",
      "Train Epoch: 50 [1000/2566 (39%)]\tLoss: 0.043733\n",
      "Train Epoch: 50 [1040/2566 (40%)]\tLoss: 0.117058\n",
      "Train Epoch: 50 [1080/2566 (42%)]\tLoss: 0.035352\n",
      "Train Epoch: 50 [1120/2566 (44%)]\tLoss: 0.065923\n",
      "Train Epoch: 50 [1160/2566 (45%)]\tLoss: 0.027443\n",
      "Train Epoch: 50 [1200/2566 (47%)]\tLoss: 0.070052\n",
      "Train Epoch: 50 [1240/2566 (48%)]\tLoss: 0.022343\n",
      "Train Epoch: 50 [1280/2566 (50%)]\tLoss: 0.068123\n",
      "Train Epoch: 50 [1320/2566 (51%)]\tLoss: 0.016203\n",
      "Train Epoch: 50 [1360/2566 (53%)]\tLoss: 0.158283\n",
      "Train Epoch: 50 [1400/2566 (55%)]\tLoss: 0.078748\n",
      "Train Epoch: 50 [1440/2566 (56%)]\tLoss: 0.028293\n",
      "Train Epoch: 50 [1480/2566 (58%)]\tLoss: 0.016704\n",
      "Train Epoch: 50 [1520/2566 (59%)]\tLoss: 0.106032\n",
      "Train Epoch: 50 [1560/2566 (61%)]\tLoss: 0.011459\n",
      "Train Epoch: 50 [1600/2566 (62%)]\tLoss: 0.006024\n",
      "Train Epoch: 50 [1640/2566 (64%)]\tLoss: 0.022142\n",
      "Train Epoch: 50 [1680/2566 (65%)]\tLoss: 0.018560\n",
      "Train Epoch: 50 [1720/2566 (67%)]\tLoss: 0.011094\n",
      "Train Epoch: 50 [1760/2566 (69%)]\tLoss: 0.031813\n",
      "Train Epoch: 50 [1800/2566 (70%)]\tLoss: 0.009928\n",
      "Train Epoch: 50 [1840/2566 (72%)]\tLoss: 0.207810\n",
      "Train Epoch: 50 [1880/2566 (73%)]\tLoss: 0.141122\n",
      "Train Epoch: 50 [1920/2566 (75%)]\tLoss: 0.035403\n",
      "Train Epoch: 50 [1960/2566 (76%)]\tLoss: 0.092710\n",
      "Train Epoch: 50 [2000/2566 (78%)]\tLoss: 0.023155\n",
      "Train Epoch: 50 [2040/2566 (79%)]\tLoss: 0.283677\n",
      "Train Epoch: 50 [2080/2566 (81%)]\tLoss: 0.056879\n",
      "Train Epoch: 50 [2120/2566 (83%)]\tLoss: 0.049580\n",
      "Train Epoch: 50 [2160/2566 (84%)]\tLoss: 0.016235\n",
      "Train Epoch: 50 [2200/2566 (86%)]\tLoss: 0.022659\n",
      "Train Epoch: 50 [2240/2566 (87%)]\tLoss: 0.020866\n",
      "Train Epoch: 50 [2280/2566 (89%)]\tLoss: 0.018130\n",
      "Train Epoch: 50 [2320/2566 (90%)]\tLoss: 0.023637\n",
      "Train Epoch: 50 [2360/2566 (92%)]\tLoss: 0.018552\n",
      "Train Epoch: 50 [2400/2566 (93%)]\tLoss: 0.018464\n",
      "Train Epoch: 50 [2440/2566 (95%)]\tLoss: 0.057547\n",
      "Train Epoch: 50 [2480/2566 (97%)]\tLoss: 0.053655\n",
      "Train Epoch: 50 [2520/2566 (98%)]\tLoss: 0.021699\n",
      "Train Epoch: 50 [2560/2566 (100%)]\tLoss: 0.034259\n",
      "epoch:50,loss:0.07860810875622817\n",
      "Train set: Average loss: 0.0429, Accuracy: 2535/2566 (99%)\n",
      "Val set: Average loss: 0.7116, Accuracy: 265/327 (81%)\n",
      "training_time:4874.925837755203\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    val(model, DEVICE, train_loader, 'train')\n",
    "    val(model, DEVICE, test_loader, 'val')\n",
    "torch.save(model, 'model_last.pth')\n",
    "np.savetxt('logs.txt', logs, fmt='%s')\n",
    "end_time = time()\n",
    "print(f'training_time:{end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2c2ef7-5190-4440-a58d-190576812005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize loss and accuracy curves\n",
    "path = 'logs.txt'\n",
    "with open(path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "for line in lines:\n",
    "    if 'Val set' in line:\n",
    "        line = line.split(',')\n",
    "        val_acc.append(float(line[-1].split('(')[-1].split('%')[0]))\n",
    "        val_loss.append(float(line[0].split(':')[-1]))\n",
    "    elif 'Train set' in line:\n",
    "        line = line.split(',')\n",
    "        train_acc.append(float(line[-1].split('(')[-1].split('%')[0]))\n",
    "        train_loss.append(float(line[0].split(':')[-1]))\n",
    "        \n",
    "x = np.arange(1, len(val_acc)+1)\n",
    "\n",
    "plt.plot(x, train_acc, label='train accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.plot(x, val_acc, label='val accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.title('accuracy curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "# #Change the scale range of the vertical axis\n",
    "plt.ylim(0,101)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, train_loss, label='train loss')\n",
    "plt.legend(loc='best')\n",
    "plt.plot(x, val_loss, label='val loss')\n",
    "plt.legend(loc='best')\n",
    "plt.title('loss curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78462ab5-7baa-4b8a-9950-be5e43e56288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†ç¡®çŽ‡ï¼š0.8103975535168195\n",
      "æ··æ·†çŸ©é˜µ: \n",
      "[[ 17  13   2   1   0]\n",
      " [ 11  68   4   3   3]\n",
      " [  2   1 159   0   0]\n",
      " [  1  11   0  14   0]\n",
      " [  0   5   0   5   7]]\n",
      "æ¯ä¸€ç±»çš„precisionã€recallå’Œf1-score: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mild\n",
      "       0.55      0.52      0.53        33\n",
      "      Moderate\n",
      "       0.69      0.76      0.73        89\n",
      "         No_DR\n",
      "       0.96      0.98      0.97       162\n",
      "Proliferate_DR\n",
      "       0.61      0.54      0.57        26\n",
      "        Severe\n",
      "       0.70      0.41      0.52        17\n",
      "\n",
      "       accuracy                           0.81       327\n",
      "      macro avg       0.70      0.64      0.66       327\n",
      "   weighted avg       0.81      0.81      0.81       327\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "with open('datasets/classes.txt', 'r') as f:\n",
    "    classesname = tuple(f.readlines())\n",
    "model = torch.load('model_last.pth', map_location=DEVICE.type)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "correct = 0\n",
    "total_num = len(test_loader.dataset)\n",
    "labels = []\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data).to(DEVICE), Variable(target).to(DEVICE)\n",
    "        output = model(data)\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        labels.append(target.view(-1))\n",
    "        preds.append(pred.view(-1))\n",
    "        correct += torch.sum(pred == target)\n",
    "    correct = correct.data.item()\n",
    "    acc = correct / total_num\n",
    "    print('Accuracyï¼š{}'.format(acc))\n",
    "    labels = torch.cat(labels)\n",
    "    preds = torch.cat(preds)\n",
    "    if 'cuda' in DEVICE.type:\n",
    "        preds = np.array(preds.cpu())\n",
    "        labels = np.array(labels.cpu())\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    # Print confusion_matrix\n",
    "    print(\"confusion_matrix: \")\n",
    "    print(cm)\n",
    "    print(\"For each category:precisionã€recall and f1-score: \")\n",
    "    print(classification_report(labels, preds, target_names=classesname))\n",
    "    # Draw the confusion matrix\n",
    "    # ConfusionMatrixDisplay Required parameters: confusion_matrix, display_labels\n",
    "    plt.rcParams.update({'font.size': 7})\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classesname)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d9730-1bae-4586-b5e1-5155cddd4bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
